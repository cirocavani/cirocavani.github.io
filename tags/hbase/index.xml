<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hbase on Ciro Cavani</title>
    <link>http://cirocavani.github.io/tags/hbase/index.xml</link>
    <description>Recent content in Hbase on Ciro Cavani</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>pt-br</language>
    <copyright>&amp;copy; 2016. All rights reserved.</copyright>
    <atom:link href="http://cirocavani.github.io/tags/hbase/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Configuração do Hadoop, HBase e Kafka na Máquina Local com Docker</title>
      <link>http://cirocavani.github.io/post/configuracao-do-hadoop-hbase-e-kafka-na-maquina-local-com-docker/</link>
      <pubDate>Wed, 16 Sep 2015 23:26:07 -0300</pubDate>
      
      <guid>http://cirocavani.github.io/post/configuracao-do-hadoop-hbase-e-kafka-na-maquina-local-com-docker/</guid>
      <description>

&lt;p&gt;Esse tutorial é sobre a criação de uma imagem do Docker com a configuração local do Hadoop, HBase e Kafka. Nesse procedimento, o Hadoop é configurado no modo pseudo-distribuído com cada serviço rodando em uma instância própria da JVM, mas todas na mesma máquina. O HBase e o Kafka também rodam em modo &amp;lsquo;distribuído&amp;rsquo; compartilhando uma instância separada do ZooKeeper. Esse procedimento é muito útil para testar funcionalidades desses serviços e aprendizado, mas não é uma solução completa para uso em produção.&lt;/p&gt;

&lt;h2 id=&#34;pré-requisito&#34;&gt;Pré-requisito&lt;/h2&gt;

&lt;p&gt;Nesse procedimento, é necessário que o Docker esteja instalado e funcionando; também é necessário acesso à Internet.&lt;/p&gt;

&lt;p&gt;Originalmente, esse procedimento foi testado no ArchLinux atualizado até final de Agosto/2015.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://wiki.archlinux.org/index.php/Docker&#34;&gt;https://wiki.archlinux.org/index.php/Docker&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo docker version

&amp;gt; Client:
&amp;gt;  Version:      1.8.1
&amp;gt;  API version:  1.20
&amp;gt;  Go version:   go1.4.2
&amp;gt;  Git commit:   d12ea79
&amp;gt;  Built:        Sat Aug 15 17:29:10 UTC 2015
&amp;gt;  OS/Arch:      linux/amd64
&amp;gt;
&amp;gt; Server:
&amp;gt;  Version:      1.8.1
&amp;gt;  API version:  1.20
&amp;gt;  Go version:   go1.4.2
&amp;gt;  Git commit:   d12ea79
&amp;gt;  Built:        Sat Aug 15 17:29:10 UTC 2015
&amp;gt;  OS/Arch:      linux/amd64
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;configuração&#34;&gt;Configuração&lt;/h2&gt;

&lt;p&gt;Hadoop, ZooKeeper, HBase e Kafka.&lt;/p&gt;

&lt;h3 id=&#34;container&#34;&gt;Container&lt;/h3&gt;

&lt;p&gt;Começamos com a criação de um conainer do Docker com a imagem do CentOS6.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Importante: para os endereços com &lt;code&gt;grandesdados-hadoop&lt;/code&gt; funcionarem fora do container, direto na máquina host, é necessário colocar no &lt;code&gt;/etc/hosts&lt;/code&gt; da máquina host o endereço IP do container do Docker para esse nome.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Ao executar o comando &lt;code&gt;run&lt;/code&gt;, o Docker automaticamente fará o download da imagem e a shell será inicializada dentro de um novo container.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo docker run -i -t --name=grandesdados-hadoop --hostname=grandesdados-hadoop centos:6 /bin/bash

&amp;gt; Unable to find image &#39;centos:6&#39; locally
&amp;gt; 6: Pulling from library/centos
&amp;gt;
&amp;gt; f1b10cd84249: Pull complete
&amp;gt; fb9cc58bde0c: Pull complete
&amp;gt; a005304e4e74: Already exists
&amp;gt; library/centos:6: The image you are pulling has been verified. Important: image verification is a tech preview feature and should not be relied on to provide security.
&amp;gt;
&amp;gt; Digest: sha256:25d94c55b37cb7a33ad706d5f440e36376fec20f59e57d16fe02c64698b531c1
&amp;gt; Status: Downloaded newer image for centos:6
&amp;gt; [root@grandesdados-hadoop /]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Já dentro do container criamos um usuário e local que serão usados para a instalação e execução dos processos.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;adduser -m -d /hadoop hadoop
cd hadoop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A versão usada nesse procedimento é o Java 8, atual versão estável da Oracle.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -k -L -H &amp;quot;Cookie: oraclelicense=accept-securebackup-cookie&amp;quot; -O http://download.oracle.com/otn-pub/java/jdk/8u60-b27/jdk-8u60-linux-x64.rpm
rpm -i jdk-8u60-linux-x64.rpm

echo &#39;export JAVA_HOME=&amp;quot;/usr/java/jdk1.8.0_60&amp;quot;&#39; &amp;gt; /etc/profile.d/java.sh
source /etc/profile.d/java.sh

echo $JAVA_HOME

&amp;gt; /usr/java/jdk1.8.0_60

java -version

&amp;gt; java version &amp;quot;1.8.0_60&amp;quot;
&amp;gt; Java(TM) SE Runtime Environment (build 1.8.0_60-b27)
&amp;gt; Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para completar o ambiente de execução, instalamos os serviços e bibliotecas necessárias.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;yum install -y tar openssh-clients openssh-server rsync gzip zlib openssl fuse bzip2 snappy

&amp;gt; (...)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(configuração do SSH para acesso sem senha)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;service sshd start
chkconfig sshd on

su - hadoop

ssh-keygen -C hadoop -P &#39;&#39; -f ~/.ssh/id_rsa
cp ~/.ssh/{id_rsa.pub,authorized_keys}

ssh-keyscan grandesdados-hadoop &amp;gt;&amp;gt;  ~/.ssh/known_hosts
ssh-keyscan localhost &amp;gt;&amp;gt; ~/.ssh/known_hosts
ssh-keyscan 127.0.0.1 &amp;gt;&amp;gt; ~/.ssh/known_hosts
ssh-keyscan 0.0.0.0 &amp;gt;&amp;gt; ~/.ssh/known_hosts

ssh grandesdados-hadoop

&amp;gt; Warning: Permanently added the RSA host key for IP address &#39;172.17.0.12&#39; to the list of known hosts.
&amp;gt; (nova shell, sem login nem confirmação)

# (sair do shell do ssh)
exit
# (sair do shell do su)
exit

whoami

&amp;gt; root
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;hadoop&#34;&gt;Hadoop&lt;/h3&gt;

&lt;p&gt;Procedimento para configuração local do Hadoop em modo pseudo-distribuído com uma JVM por serviço.&lt;/p&gt;

&lt;p&gt;Esse procedimento é baseado na &lt;a href=&#34;http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/SingleCluster.html&#34;&gt;documentação do Hadoop&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Serviços:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;HDFS: NameNode, SecondaryNameNode, DataNode&lt;/li&gt;
&lt;li&gt;YARN: ResouceManager, NodeManager&lt;/li&gt;
&lt;li&gt;MR: HistoryServer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instalação&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;O pacote usado nesse procedimento é o Hadoop 2.7.1 para CentOS6 descrito outro &lt;a href=&#34;http://cirocavani.github.io/post/compilacao-do-hadoop-para-centos6-rhel6-usando-docker/&#34;&gt;artigo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Primeiramente, colocamos o pacote dentro do container.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# (shell fora do container)
sudo docker cp hadoop-2.7.1.tar.gz grandesdados-hadoop:/hadoop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;De volta ao container como usuário root.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;tar zxf hadoop-2.7.1.tar.gz -C /opt
chown hadoop:hadoop -R /opt/hadoop-2.7.1

echo &#39;export PATH=$PATH:/opt/hadoop-2.7.1/bin:/opt/hadoop-2.7.1/sbin&#39; &amp;gt; /etc/profile.d/hadoop.sh
source /etc/profile.d/hadoop.sh

hadoop version

&amp;gt; Hadoop 2.7.1
&amp;gt; Subversion Unknown -r Unknown
&amp;gt; Compiled by hadoop on 2015-09-01T00:30Z
&amp;gt; Compiled with protoc 2.5.0
&amp;gt; From source with checksum fc0a1a23fc1868e4d5ee7fa2b28a58a
&amp;gt; This command was run using /opt/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar

mkdir -p /data/hadoop
chown hadoop:hadoop /data/hadoop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Configuração&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(para a configuração, deve ser usado o usuário hadoop: &lt;code&gt;su - hadoop&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;Editar &lt;code&gt;/opt/hadoop-2.7.1/etc/hadoop/core-site.xml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;/data/hadoop&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hdfs://grandesdados-hadoop&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Editar &lt;code&gt;/opt/hadoop-2.7.1/etc/hadoop/hdfs-site.xml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.blocksize&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;8M&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Editar &lt;code&gt;/opt/hadoop-2.7.1/etc/hadoop/yarn-site.xml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Editar &lt;code&gt;/opt/hadoop-2.7.1/etc/hadoop/mapred-site.xml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;mapreduce.jobtracker.staging.root.dir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;/user&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Setup Inicial (antes da primeira inicialização).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;hdfs namenode -format

&amp;gt; 15/09/16 02:12:03 INFO namenode.NameNode: STARTUP_MSG:
&amp;gt; /************************************************************
&amp;gt; STARTUP_MSG: Starting NameNode
&amp;gt; STARTUP_MSG:   host = grandesdados-hadoop/172.17.0.12
&amp;gt; STARTUP_MSG:   args = [-format]
&amp;gt; STARTUP_MSG:   version = 2.7.1
&amp;gt; (...)
&amp;gt; INFO namenode.NameNode: createNameNode [-format]
&amp;gt; Formatting using clusterid: CID-5daa32a0-3ab6-405e-bfd2-05c0a6e1e7e6
&amp;gt; (...)
&amp;gt; INFO common.Storage: Storage directory /data/hadoop/dfs/name has been successfully formatted.
&amp;gt; (...)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;HDFS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(como usuário hadoop &lt;code&gt;su - hadoop&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;start-dfs.sh

&amp;gt; Starting namenodes on [grandesdados-hadoop]
&amp;gt; grandesdados-hadoop: starting namenode, logging to /opt/hadoop-2.7.1/logs/hadoop-hadoop-namenode-grandesdados-hadoop.out
&amp;gt; localhost: starting datanode, logging to /opt/hadoop-2.7.1/logs/hadoop-hadoop-datanode-grandesdados-hadoop.out
&amp;gt; Starting secondary namenodes [0.0.0.0]
&amp;gt; 0.0.0.0: starting secondarynamenode, logging to /opt/hadoop-2.7.1/logs/hadoop-hadoop-secondarynamenode-grandesdados-hadoop.out

# criação do diretório do usuário hadoop
hdfs dfs -mkdir -p /user/hadoop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interface Web do Name Node:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://grandesdados-hadoop:50070/&#34;&gt;http://grandesdados-hadoop:50070/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Interface Web do Data Node (vazia):&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://grandesdados-hadoop:50075/&#34;&gt;http://grandesdados-hadoop:50075/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Interface Web do Secondary Name Node:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://grandesdados-hadoop:50090/&#34;&gt;http://grandesdados-hadoop:50090/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Para parar o serviço:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;stop-dfs.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;YARN&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(como usuário hadoop &lt;code&gt;su - hadoop&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;start-yarn.sh

&amp;gt; starting yarn daemons
&amp;gt; starting resourcemanager, logging to /opt/hadoop-2.7.1/logs/yarn-hadoop-resourcemanager-grandesdados-hadoop.out
&amp;gt; localhost: starting nodemanager, logging to /opt/hadoop-2.7.1/logs/yarn-hadoop-nodemanager-grandesdados-hadoop.out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interface Web do Resource Manager:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://grandesdados-hadoop:8088/&#34;&gt;http://grandesdados-hadoop:8088/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Interface Web do Node Manager:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://grandesdados-hadoop:8042/&#34;&gt;http://grandesdados-hadoop:8042/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Para parar o serviço:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;stop-yarn.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;History Server&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(como usuário hadoop &lt;code&gt;su - hadoop&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mr-jobhistory-daemon.sh start historyserver

&amp;gt; starting historyserver, logging to /opt/hadoop-2.7.1/logs/mapred-hadoop-historyserver-grandesdados-hadoop.out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interface Web do History Server:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://grandesdados-hadoop:19888/&#34;&gt;http://grandesdados-hadoop:19888/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Para parar o serviço:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mr-jobhistory-daemon.sh stop historyserver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Teste&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(para os testes, deve ser usado o usuário hadoop: &lt;code&gt;su - hadoop&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;Processos:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ps x

&amp;gt;   PID TTY      STAT   TIME COMMAND
&amp;gt;  5162 ?        S      0:00 -bash
&amp;gt;  5327 ?        Sl     0:08 /usr/java/jdk1.8.0_60/bin/java -Dproc_namenode (...)
&amp;gt;  5423 ?        Sl     0:07 /usr/java/jdk1.8.0_60/bin/java -Dproc_datanode (...)
&amp;gt;  5612 ?        Sl     0:06 /usr/java/jdk1.8.0_60/bin/java -Dproc_secondarynamenode (...)
&amp;gt;  5772 ?        Sl     0:08 /usr/java/jdk1.8.0_60/bin/java -Dproc_resourcemanager (...)
&amp;gt;  5870 ?        Sl     0:07 /usr/java/jdk1.8.0_60/bin/java -Dproc_nodemanager (...)
&amp;gt;  6189 ?        Sl     0:08 /usr/java/jdk1.8.0_60/bin/java -Dproc_historyserver (...)
&amp;gt;  6273 ?        R+     0:00 ps x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Exemplos de MapReduce:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;yarn jar /opt/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar

&amp;gt; An example program must be given as the first argument.
&amp;gt; Valid program names are:
&amp;gt;   aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.
&amp;gt;   aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.
&amp;gt;   bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.
&amp;gt;   dbcount: An example job that count the pageview counts from a database.
&amp;gt;   distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.
&amp;gt;   grep: A map/reduce program that counts the matches of a regex in the input.
&amp;gt;   join: A job that effects a join over sorted, equally partitioned datasets
&amp;gt;   multifilewc: A job that counts words from several files.
&amp;gt;   pentomino: A map/reduce tile laying program to find solutions to pentomino problems.
&amp;gt;   pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.
&amp;gt;   randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.
&amp;gt;   randomwriter: A map/reduce program that writes 10GB of random data per node.
&amp;gt;   secondarysort: An example defining a secondary sort to the reduce.
&amp;gt;   sort: A map/reduce program that sorts the data written by the random writer.
&amp;gt;   sudoku: A sudoku solver.
&amp;gt;   teragen: Generate data for the terasort
&amp;gt;   terasort: Run the terasort
&amp;gt;   teravalidate: Checking results of terasort
&amp;gt;   wordcount: A map/reduce program that counts the words in the input files.
&amp;gt;   wordmean: A map/reduce program that counts the average length of the words in the input files.
&amp;gt;   wordmedian: A map/reduce program that counts the median length of the words in the input files.
&amp;gt;   wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Rodando o Cálculo do Pi com MapReduce:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;yarn jar /opt/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar pi 16 100000

&amp;gt; Number of Maps  = 16
&amp;gt; Samples per Map = 100000
&amp;gt; (...)
&amp;gt; INFO impl.YarnClientImpl: Submitted application application_1442439610364_0001
&amp;gt; INFO mapreduce.Job: The url to track the job: http://grandesdados-hadoop:8088/proxy/application_1442439610364_0001/
&amp;gt; INFO mapreduce.Job: Running job: job_1442439610364_0001
&amp;gt; (...)
&amp;gt; Job Finished in 48.333 seconds
&amp;gt; Estimated value of Pi is 3.14157500000000000000
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;zookeeper&#34;&gt;ZooKeeper&lt;/h3&gt;

&lt;p&gt;Esse procedimento é baseado na &lt;a href=&#34;https://zookeeper.apache.org/doc/r3.4.6/zookeeperStarted.html&#34;&gt;documentação do ZooKeeper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Dentro do container como usuário root:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -L -O http://archive.apache.org/dist/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz
tar zxf zookeeper-3.4.6.tar.gz -C /opt
chown hadoop:hadoop -R /opt/zookeeper-3.4.6

echo &#39;export PATH=$PATH:/opt/zookeeper-3.4.6/bin&#39; &amp;gt; /etc/profile.d/zookeeper.sh
source /etc/profile.d/zookeeper.sh

mkdir -p /data/zookeeper
chown hadoop:hadoop /data/zookeeper
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Editar &lt;code&gt;/opt/zookeeper-3.4.6/conf/zoo.cfg&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;tickTime=6000
dataDir=/data/zookeeper
clientPort=2181
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inicializando o serviço:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;su - hadoop
zkServer.sh start

&amp;gt; JMX enabled by default
&amp;gt; Using config: /opt/zookeeper-3.4.6/bin/../conf/zoo.cfg
&amp;gt; Starting zookeeper ... STARTED
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para parar o serviço:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;zkServer.sh stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Teste&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(para os testes, deve ser usado o usuário hadoop: &lt;code&gt;su - hadoop&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;Processo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ps x | grep zoo

&amp;gt; 8246 ?        Sl     0:00 /usr/java/jdk1.8.0_60/bin/java (...) org.apache.zookeeper.server.quorum.QuorumPeerMain (...)
&amp;gt; 8291 ?        S+     0:00 grep zoo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Telnet:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;echo &#39;ruok&#39; |  curl telnet://grandesdados-hadoop:2181

&amp;gt; imok
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cliente:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;zkCli.sh -server grandesdados-hadoop:2181

&amp;gt; Connecting to grandesdados-hadoop:2181
&amp;gt; ...
&amp;gt; [zk: grandesdados-hadoop:2181(CONNECTED) 0] ls /
&amp;gt; [zookeeper]
&amp;gt; [zk: grandesdados-hadoop:2181(CONNECTED) 1] help
&amp;gt; ZooKeeper -server host:port cmd args
&amp;gt; 	stat path [watch]
&amp;gt; 	set path data [version]
&amp;gt; 	ls path [watch]
&amp;gt; 	delquota [-n|-b] path
&amp;gt; 	ls2 path [watch]
&amp;gt; 	setAcl path acl
&amp;gt; 	setquota -n|-b val path
&amp;gt; 	history
&amp;gt; 	redo cmdno
&amp;gt; 	printwatches on|off
&amp;gt; 	delete path [version]
&amp;gt; 	sync path
&amp;gt; 	listquota path
&amp;gt; 	rmr path
&amp;gt; 	get path [watch]
&amp;gt; 	create [-s] [-e] path data acl
&amp;gt; 	addauth scheme auth
&amp;gt; 	quit
&amp;gt; 	getAcl path
&amp;gt; 	close
&amp;gt; 	connect host:port
&amp;gt; [zk: grandesdados-hadoop:2181(CONNECTED) 3] quit
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;hbase&#34;&gt;HBase&lt;/h3&gt;

&lt;p&gt;Esse procedimento é baseado na &lt;a href=&#34;http://hbase.apache.org/book.html#quickstart&#34;&gt;documentação do HBase&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Dentro do container como usuário root:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -L -O http://archive.apache.org/dist/hbase/1.1.2/hbase-1.1.2-bin.tar.gz
tar zxf hbase-1.1.2-bin.tar.gz -C /opt
chown hadoop:hadoop -R /opt/hbase-1.1.2

echo &#39;export PATH=$PATH:/opt/hbase-1.1.2/bin&#39; &amp;gt; /etc/profile.d/hbase.sh
source /etc/profile.d/hbase.sh

mkdir -p /data/hbase/tmp
chown hadoop:hadoop -R /data/hbase
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Editar &lt;code&gt;/opt/hbase-1.1.2/conf/hbase-site.xml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.cluster.distributed&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hdfs:///hbase&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.tmp.dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/data/hbase/tmp&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.zookeeper.quorum&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;grandesdados-hadoop&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Editar &lt;code&gt;/opt/hbase-1.1.2/conf/hbase-env.sh&lt;/code&gt;:
&lt;br/&gt;(manter conteúdo original, só alterar os valores abaixo)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export HBASE_OPTS=&amp;quot;-XX:+UseConcMarkSweepGC -Djava.net.preferIPv4Stack=true&amp;quot;
export HBASE_MANAGES_ZK=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inicializando o serviço:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;su - hadoop
start-hbase.sh

&amp;gt; starting master, logging to /opt/hbase-1.1.2/bin/../logs/hbase-hadoop-master-grandesdados-hadoop.out
&amp;gt; Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0
&amp;gt; Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0
&amp;gt; starting regionserver, logging to /opt/hbase-1.1.2/bin/../logs/hbase-hadoop-1-regionserver-grandesdados-hadoop.out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interface Web do Master:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://grandesdados-hadoop:16010/&#34;&gt;http://grandesdados-hadoop:16010/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Interface Web do Region Server:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://grandesdados-hadoop:16301/&#34;&gt;http://grandesdados-hadoop:16301/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Para parar o serviço:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;stop-hbase.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Teste&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(para os testes, deve ser usado o usuário hadoop: &lt;code&gt;su - hadoop&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;Processo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ps x | grep hbase

&amp;gt; 8790 ?        S      0:00 bash /opt/hbase-1.1.2/bin/hbase-daemon.sh --config /opt/hbase-1.1.2/bin/../conf foreground_start master
&amp;gt; 8804 ?        Sl     0:14 /usr/java/jdk1.8.0_60/bin/java -Dproc_master (...)
&amp;gt; 8915 ?        S      0:00 bash /opt/hbase-1.1.2/bin/hbase-daemon.sh --config /opt/hbase-1.1.2/bin/../conf foreground_start regionserver -D hbase.regionserver.port=16201 -D hbase.regionserver.info.port=16301
&amp;gt; 8929 ?        Sl     0:14 /usr/java/jdk1.8.0_60/bin/java -Dproc_regionserver (...)
&amp;gt; 9329 ?        S+     0:00 grep hbase
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cliente:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;hbase shell

&amp;gt; HBase Shell; enter &#39;help&amp;lt;RETURN&amp;gt;&#39; for list of supported commands.
&amp;gt; Type &amp;quot;exit&amp;lt;RETURN&amp;gt;&amp;quot; to leave the HBase Shell
&amp;gt; Version 1.1.2, rcc2b70cf03e3378800661ec5cab11eb43fafe0fc, Wed Aug 26 20:11:27 PDT 2015
&amp;gt;
&amp;gt; hbase(main):001:0&amp;gt; status
&amp;gt; 1 servers, 0 dead, 2.0000 average load
&amp;gt;
&amp;gt; hbase(main):002:0&amp;gt; help
&amp;gt; HBase Shell, version 1.1.2, rcc2b70cf03e3378800661ec5cab11eb43fafe0fc, Wed Aug 26 20:11:27 PDT 2015
&amp;gt; Type &#39;help &amp;quot;COMMAND&amp;quot;&#39;, (e.g. &#39;help &amp;quot;get&amp;quot;&#39; -- the quotes are necessary) for help on a specific command.
&amp;gt; Commands are grouped. Type &#39;help &amp;quot;COMMAND_GROUP&amp;quot;&#39;, (e.g. &#39;help &amp;quot;general&amp;quot;&#39;) for help on a command group.
&amp;gt; (...)
&amp;gt; hbase(main):004:0&amp;gt; exit
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;kafka&#34;&gt;Kafka&lt;/h3&gt;

&lt;p&gt;Esse procedimento é baseado na &lt;a href=&#34;http://kafka.apache.org/documentation.html#quickstart&#34;&gt;documentação do Kafka&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Dentro do container como usuário root:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -L -O http://archive.apache.org/dist/kafka/0.8.2.1/kafka_2.10-0.8.2.1.tgz
tar zxf kafka_2.10-0.8.2.1.tgz -C /opt
chown hadoop:hadoop -R /opt/kafka_2.10-0.8.2.1

echo &#39;export PATH=$PATH:/opt/kafka_2.10-0.8.2.1/bin&#39; &amp;gt; /etc/profile.d/kafka.sh
source /etc/profile.d/kafka.sh

mkdir -p /data/kafka
chown hadoop:hadoop /data/kafka
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Editar &lt;code&gt;/opt/kafka_2.10-0.8.2.1/config/server.properties&lt;/code&gt;:
&lt;br/&gt;(manter conteúdo original, só alterar os valores abaixo)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;log.dirs=/data/kafka
zookeeper.connect=grandesdados-hadoop:2181
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inicializando o serviço:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;su - hadoop
kafka-server-start.sh /opt/kafka_2.10-0.8.2.1/config/server.properties &amp;amp;&amp;gt; kafka.out &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para parar o serviço:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kafka-server-stop.sh /opt/kafka_2.10-0.8.2.1/config/server.properties
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Teste&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(para os testes, deve ser usado o usuário hadoop: &lt;code&gt;su - hadoop&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;Processo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ps x | grep kafka

&amp;gt; 9818 ?        Sl     0:03 /usr/java/jdk1.8.0_60/bin/java (...) kafka.Kafka /opt/kafka_2.10-0.8.2.1/config/server.properties
&amp;gt; 9928 ?        S+     0:00 grep kafka
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Enviando e recebendo mensagens:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kafka-topics.sh \
--create \
--zookeeper grandesdados-hadoop:2181 \
--replication-factor 1 \
--partitions 1 \
--topic test

&amp;gt; Created topic &amp;quot;test&amp;quot;.

echo &#39;Primeira mensagem de teste&#39; | kafka-console-producer.sh --broker-list grandesdados-hadoop:9092 --topic test

&amp;gt; (...)

kafka-console-consumer.sh --zookeeper grandesdados-hadoop:2181 --topic test --from-beginning

&amp;gt; Primeira mensagem de teste
&amp;gt; ^CConsumed 1 messages
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusão&#34;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;A revolução em BigData é um fenômeno da tecnologia desenvolvida ao longo dos últimos anos focada na manipulação de um grande volume de dados em máquinas de baixo custo. Essa é a tecnologia que torna possível combinar uma solução de dados escalável com processos para geração de resultados relevantes, tanto no desenvolvimento de produtos quanto na evolução do conhecimento. O importante é entender como essa tecnologia pode ser usada para agregar valor ao negócio e permitir imaginar soluções inovadoras.&lt;/p&gt;

&lt;p&gt;Esse artigo documenta o passo-a-passo de uma configuração local do Hadoop, ZooKeeper, HBase e Kafka. Esses são os serviços essenciais em uma plataforma de BigData que, juntamente com o Spark, possibilitam o desenvolvimento de soluções tanto para processamento batch quanto para tempo real.&lt;/p&gt;

&lt;p&gt;Em artigos futuros, entrarei usando essa solução para desenvolver e testar algumas aplicações de BigData usando Spark.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BigData na Globo.com</title>
      <link>http://cirocavani.github.io/post/bigdata-na-globocom/</link>
      <pubDate>Thu, 27 Aug 2015 06:00:00 -0300</pubDate>
      
      <guid>http://cirocavani.github.io/post/bigdata-na-globocom/</guid>
      <description>

&lt;p&gt;A proposta desse artigo é fundamentar alguns conceitos de BigData e explorar a dinâmica de como tratar um grande volume de dados para extrair valor. A ideia é apresentar a solução de dados na Plataforma de BigData da Globo.com usada pelo Sistema de Recomendação e comentar a experiência do seu desenvolvimento.&lt;/p&gt;

&lt;p&gt;Esse artigo é uma atualização e expansão da palestra realizada no Rio BigData Meetup em 21 de Outubro de 2014.&lt;/p&gt;

&lt;p&gt;Os slides originais dessa palestra podem ser acessados &lt;a href=&#34;http://www.slideshare.net/cirocavani/rio-big-data-meetup-20141021&#34;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;personalização&#34;&gt;Personalização&lt;/h2&gt;

&lt;p&gt;A Globo.com é a empresa de Internet do Grupo Globo e tem alguns dos maiores portais do Brasil (G1, Globo Esporte, GShow e Vídeos). São dezenas de milhões de acessos por dia aos portais com cerca de 10 milhões de visitantes únicos e uma produção massiva de conteúdo bastante variado em Jornalismo, Esporte, Variedades e Vídeos (juntamente com TV Globo, Editora Globo e demais empresas do Grupo).&lt;/p&gt;

&lt;p&gt;Com a proposta de personalizar o conteúdo da Globo.com nos diversos produtos que formam cada portal, o time de Personalização foi criado a um pouco mais de 2 anos e meio. Esse time recebeu o desafio de implementar um Sistema de Recomendação que pudesse recomendar para milhões de usuários ativos milhares de itens variados (notícias, vídeos, filmes, &amp;hellip;).&lt;/p&gt;

&lt;p&gt;A Plataforma de BigData da Globo.com surgiu dessa experiência.&lt;/p&gt;

&lt;h3 id=&#34;recomendação&#34;&gt;Recomendação&lt;/h3&gt;

&lt;p&gt;O Sistema de Recomendação é projetado para coletar os sinais produzidos tanto pela dinâmica do usuário quanto pela dinâmica do conteúdo e filtrar o que é relevante para o usuário no momento em que isso é importante.&lt;/p&gt;

&lt;p&gt;Nesse sistema, os dados coletados são processados e transformados em modelos de Usuário, Conteúdo e Contexto. Esses modelos são usados para apresentar as melhores opções de acordo com parâmetros do Produto. A interação do usuário com o conteúdo recomendado é avaliada através de testes usados para orientar constantes mudanças em busca de melhorar a performance do sistema.&lt;/p&gt;

&lt;p&gt;A plataforma desenvolvida na Globo.com para recomendação personalizada de conteúdo é projetada para suportar milhões de usuários ativos, milhares de itens variados em fluxo, diversos contextos de produtos.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://cirocavani.github.io/images/posts/bigdata_na_globocom/recsys.png&#34; alt=&#34;Sistema de Recomendação na Globo.com&#34; style=&#34;width: 100%; padding:1px; border:1px solid #021a40;&#34;/&gt;&lt;/p&gt;

&lt;p&gt;Essa plataforma pode ser dividida em três partes: recebimento de dados em tempo real, processamento de algoritmos de recomendação, e; consulta on-line com combinação de modelos.&lt;/p&gt;

&lt;p&gt;O processo de recebimento de dados em tempo real é baseado no Kafka, um cluster de distribuição de mensagens de alta performance que suporta bilhões de mensagens por dia. Esse sistema é usado para receber todos os sinais coletados sobre o usuário possibilitando o processamento stream em tempo real. Eventualmente, esse stream é armazenado permanentemente no cluster Hadoop para processamento em lote e análise.&lt;/p&gt;

&lt;p&gt;O processo dos algoritmos de recomendação varia para cada caso. Um caso típico é o processamento em três etapas no cluster Hadoop. Na primeira etapa, é feito um condicionamento de dados novos, seja para transformar em um formato específico ou para atualizar um pré-modelo gerado na última execução. Na segunda etapa, é executado o algoritmo de recomendação. Na terceira etapa, o resultado do algoritmo é verificado e transformado no formato que pode ser consultado. Normalmente a execução é condicionada por vários parâmetros e é comum ter diferentes configurações do mesmo algoritmo em teste.&lt;/p&gt;

&lt;p&gt;A consulta on-line é a API que os desenvolvedores da Globo.com podem usar para recomendar conteúdo personalizado. A API faz a combinação dos diversos modelos de recomendação e filtra o conteúdo mais relevante para o usuário. A API permite criar testes A/B com combinações diferentes dos modelos permitindo avaliar a performance de um algoritmo contra outro, ou o mesmo com parâmetros diferentes.&lt;/p&gt;

&lt;p&gt;Um exemplo de como esse sistema funciona:&lt;br/&gt;
(esse exemplo captura a essência, mas é só uma ilustração)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Collaborative Filtering por Fatores Latentes usando Spark&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Toda vez que uma página da Globo.com é visitada, é gerado um sinal de pageview com o id do usuário, a URL e o timestamp. Esse sinal passa pelo Kafka e é armazenado no Hadoop. Um job Spark varre todos os sinais não processados na última execução (timestamp) gerando um arquivo com o formato user-item-rating (rating = número de visitas, por simplicidade) correspondendo a matriz de Preferências. Outro job Spark é executado para processar a matriz de Preferências, fazer a fatoração e gerar os vetores de fatores latentes por usuário e item. Os fatores latentes são usados para calcular os scores (ratings) não observados (itens não vistos pelo usuário) e os 30 maiores são armazenados no HBase. Quando configurada para esse algoritmo, a API consulta o HBase e usa a lista com score para combinar com outros resultados e fazer a recomendação.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;tecnologia&#34;&gt;Tecnologia&lt;/h2&gt;

&lt;p&gt;A revolução em BigData é um fenômeno da tecnologia desenvolvida ao longo dos últimos anos focada na manipulação de um grande volume de dados em máquinas de baixo custo. Essa é a tecnologia que torna possível combinar uma solução de dados escalável com processos para geração de resultados relevantes, tanto no desenvolvimento de produtos quanto na evolução do conhecimento. O importante é entender como essa tecnologia pode ser usada para agregar valor ao negócio e permitir imaginar soluções inovadoras.&lt;/p&gt;

&lt;p&gt;A Globo.com tem Data Center próprio e a instalação do cluster de BigData foi feito em máquinas físicas. Esse cluster é composto por um conjunto maior de máquinas com Hadoop e HBase e outro conjunto menor de máquinas com Kafka e ZooKeeper. O principal framework que usamos é o Spark, mas ainda temos alguns scripts Pig de processos antigos (legado). É um cluster de média capacidade que deve crescer com o tempo / necessidades.&lt;/p&gt;

&lt;p&gt;Em artigos futuros, entrarei em mais detalhes sobre como usar essas tecnologias.&lt;/p&gt;

&lt;h3 id=&#34;hadoop&#34;&gt;Hadoop&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://cirocavani.github.io/images/posts/bigdata_na_globocom/hadoop-logo.jpg&#34; alt=&#34;Hadoop logo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;O Hadoop torna o custo por byte de armazenamento / processamento marginal e por isso torna BigData possível na escala de hoje. O problema que o Hadoop resolve é de otimizar a utilização de múltiplas máquinas por vários processos.&lt;/p&gt;

&lt;p&gt;Com o Hadoop é possível ter escalabilidade horizontal, alta taxa de utilização (CPU/Memória) e tolerância a falhas com máquinas de baixo custo para armazenar e processar dados. Um processo que é dimensionado inicialmente para um certo volume pode ser escalado colocando mais máquinas no cluster, muitos processos podem compartilhar esse cluster sem preocupação de sobrecarga, ociosidade ou interferência e a perda de máquinas ou falhas de disco não resultam em indisponibilidade ou perda de dado.&lt;/p&gt;

&lt;p&gt;O paradigma essencial é partir da premissa de que máquinas de baixo custo não são confiáveis e que é mais eficiente levar o processamento para onde o dado fica armazenado do que movimentar dados. O Hadoop é um sistema que foi desenvolvido com essa essência.&lt;/p&gt;

&lt;p&gt;O Hadoop é um sistema de arquivos distribuído (HDFS) e um sistema de execução distribuído (YARN), de outra forma, é essencialmente um sistema operacional para um cluster de máquinas onde dados são armazenados e processados. O processamento de dados é feito através de aplicações que fazem uso das APIs desses sistemas, contudo elas são mais elementares. Existem vários frameworks que são abstrações dessas APIs e oferecem modelos de programação mais elaborados, com funcionalidades mais avançadas.&lt;/p&gt;

&lt;p&gt;O framework original do Hadoop é o MapReduce voltado para processamento em lote com grande latência. Pig é uma ferramenta que permite escrever scripts em uma linguagem alto nível de manipulação de dados; os scripts são compilado para Java (byte-code) e executados no Hadoop na forma de vários jobs MapReduce. Essa é a tecnologia que usamos no início de BigData na Globo (Recomendação e Busca). Contudo, MapReduce é uma tecnologia que está sendo abandonada - o Google, seu criador, anunciou há algum tempo que deixou de usar MapReduce e passou a usar a tecnologia do Dataflow (oferecida ao grande público como Cloud Dataflow).&lt;/p&gt;

&lt;p&gt;Por outro lado, a indústria também vem abandonando o MapReduce e, entre várias opções, duas tecnologias estão em evidência: Spark e Tez. O Spark apresenta uma API simples de operações sobre coleções de dados, internamente constrói um grafo de tarefas que são executadas em blocos de memória distribuídos; suporta SQL, Machine Learning, Streaming e Processamento de Grafos; está sendo usado como engine para diversas ferramentas analíticas (Mahout, por exemplo). O Tez é mais baixo nível e encapsula a construção de grafos de execução que podem ser otimizados dinamicamente; é usado principalmente como engine em ferramentas mais elaboradas (Hive e Pig, por exemplo). Uma terceira tecnologia que pode ganhar tração é o Flink, também segue o princípio de API simples, construção de plano de execução, otimização.&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;O HDFS é composto por dois serviços: o NameNode que mantém o índice de arquivos e o mapeamento desses arquivos para os blocos de dados distribuídos pelo cluster (roda em uma máquina); e o DataNode que armazena os blocos de arquivo (roda em todas as máquinas). Um arquivo pode ser dividido em vários blocos e cada bloco ainda é replicado em 3 DataNodes. O NameNode controla as operações de manipulação de arquivo, mas as operação de leitura e escrita são feitas diretamente com os DataNodes. A perda do DataNode não resulta em perda de dados e o número de réplicas é sempre ajustado. As réplicas também são usadas para o acesso local aos dados por processos paralelos.&lt;/p&gt;

&lt;p&gt;O YARN é composto por dois serviços: o ResourceManager que mantém registro e controla a alocação de memória e processador das máquinas do cluster (roda em uma máquina); e o NodeManager que controla os container de execução de uma aplicação (roda em todas as máquinas). Um terceiro serviço é o ApplicationMaster associado a cada aplicação que roda no cluster e negocia com o ResourceManager a alocação de recursos providas nos NodeManagers. Um framework como Spark roda como um ApplicationMaster que cria containers &amp;lsquo;workers&amp;rsquo; para processar os dados em memória. MapReduce segue esse mesmo princípio, criando containers de Mappers e Reducers. Também é possível rodar aplicações standalone como HBase e Kafka usando YARN.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Hadoop&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://hadoop.apache.org/&#34;&gt;https://hadoop.apache.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spark&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://spark.apache.org/&#34;&gt;https://spark.apache.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tez&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://tez.apache.org/&#34;&gt;https://tez.apache.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Flink&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://flink.apache.org/&#34;&gt;https://flink.apache.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MapReduce and Spark&lt;/strong&gt; (30/Dez/2013)&lt;br/&gt;
&lt;a href=&#34;http://vision.cloudera.com/mapreduce-spark/&#34;&gt;http://vision.cloudera.com/mapreduce-spark/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Google Cound Dataflow&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://cloud.google.com/dataflow/&#34;&gt;https://cloud.google.com/dataflow/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Google File System&lt;/strong&gt; (Out/2003)&lt;br/&gt;
&lt;a href=&#34;http://research.google.com/archive/gfs.html&#34;&gt;http://research.google.com/archive/gfs.html&lt;/a&gt;&lt;br/&gt;
&lt;a href=&#34;http://research.google.com/archive/gfs-sosp2003.pdf&#34;&gt;http://research.google.com/archive/gfs-sosp2003.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MapReduce: Simplified Data Processing on Large Clusters&lt;/strong&gt; (Dez/2004)&lt;br/&gt;
&lt;a href=&#34;http://research.google.com/archive/mapreduce.html&#34;&gt;http://research.google.com/archive/mapreduce.html&lt;/a&gt;&lt;br/&gt;
&lt;a href=&#34;http://research.google.com/archive/mapreduce-osdi04.pdf&#34;&gt;http://research.google.com/archive/mapreduce-osdi04.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Apache Hadoop YARN: yet another resource negotiator&lt;/strong&gt; (Out/2013)&lt;br/&gt;
&lt;a href=&#34;http://www.socc2013.org/home/program/a5-vavilapalli.pdf&#34;&gt;http://www.socc2013.org/home/program/a5-vavilapalli.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;hbase&#34;&gt;HBase&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://cirocavani.github.io/images/posts/bigdata_na_globocom/hbase-logo.png&#34; alt=&#34;HBase logo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;O HBase é um banco de dados sem esquema que suporta bilhões de linhas por tabela. Inspirado no paper BigTable do Google, suporta uma estrutura chave-valor ordenada, multidimensional, esparsa, distribuída e persistente. O principal benefício é o suporte para operação eficiente com pequenos blocos de bytes em complemento ao HDFS que opera melhor com grandes blocos.&lt;/p&gt;

&lt;p&gt;O HBase opera como um cluster distribuído tolerante a falhas. A leitura e escrita são consistentes, a consulta pela chave tem baixa latência e suporta um grande número de operações concorrentes. Contudo, sua API é limitada a operações simples (put, get, scan, array de bytes) e estendida com processadores executados dentro do cluster (filter, coprocessor). Não tem suporte nativo para linguagem de consulta e também não suporta transação multi-chave.&lt;/p&gt;

&lt;p&gt;Estruturar os dados no HBase conciliando performance de escrita, performance de leitura e representação expressiva é um grande desafio. Os dados tem índice único pela chave da tabela e a performance depende da distribuição dessas chaves entre os servidores.&lt;/p&gt;

&lt;p&gt;O Phoenix é uma solução complementar ao HBase com suporte a índices secundários e consultas SQL. O Phoenix opera dentro do HBase (coprocessador) e do lado da aplicação, fazendo a intermediação das operações de leitura e escrita condicionando os dados. A modelagem de dados fica acoplada com a ferramenta. A API para aplicação que usa o Phoenix é um driver JDBC, incluindo suporte para esquema e dados &amp;lsquo;tipados&amp;rsquo; (não só bytes).&lt;/p&gt;

&lt;p&gt;O Spark tem suporte ao HBase através da API de Input/Output padrão do Hadoop ou direto com a API do HBase. Ou seja, são operações simples para leitura e escrita, mas não consulta mais elaboradas. O HBase ainda não é suportado pelo Spark SQL / Catalyst, framework para processamento de dados estruturados com suporte a execução e otimização de consultas SQL e operações em tabela (DataFrame). Esse é um projeto em andamento e vai possibilitar que consultas complexas sejam otimizadas, minimizando a movimentação de dados e transferindo parte do processamento para o HBase.&lt;/p&gt;

&lt;p&gt;O HBase tem papel fundamental como intermediário entre as aplicações que fazem processamento de grandes volumes de dados e aplicações que fazem acesso a dados específicos. O caso de uso principal em que usamos o HBase é para o armazenamento de resultados que podem ser usados externamente ao Hadoop. Isso diminui a movimentação de grande volumes de dados para sistemas externos e mantém a baixa latência necessária para uso externo.&lt;/p&gt;

&lt;p&gt;O Google, criador do BigTable que inspirou o HBase, acabou &amp;lsquo;desistindo&amp;rsquo; dessa tecnologia e implementando um sistema mais complexo com suporte a operações transacionais em larga escala. Esse sistema chama-se Spanner e tem muitos dos princípios originais do BigTable, mas com suporte a serialização global de operações. Para manter esse sincronismo, são usados relógios atômicos e GPS de precisão. Apesar de ser um &amp;lsquo;sucessor&amp;rsquo; do BigTable no Google, dificilmente vai tornar obsoleto o HBase ou inspirar um novo banco (eu posso estar errado aqui!).&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;O HBase é composto por dois serviços: o HMaster monitora os RegionServers, distribui as Regiões de Dados (Splits, bloco de Linhas de uma Tabela) e recebe operações de DDL, e; o RegionServer mantém Regiões de Dados em um DataNode / HDFS, recebe acesso direto dos Clientes para operação de leitura/escrita. O ZooKeeper é usado para metadados / configurações. Escalabilidade através da adição de servidores (RegionServers).&lt;/p&gt;

&lt;p&gt;O RegionServer tem um WAL (Write Ahead Log), um BlockCache e várias Regiões de Dados. Cada Região tem vários Armazenamentos (Store), um por Família, cada um com vários arquivos (HFile) e um Armazenamento em Memória (MemStore). O WAL e os HFiles são persistidos no HDFS. Os arquivos de dados (HFiles) são reescritos de tempos em tempos para obter localidade e ajustar automaticamente a performance.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;HBase&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://hbase.apache.org/&#34;&gt;https://hbase.apache.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phoenix&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://phoenix.apache.org/&#34;&gt;https://phoenix.apache.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spark SQL on HBase&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://github.com/Huawei-Spark/Spark-SQL-on-HBase&#34;&gt;https://github.com/Huawei-Spark/Spark-SQL-on-HBase&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Slider&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://slider.incubator.apache.org/&#34;&gt;https://slider.incubator.apache.org/&lt;/a&gt;&lt;br/&gt;
&lt;a href=&#34;https://wiki.apache.org/incubator/SliderProposal&#34;&gt;https://wiki.apache.org/incubator/SliderProposal&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bigtable: A Distributed Storage System for Structured Data&lt;/strong&gt; (Nov/2006)&lt;br/&gt;
&lt;a href=&#34;http://research.google.com/archive/bigtable.html&#34;&gt;http://research.google.com/archive/bigtable.html&lt;/a&gt;&lt;br/&gt;
&lt;a href=&#34;http://research.google.com/archive/bigtable-osdi06.pdf&#34;&gt;http://research.google.com/archive/bigtable-osdi06.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spanner: Google&amp;rsquo;s Globally-Distributed Database&lt;/strong&gt; (Out/2012)&lt;br/&gt;
&lt;a href=&#34;http://research.google.com/archive/spanner.html&#34;&gt;http://research.google.com/archive/spanner.html&lt;/a&gt;&lt;br/&gt;
&lt;a href=&#34;http://research.google.com/archive/spanner-osdi2012.pdf&#34;&gt;http://research.google.com/archive/spanner-osdi2012.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;kafka&#34;&gt;Kafka&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://cirocavani.github.io/images/posts/bigdata_na_globocom/kafka-logo.png&#34; alt=&#34;Kafka logo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;O Kafka é um cluster de distribuição de mensagens que escala para um grande número de produtores e consumidores. Criado pelo LinkedIn, o Kafka tornou-se a tecnologia fundamental para uma arquitetura de dados stream. Com o Kafka, é possível concentrar e distribuir um fluxo muito grande de dados que podem ser acessados tanto para processamento em tempo real quanto para processamento em lote. O principal benefício é o acesso direto ao dado quase no mesmo instante em que ele é gerado.&lt;/p&gt;

&lt;p&gt;O Kafka opera como um cluster distribuído tolerante a falhas. É baseado no modelo Publish-Subscribe e essencialmente funciona como um &amp;lsquo;append-only log&amp;rsquo;, com novas mensagens adicionadas no final de cada arquivo e leitura sequencial. As aplicações podem enviar mensagens para um determinado tópico; os servidores recebem essas mensagens e as armazenam em arquivos, e as aplicações podem pegar essas mensagens por tópico em qualquer momento (tempo real ou lote). Por &amp;lsquo;concepção&amp;rsquo;, as funcionalidades são bastante limitadas e orientadas para suportar um fluxo grande de mensagens.&lt;/p&gt;

&lt;p&gt;O Kafka se destaca em: Performance, com alto throughput no recebimento e distribuição; Escalabilidade, muitos consumidores, isolamento entre consumidores, e; Mensagens pequenas, não estruturadas / opacas (bytes).&lt;/p&gt;

&lt;p&gt;O Spark Streaming tem suporte ao Kafka como fonte de dados. São duas APIs, a de Receivers que consome constantemente mensagens e armazena na aplicação e a Direta que mapeia os offsets das mensagens e faz consumo por intervalo (do mini-batch) - a API Direta é mais específica e melhor com o Kafka. A partir do stream do Kafka, é possível processar os dados usando as funcionalidades sofisticadas do Spark. Um caso de uso é o armazenamento em lote de arquivos Parquet para consulta histórica. Outro exemplo é o processamento de janelas para a geração de métricas em tempo real (conversão, visualizações). O Spark é a ferramente ideal para o processamento e análise dos dados distribuídos pelo Kafka.&lt;/p&gt;

&lt;p&gt;Com o Kafka, a gente construiu um sistema de coleta de atividades. Seu propósito é tornar possível receber, distribuir e armazenar toda e qualquer informação sobre o usuário escalando para bilhões de mensagens por dia trafegando pelo sistema. Nesse sistema trafegam as páginas visitadas, vídeos assistidos, buscas, comentários, compartilhamentos, track da página, &amp;hellip; todos esses dados são processados e armazenados no HDFS no formato Parquet. Simultaneamente, esse dados são usados em vários outros sistemas, Recomendação e Busca entre eles. Essa arquitetura de dados vem se mostrando fundamental no desenvolvimento de soluções de dados inovadoras.&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;O Kafka é composto por três partes: o cluster de Brokers, a API do Produtor e a API do Consumidor. As APIs são usadas nas aplicações que se comunicam com o cluster e implementam o protocolo do Kafka. Implementações alternativas desse protocolo podem ser usadas para comunicação direta com o cluster.&lt;/p&gt;

&lt;p&gt;Os Brokers organizam as Mensagens em Tópicos, podem ficar em memória temporariamente e são sincronizadas para disco de tempos em tempos, mantidas por tempo determinado (também pode ser limitada por espaço). Os Tópicos são divididos em Partições, cada uma com um arquivo em disco diferente. Os Tópicos podem ser replicados em vários Broker. Cada Broker gerencia um grupo de Tópicos. A perda de um Broker ou a corrupção de um arquivo de Partição não resulta em perda dos dados. Os Tópicos são criados com um nome único, número de Partições e número de Réplicas.&lt;/p&gt;

&lt;p&gt;Produtores enviam Mensagens para um Tópico específico, para o qual é possível definir a Partição para distribuir a carga. Produtores também podem publicar mensagens de forma assíncrona, fazendo buffer em memória na aplicação. Consumidores precisam pegar mensagens de cada Partição, controlando o índice da mensagem consumida (offset). Ou seja, o número de Partições determina o paralelismo de consumo e o controle de consumo é responsabilidade da aplicação (o Broker não mantém controle do consumo).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kafka&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://kafka.apache.org/&#34;&gt;https://kafka.apache.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Building LinkedIn&amp;rsquo;s Real-time Activity Data Pipeline&lt;/strong&gt; (Jun/2012)&lt;br/&gt;
&lt;a href=&#34;http://sites.computer.org/debull/A12june/pipeline.pdf&#34;&gt;http://sites.computer.org/debull/A12june/pipeline.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Log: What every software engineer should know about real-time data&amp;rsquo;s unifying abstraction&lt;/strong&gt; (Dez/2013)&lt;br/&gt;
&lt;a href=&#34;http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying&#34;&gt;http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spark Streaming&lt;/strong&gt;&lt;br/&gt;
httpd://spark.apache.org/streaming/&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kafka on YARN (Slider)&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://github.com/DataTorrent/koya&#34;&gt;https://github.com/DataTorrent/koya&lt;/a&gt;&lt;br/&gt;
&lt;a href=&#34;https://slider.incubator.apache.org/&#34;&gt;https://slider.incubator.apache.org/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;spark&#34;&gt;Spark&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://cirocavani.github.io/images/posts/bigdata_na_globocom/spark-logo.png&#34; alt=&#34;Spark logo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;O Spark é um framework para processamento distribuído de grande quantidade de dados com foco em análise interativa. O Spark tem uma API simples que torna fácil escrever desde pequenas manipulações de dados, até algoritmos iterativos complexos, de forma escalável e com boas práticas de engenharia de software (testes). Essa característica o torna a ferramenta ideal de trabalho tanto para o Data Engineer que tem que desenvolver uma solução de dados, quanto para o Data Scientist que tem que desenvolver modelos e visualizações. Dessa forma, o Spark hoje é composto por um conjunto grande de funcionalidades com suporte DataFrame / SQL, Machine Learning, Streaming e Processamento de Grafos. Também é o projeto mais popular na área de BigData e cada nova versão adiciona ainda mais abrangência e inovação.&lt;/p&gt;

&lt;p&gt;O projeto começou em Berkeley (pesquisa) como alternativa ao MapReduce para processar dados no cluster usando memória das máquinas como armazenamento entre etapas e vem evoluindo para ser um framework completo de analise de dados. Já fazem parte do projeto: o Spark SQL / DataFrame para processamento de dados estruturados, tabular; Spark ML / MLlib com uma pipeline de processamento e algoritmos de Machine Learning; o Spark Streaming para processamento de stream, mini-batches, diversas fontes como o Kafka, e; o GraphX para processamento de grafo, algoritmos de rede. Esses componentes são construídos sobre a mesma API básica do Spark e podem ser usadas em conjunto. Da proposta original aos componentes avançados, o Spark substitui o MapReduce completamente (o Tez é outra abordagem para substituir o MapReduce).&lt;/p&gt;

&lt;p&gt;O MapReduce foi a solução original do Google para processamento de grande volume de dados com tolerância a falhas. O grande problema do MapReduce é a grande latência entre as diversas etapas de processamento que se baseiam em resultados intermediários armazenados no HDFS. Processamentos mais complexos são feitos através do encadeamento de vários jobs MapReduce (Pig faz isso). Esse modelo de execução escala bem, mas é bastante limitante para implementar algoritmos que fazem várias iterações sobre os mesmos dados, por exemplo. Esse é o cenário que fez o pessoal de Berkeley procurar uma nova solução que deu origem ao Spark.&lt;/p&gt;

&lt;p&gt;O Spark apresenta uma API simples de operações sobre coleções de dados (RDD), internamente constrói um grafo de tarefas (DAG) que são executadas em blocos de memória distribuídos (Blocks). A ideia básica é carregar os dados em coleções imutáveis e aplicar transformações que resultam em novas coleções, no final, usar uma ação para materializar os resultados (como salvar em disco). Esse encadeamento de ações forma o grafo das transformações, contudo, a execução as tarefas em si é feita sob-demanda (lazy), os resultados intermediários são mantidos em memória e a perda dessa informação resulta na reexecução das tarefas necessárias para regerar o dado (ao invés de quebrar a aplicação). Dessa forma, o Spark é capaz de representar o processamento através de tarefas e usar a memória para comunicação entre essas tarefas, falhas de execução ou limpeza de memória fazem o grafo ser reprocessado, mantendo a garantia de tolerância a falhas e resiliência (ponto forte do MapReduce).&lt;/p&gt;

&lt;p&gt;Na prática, o Spark facilita bastante o desenvolvimento de aplicações de processamento de dados mas ainda exige que o desenvolvedor dê grande atenção ao detalhes da aplicação, tendo que buscar soluções &amp;lsquo;eficientes&amp;rsquo; para pontos críticos só identificados com carga. Com Spark, é possível escrever testes (funcionais) que rodam na Integração Contínua validando o comportamento da aplicação, contudo, quando submetido a carga de produção, a aplicação pode apresentar muitos erros difíceis de identificar o motivo (problema de memória, demora excessiva). A solução costuma ser analisar os logs / histórico, procurar as ineficiências, ajustar parâmetros, particionar, cachear, mudar algoritmos e/ou a ordem em que os dados são processados. Ou seja, além de escrever um código correto, é preciso que o código seja &amp;lsquo;dimensionado&amp;rsquo; para os dados corretamente também.&lt;/p&gt;

&lt;p&gt;O pessoal do Spark vem trabalhando em várias abordagem para resolver esses problemas. Uma primeira abordagem é a alocação dinâmica de &amp;lsquo;worker&amp;rsquo; dando elasticidade a aplicação quando tem mais processamento e liberando recursos quando não é mais necessário. Uma segunda abordagem é oferecer uma abstração de dados mais estruturada que permite otimizar dinamicamente as manipulações de dados - o Spark introduziu o DataFrame que é uma tabela (ao invés de puramente uma coleção) que usa o Catalyst, um framework para processamento de dados estruturados que constrói um plano de execução e faz otimização para operações com a tabela e consultas SQL. Uma terceira abordagem é o esforço do Projeto Tungsten com objetivo trazer para o Spark funcionalidades de mais baixo nível, como representação compacta de dados em memória (evitando o GC da JVM), cache automático e geração dinâmica de código - nesse primeiro momento, o foco é o DataFrame / SQL. Por fim, de forma geral, uma abordagem é tornar a execução das tarefas &amp;lsquo;automaticamente&amp;rsquo; mais eficientes através de heurísticas da execução - esse é um projeto que deve frutificar nas próximas versões do Spark. Hoje a recomendação é usar o DataFrame e deixar o framework executar de forma otimizada, e usar o RDD (coleção) quando isso não funcionar.&lt;/p&gt;

&lt;p&gt;O Spark é o framework principal que a gente usa no processamento de dados. Com Spark Streaming, usamos para consumir mensagens no Kafka e persistir de forma permanente em Parquet no HDFS. Com Spark ML, usamos para construir a matriz de preferência do usuário-conteúdo e calcular a fatoração do modelo do usuário e do item para o Collaborative Filtering (ALS). Com Spark SQL / DataFrame construímos visualizações de métricas para avaliar os resultados dos testes A/B. Esses são alguns exemplos da versatilidade do Spark e de como ele é útil em uma Plataforma de BigData.&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;Uma aplicação feita com Spark pode rodar em um cluster do próprio Spark (standalone), no Mesos ou no YARN. A gente usa somente o YARN.&lt;/p&gt;

&lt;p&gt;A execução da aplicação Spark é composta por dois componentes: o Driver que inicializa a aplicação, define o grafo de tarefas e as transformações e controla a execuções das tarefas, e; os Executors que armazenam blocos de dados em memória e executam as tarefas. Em um cluster YARN, o Spark tem duas formas de execução: uma em que o Driver é executada em uma máquina local e os Executores no cluster e a outra em que o Driver também é executado no cluster. No segundo caso, o launcher do Spark negocia com o ResourceManager um container para rodar o Driver. Esse conteiner é alocado pelo NodeManager e inicializado com o Driver. Em ambos os casos, o Driver negocia com o ResourceManager os containers dos Executores que são inicializados pelo NodeManager, nesse momento, eles sabem o endereço de volta do Driver e abrem a comunicação (os dados podem trafegar entre Driver-Executores e entre Executores). Nesse ponto em diante, o processamento começa.&lt;/p&gt;

&lt;p&gt;O processamento em si é composto pelo grafo de tarefas que é construída a partir das transformações nas coleções de dados. A coleção que pode ser gerada a partir das transformações é chamada de RDD (Resilient Distributed Datasets) e é uma abstração lazy de um conjunto de dados. O resultado só é realmente materializado quando uma ação é executada no RDD, por exemplo, count ou collect. A materialização é feita através da análise de todas as tarefas do grafo de execução que precisam ser feitas para gerar esse resultado. Nesse momento, o Driver passa a enviar tarefas para os executores e armazenar os resultados nos Blocos de memória de cada Executor. Um RDD materializado é representado em partições que ficam distribuídas nos Executores. No momento em que novos RDDs são materializados, os antigos vão sendo desalocados, contudo, caso os dados sejam novamente necessários, o Driver reexecuta as tarefas para materializar esses resultados. É possível fazer cache dos dados materializados, tanto em memória quanto em disco, evitando assim o reprocessamento.&lt;/p&gt;

&lt;p&gt;Essa é a base do modelo de execução do Spark que é estendido pelas outras ferramentas do pacote, DataFrame / SQL com Catalyst, Spark Streaming com DStream e os mini-batches, as pipelines e algoritmos de Machine Learning e o processamento de grafos do GraphX.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Spark&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://spark.apache.org/&#34;&gt;https://spark.apache.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Provide elastic scaling within a Spark application&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://issues.apache.org/jira/browse/SPARK-3174&#34;&gt;https://issues.apache.org/jira/browse/SPARK-3174&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Projeto Tungsten&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://issues.apache.org/jira/browse/SPARK-7075&#34;&gt;https://issues.apache.org/jira/browse/SPARK-7075&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Adaptive execution in Spark&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://issues.apache.org/jira/browse/SPARK-9850&#34;&gt;https://issues.apache.org/jira/browse/SPARK-9850&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing&lt;/strong&gt; (2012)&lt;br/&gt;
&lt;a href=&#34;http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&#34;&gt;http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discretized Streams: A Fault-Tolerant Model for Scalable Stream Processing&lt;/strong&gt; (2012)&lt;br/&gt;
&lt;a href=&#34;http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-259.pdf&#34;&gt;http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-259.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;conclusão&#34;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;Esse artigo é a catarse de um pouco mais de 2 anos de trabalho feito pelo time de Personalização no desenvolvimento do Sistema de Recomendação. Esse trabalho teve início com um grupo de cerca de 5 pessoas e, com alguma troca, se manteve nesse tamanho por grande parte do tempo. Há cerca de 9 meses, BigData ganhou internamente em importância e foi criada uma área dentro da empresa com esse foco. Onde antes tínhamos o Sistema de Recomendação como o único produto de dados dessa Plataforma de BigData, desse ponto em diante passamos a ter o desafio de tornar essa tecnologia disponível e útil para toda a empresa e todo o Grupo. Também estamos aumentando o time e planejando o crescimento para atender outros casos de uso. O mais importante dessa experiência é que construímos um conhecimento sólido para continuarmos desenvolvendo essa plataforma e expandir sua capacidade e valor dentro do Grupo.&lt;/p&gt;

&lt;p&gt;Eu gostaria de pensar no futuro, e nesse sentido, a minha expectativa é que a gente construa na Globo.com uma plataforma similar ao Google Cloud Platform.&lt;/p&gt;

&lt;div class=&#34;embed video-player&#34;&gt;
&lt;iframe class=&#34;youtube-player&#34; type=&#34;text/html&#34;
    width=&#34;640&#34; height=&#34;385&#34;
    src=&#34;http://www.youtube.com/embed/Y0Z58YQSXv0&#34;
    allowfullscreen frameborder=&#34;0&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;A gente vem de um modelo em que um mesmo time faz o desenvolvimento do Sistema de Recomendação, coleta e processamento de dados, a análise e construção de modelos, e a gestão da Plataforma de BigData. A proposta é expandir cada uma dessas responsabilidade em times próprios e adicionar outros com foco em suportar novas demandas. É nesse processo que nos encontramos nesse momento.&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;Meu interesse pessoal é Inteligência Artificial e, alinhado com os avanços que vem sendo feitos na área, BigData é muito importante para o desenvolvimento dos modelos que estão produzindo os melhores resultados. De outra forma, também é válido dizer que a construção de agentes inteligentes é uma forma de tornar tratável uma quantidade muito grande de dados. Por uma formulação ou por outra, a ideia comum é que BigData e Inteligência Artificial tem uma intersecção importante.&lt;/p&gt;

&lt;p&gt;Tem muita tecnologia nova sendo desenvolvida que propõe formas de tratar imagens, vídeo, linguagem natural na qual um algoritmo pode executar funções que hoje são feitas por pessoas. O que eu pretendo fazer é identificar onde essa tecnologia pode ser usada na Globo.com e trazer esse conhecimento.&lt;/p&gt;

&lt;p&gt;Em específico, acredito que em algum momento teremos um time de Deep Learning para estudo e desenvolvimento de soluções novas usando BigData.&lt;/p&gt;

&lt;p&gt;Esse é o tema do meu Mestrado e no futuro, trarei mais material sobre o assunto.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>