<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tensorflow on Ciro Cavani</title>
    <link>http://cirocavani.github.io/tags/tensorflow/index.xml</link>
    <description>Recent content in Tensorflow on Ciro Cavani</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>pt-br</language>
    <copyright>&amp;copy; 2016. All rights reserved.</copyright>
    <atom:link href="http://cirocavani.github.io/tags/tensorflow/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>TensorFlow: Recomendação com ALS (Collaborative Filtering)</title>
      <link>http://cirocavani.github.io/post/tensorflow-recomendacao-com-als-collaborative-filtering/</link>
      <pubDate>Wed, 01 Mar 2017 11:31:34 -0300</pubDate>
      
      <guid>http://cirocavani.github.io/post/tensorflow-recomendacao-com-als-collaborative-filtering/</guid>
      <description>

&lt;p&gt;Esse artigo é sobre a análise do ALS implementado no TensorFlow. O ALS é um método para fatoração de matriz usado como algoritmo de &lt;em&gt;Collaborative Filtering&lt;/em&gt; em Sistemas de Recomendação. A análise consiste no treinamento e &lt;em&gt;tuning&lt;/em&gt; desse algoritmo e a avaliação do erro final. Para comparação, o mesmo algoritmo é implementado com o Spark. A metodologia usada tem características peculiares de como a Recomendação e o ALS funcionam. O resultado mostra que o Spark tem performance melhor que o TensorFlow no erro final.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Código&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Recommendation/ALS.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;motivação&#34;&gt;Motivação&lt;/h2&gt;

&lt;p&gt;Desde que comecei a trabalhar com Recomendação na Globo.com, já coloquei em Produção mais de uma implementação do ALS. É um algoritmo relativamente fácil de entender e que tem excelentes resultados na prática. Atualmente, a implementação que usamos em Produção é a do &lt;a href=&#34;https://youtu.be/Q0VXllYilM0&#34;&gt;Spark 2&lt;/a&gt;. O TensorFlow é uma tecnologia que possibilita a implementação de algoritmos mais sofisticados de Inteligência Artificial que tenho interesse em usar em Produção. Seria ideal que pudesse ser usado nos algoritmos mais comuns que tem boa performance.&lt;/p&gt;

&lt;p&gt;Com base nessa ideia, essa análise é uma primeira comparação entre essas duas implementações do TensorFlow e do Spark.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;IMPORTANTE&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Essa análise foi feita com um dataset pequeno com objetivo de facilitar o desenvolvimento, portanto, os resultados obtidos são apenas para ter uma ideia e não servervem para chegar em &lt;em&gt;conclusões definitivas&lt;/em&gt; sobre essas implementações.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Tomei conhecimento de que o TensorFlow tinha a implementação do ALS a partir de um vídeo do &lt;a href=&#34;https://events.withgoogle.com/tensorflow-dev-summit/&#34;&gt;TensorFlow Dev Summit&lt;/a&gt; que ocorreu em 15/Fevereiro (WALS no tempo 2:20):&lt;/p&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/Tuv5QYKU-MM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;


&lt;h2 id=&#34;introdução&#34;&gt;Introdução&lt;/h2&gt;

&lt;p&gt;A ideia geral é simples:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Usuários dão rating para alguns filmes e o algoritmo gera uma lista de outros filmes que o usuário também daria um bom rating.&lt;/p&gt;

&lt;p&gt;O ALS é um método de fatoração de matriz que é usado para &amp;lsquo;completar&amp;rsquo; os ratings dos filmes que o usuário não deu rating, baseado nos ratings que vários usuários deram aos filmes.&lt;/p&gt;

&lt;p&gt;Cada usuário e filme é transformado em um vetor de números (fatores) que são ajustados para representar o interesse do usuário em uma determinada característica de um filme (cada fator é um &amp;lsquo;peso&amp;rsquo; que indica quanto o usuário gosta e quanto o filme oferece). O produto entre os fatores do usuário e os fatores do filme tem que ser &amp;lsquo;igual&amp;rsquo; ao rating que o usuário deu ao filme.&lt;/p&gt;

&lt;p&gt;No caso dos filmes que o usuário não deu rating (não viu?), esse produto é o &amp;lsquo;rating previsto&amp;rsquo;. Ordenando todos os ratings previstos, os maiores são usados para recomendação.&lt;/p&gt;

&lt;p&gt;Esse é o algoritmo de Collaborative Filtering com ALS.&lt;/p&gt;

&lt;p&gt;Esse é o algoritmo que ficou famoso no prêmio Netflix.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nesse trabalho, a análise do ALS consiste em:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Preparação de Dados: ratings do MovieLens, dataset para treinamento, validação e teste&lt;/li&gt;
&lt;li&gt;Treinamento com TensorFlow: algoritmo que completa a matriz de ratings com ALS do TensorFlow&lt;/li&gt;
&lt;li&gt;Treinamento com Spark: algoritmo que completa a matriz de ratings com ALS do Spark&lt;/li&gt;
&lt;li&gt;Seleção de Parâmetros: busca da combinação com menor erro no dataset de validação&lt;/li&gt;
&lt;li&gt;Comparação: avaliação do erro no dataset de teste da melhor combinação de parâmetros&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;preparação-dos-dados&#34;&gt;Preparação dos Dados&lt;/h2&gt;

&lt;p&gt;Os dados usados nessa análise são do &lt;a href=&#34;https://grouplens.org/datasets/movielens/&#34;&gt;MovieLens&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MovieLens Small&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[ &lt;a href=&#34;http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html&#34;&gt;README&lt;/a&gt; ]
[ &lt;a href=&#34;http://files.grouplens.org/datasets/movielens/ml-latest-small.zip&#34;&gt;ZipFile&lt;/a&gt; ]&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This dataset (ml-latest-small) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 100004 ratings and 1296 tag applications across 9125 movies. These data were created by 671 users between January 09, 1995 and October 16, 2016. This dataset was generated on October 17, 2016.&lt;/p&gt;

&lt;p&gt;Users were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.&lt;/p&gt;

&lt;p&gt;The data are contained in the files links.csv, movies.csv, ratings.csv and tags.csv.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;O dataset consiste de 100.004 ratings registrados por 671 usuários em 9.066 vídeos (o número de vídeos com rating é menor que o número de vídeos com tag, 9.125). Como esperado, a matriz de usuários por vídeos é bastante esparsa: apenas 1,64% de ratings dos 6.083.286 (671 x 9.066) possíveis.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Diferente desse dataset, em que o número de usuários é bem menor que o número de itens (menor que &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;10&lt;/sub&gt;), na recomendação da Globo.com normalmente a proporção é inversa, ou seja, muito mais usuários do que itens - nas nossas próprias análises, essa é uma característica relevante.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Esse dataset é bastante pequeno e serve ao propósito de desenvolvido da análise e não para encontrar &amp;lsquo;grandes verdades&amp;rsquo;.&lt;/p&gt;

&lt;p&gt;A estratégia é dividir esses dados para treinamento, validação e teste. O dataset de treinamento será usado como os dados que o algoritmo conhece do usuário (e deve aprender sobre). O dataset de validação é para ser usado durante o treinamento para medir a performance do algoritmo, verificar overfitting (ou under) e fazer tuning de parâmetros. O dataset de teste será usado uma única vez para medir o desempenho final do algoritmo com os melhores parâmetros.&lt;/p&gt;

&lt;p&gt;O critério usado para dividir os dados é baseado em uma especificidade de Recomendação. No pipeline de Produção, um algoritmo é treinado com os dados históricos e tem sua performance avaliada em tempo real. Desconsiderando o impacto que a própria recomendação possa ter no consumo de itens, essa mesma &amp;lsquo;dinâmica temporal&amp;rsquo; é usada para dividir os dados.&lt;/p&gt;

&lt;p&gt;Os ratings são ordenados pelo timestamp em que foram feitos. Os primeiros 80% desses ratings são designados para treinamento / validação e os últimos 20% são designados para teste. Novamente, o primeiro dataset é ordenado e dividido em 80% para treinamento e 20% para validação. A divisão, portanto, fica 64% para treinamento, 16% para validação e 20% para teste.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Uma variação desse critério: separar por tempo primeiro entre 70% treinamento e 30% validação / teste, depois separar por shuffle 15% de validação e 15% de teste. (Escolhi usar só o critério de tempo porque é mais próximo de Produção)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;O dataset de treinamento tem 64.002 ratings, 435 usuários e 5.668 vídeos.&lt;/p&gt;

&lt;p&gt;O dataset de validação tem 16.001 ratings, 136 usuários e 4.112 vídeos.&lt;/p&gt;

&lt;p&gt;O dataset de teste tem 20.001 ratings, 147 usuários e 4.753 vídeos.&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;A medida de performance usada nesse análise é o RMSE (&lt;a href=&#34;https://en.wikipedia.org/wiki/Root-mean-square_error&#34;&gt;Root Mean Square Error&lt;/a&gt;) onde o &amp;lsquo;erro&amp;rsquo; é a diferença entre o rating atribuído pelo usuário a um vídeo e o rating calulado pelo produto entre o vetor de fatores desse usuário e o vetor de fatores desse vídeo. O RMSE é uma medida aproximada de quanto o algoritmo pode errar a predição de rating, para mais ou para menos. A expectativa é que esse valor seja muito pequeno para o dataset de treinamento (o ALS minimiza um função similar ao RMSE).&lt;/p&gt;

&lt;p&gt;Para efeito de avaliação de performance, temos uma especificidade do ALS. Uma vez que é necessário ter o vetor de fatores tanto do usuário quanto do vídeo para estimar o rating, apenas usuários e vídeos que estão simultaneamente no dataset de treinamento e validação (ou teste) podem ser considerados para o cálculo do RMSE. Nesse caso, estamos avaliando a capacidade de predição do algoritmo e ignorando a cobertura (tanto em usuários ou vídeos).&lt;/p&gt;

&lt;p&gt;Apenas um subconjunto do dataset de validação e teste é usado para avaliação.&lt;/p&gt;

&lt;p&gt;(Todo o dataset de treinamento pode ser usado na avaliação)&lt;/p&gt;

&lt;p&gt;A avaliação com o dataset de validação tem 944 ratings, 23 usuários e 2.424 vídeos.&lt;/p&gt;

&lt;p&gt;A avaliação com o dataset de teste tem 278 ratings, 5 usuários e 2.332 vídeos.&lt;/p&gt;

&lt;h2 id=&#34;treinamento-com-tensorflow&#34;&gt;Treinamento com TensorFlow&lt;/h2&gt;

&lt;p&gt;A implementação do algoritmo foi baseada na documentação da classe WALS do TensorFlow e nos testes dessa classe.&lt;/p&gt;

&lt;p&gt;Documentação da implementação do WALS:&lt;/p&gt;

&lt;p&gt;[ &lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/v1.0.0/tensorflow/contrib/factorization/python/ops/factorization_ops.py#L53-L166&#34;&gt;GitHub: factorization_ops.py#L53-L166&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;Documentação dos parâmetros do WALS:&lt;/p&gt;

&lt;p&gt;[ &lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/v1.0.0/tensorflow/contrib/factorization/python/ops/factorization_ops.py#L181-L214&#34;&gt;GitHub: factorization_ops.py#L181-L214&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;Código do teste da classe WALS:&lt;/p&gt;

&lt;p&gt;[ &lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/v1.0.0/tensorflow/contrib/factorization/python/ops/factorization_ops_test.py#L534-L576&#34;&gt;GitHub: factorization_ops_test.py#L534-L576&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;Código da loss function:&lt;/p&gt;

&lt;p&gt;[ &lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/df5d3cd42335e31bccb6c796169d000d73c747d3/tensorflow/contrib/factorization/python/ops/factorization_ops_test.py#L105-L158&#34;&gt;GitHub: factorization_ops_test.py#L105-L158&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;Rascunho do algoritmo:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Recommendation/ALS%20draft.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;O algoritmo é implementado em duas classes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;ALSRecommender&lt;/code&gt;: classe responsável pelo treinamento (recebe um dataset com ratings, calcula os fatores dos usuários e vídeos com o ALS e retorna o modelo com esses fatores)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ALSRecommenderModel&lt;/code&gt;: classe responsável pela inferência (recebe um par usuário e vídeo e retorna a predição do rating ou recebe um usuário e retorna os vídeos com maior rating para esse usuário)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;ALSRecommender&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A classe &lt;code&gt;ALSRecommender&lt;/code&gt; recebe três parâmetros:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;num_factors&lt;/code&gt; (default 10): número de fatores em que cada usuário e vídeo devem ser representados (valor muito grande pode resultar em overfitting, muito pequeno em underfitting; custo computacional, tamanho da matriz de usuários e vídeos)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;num_iters&lt;/code&gt; (default 10): número de repetições do método do ALS (a convergência normalmente é rápida, portando um número muito grande pode não ajudar muito; custo computacional)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;reg&lt;/code&gt; (default 1e-1): fator de regularização (impacto na convergência, valor muito grande pode resultar em instabilidade e um valor muito pequeno pode resultar em overfitting)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;O treinamento é implementado no método &lt;code&gt;fit&lt;/code&gt; e consiste em três passos: transformação dos dados em matriz esparsa, criação do ALS e execução do ALS.&lt;/p&gt;

&lt;p&gt;No final é retornanda uma instância do &lt;code&gt;ALSRecommenderModel&lt;/code&gt; com a matriz de usuários e matriz de vídeos (itens).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def fit(self, dataset, verbose=False):
    with tf.Graph().as_default(), tf.Session() as sess:
        input_matrix, mapping = self.sparse_input(dataset)
        model = self.als_model(dataset)
        self.train(model, input_matrix, verbose)
        row_factor = model.row_factors[0].eval()
        col_factor = model.col_factors[0].eval()
        return ALSRecommenderModel(row_factor, col_factor, mapping)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;O primeiro passo é a transformação de uma lista de ratings em uma matriz esparsa de usuários por vídeos, implementado no método &lt;code&gt;sparse_input&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sparse_input(self, dataset):
    mapping = new_mapping(dataset)

    indices = [(mapping.users_to_idx[r.user_id],
                mapping.items_to_idx[r.item_id])
               for r in dataset.ratings]
    values = [r.rating for r in dataset.ratings]
    shape = (dataset.n_users, dataset.n_items)

    return tf.SparseTensor(indices, values, shape), mapping
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;O segundo passo é a construção do ALS para calcular os fatores e &amp;lsquo;completar&amp;rsquo; os ratings, implementado no método &lt;code&gt;als_model&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def als_model(self, dataset):
    return WALSModel(
        dataset.n_users,
        dataset.n_items,
        self.num_factors,
        regularization=self.regularization,
        unobserved_weight=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;O tercero passo é a execução do ALS em si, que consiste na repetição de dois passos: mantem a matriz de vídeos constante e altera a matriz de usuários; mantem a matriz de usuários constante e altera a matriz de vídeos. A cada passo, o erro entre os ratings do input e os ratings aproximados deve diminuir.&lt;/p&gt;

&lt;p&gt;Execução do ALS implementada no método &lt;code&gt;train&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def train(self, model, input_matrix, verbose=False):
    rmse_op = self.rmse_op(model, input_matrix) if verbose else None

    row_update_op = model.update_row_factors(sp_input=input_matrix)[1]
    col_update_op = model.update_col_factors(sp_input=input_matrix)[1]

    model.initialize_op.run()
    model.worker_init.run()
    for _ in range(self.num_iters):
        # Update Users
        model.row_update_prep_gramian_op.run()
        model.initialize_row_update_op.run()
        row_update_op.run()
        # Update Items
        model.col_update_prep_gramian_op.run()
        model.initialize_col_update_op.run()
        col_update_op.run()

        if verbose:
            print(&#39;RMSE: {:,.3f}&#39;.format(rmse_op.eval()))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;ALSRecommenderModel&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A classe &lt;code&gt;ALSRecommenderModel&lt;/code&gt; recebe três parâmetros:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;user_factors&lt;/code&gt;: matriz densa de usuários por número de fatores&lt;/li&gt;
&lt;li&gt;&lt;code&gt;item_factors&lt;/code&gt;: matriz densa de vídeos por número de fatores&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mapping&lt;/code&gt;: objeto que converte &lt;code&gt;user_id&lt;/code&gt; para / de índice em &lt;code&gt;user_factors&lt;/code&gt;, &lt;code&gt;item_id&lt;/code&gt; para / de índice em &lt;code&gt;item_factors&lt;/code&gt; (vídeos)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A classe &lt;code&gt;ALSRecommenderModel&lt;/code&gt; implementa dois métodos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;transform&lt;/code&gt;: recebe uma lista de &lt;code&gt;(user_id, item_id)&lt;/code&gt; e retorna a predição do rating&lt;/li&gt;
&lt;li&gt;&lt;code&gt;recommend&lt;/code&gt;: recebe um &lt;code&gt;user_id&lt;/code&gt; e retorna a lista de &lt;code&gt;(item_id, rating)&lt;/code&gt; ordenada com os maiores ratings primeiro&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;O método &lt;code&gt;transform&lt;/code&gt; é o produto dos fatores do usuário e do vídeo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def transform(self, x):
    for user_id, item_id in x:
        if user_id not in self.mapping.users_to_idx \
            or item_id not in self.mapping.items_to_idx:
            yield (user_id, item_id), 0.0
            continue
        i = self.mapping.users_to_idx[user_id]
        j = self.mapping.items_to_idx[item_id]
        u = self.user_factors[i]
        v = self.item_factors[j]
        r = np.dot(u, v)
        yield (user_id, item_id), r
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;O método &lt;code&gt;recommend&lt;/code&gt; é o produto da matriz de vídeos pelo vetor de fatores de um usuário:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def recommend(self, user_id, num_items=10, items_exclude=set()):
    i = self.mapping.users_to_idx[user_id]
    u = self.user_factors[i]
    V = self.item_factors
    P = np.dot(V, u)
    rank = sorted(enumerate(P), key=lambda p: p[1], reverse=True)

    top = list()
    k = 0
    while k &amp;lt; len(rank) and len(top) &amp;lt; num_items:
        j, r = rank[k]
        k += 1

        item_id = self.mapping.items_from_idx[j]
        if item_id in items_exclude:
            continue

        top.append((item_id, r))

    return top
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Execução&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A execução consiste em instanciar a classe &lt;code&gt;ALSRecommender&lt;/code&gt;, fazer o treinamento com o método &lt;code&gt;fit&lt;/code&gt; e fazer inferências com a instância da classe &lt;code&gt;ALSRecommenderModel&lt;/code&gt; retornada.&lt;/p&gt;

&lt;p&gt;Nesse exemplo, a inferência é executada para todos os ratings de avaliação como definido na Preparação de Dados. Com o rating da inferência, é calculado o RMSE de cada conjunto de avaliação.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;als = ALSRecommender(num_factors=10, num_iters=10, reg=0.1)
print(&#39;Training...\n&#39;)
als_model = als.fit(train_data, verbose=True)
print(&#39;\nEvaluation...\n&#39;)
eval_rmse(als_model)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Saída:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;Training...

RMSE: 1.729
RMSE: 0.765
RMSE: 0.631
RMSE: 0.588
RMSE: 0.565
RMSE: 0.550
RMSE: 0.540
RMSE: 0.532
RMSE: 0.526
RMSE: 0.521

Evaluation...

RMSE (train): 0.521
RMSE (validation): 1.688
RMSE for heavy: 2.444
RMSE for moderate: 1.465
RMSE for accidental: 1.926
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;treinamento-com-spark&#34;&gt;Treinamento com Spark&lt;/h2&gt;

&lt;p&gt;Documentação da implementação:&lt;/p&gt;

&lt;p&gt;[ &lt;a href=&#34;http://spark.apache.org/docs/2.1.0/ml-collaborative-filtering.html&#34;&gt;Manual&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;Documentação dos parâmetros do ALS:&lt;/p&gt;

&lt;p&gt;[ &lt;a href=&#34;http://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#module-pyspark.ml.recommendation&#34;&gt;Python API&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;Exemplo do ALS:&lt;/p&gt;

&lt;p&gt;[ &lt;a href=&#34;https://github.com/apache/spark/blob/v2.1.0/examples/src/main/python/ml/als_example.py&#34;&gt;GitHub: als_example.py&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;A execução consiste em instanciar a classe &lt;code&gt;ALS&lt;/code&gt;, fazer o treinamento com o método &lt;code&gt;fit&lt;/code&gt; e fazer inferências com a instância da classe &lt;code&gt;ALSModel&lt;/code&gt; retornada.&lt;/p&gt;

&lt;p&gt;Nesse exemplo, a inferência é executada para todos os ratings de avaliação como definido na Preparação de Dados. Com o rating da inferência, é calculado o RMSE de cada conjunto de avaliação.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.ml.recommendation import ALS as SparkALS

spark_als = SparkALS(rank=10, maxIter=10, regParam=0.1)
spark_model = spark_als.fit(train_df)
eval_rmse_spark(spark_model)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Saída:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;RMSE (train): 0.601
RMSE (validation): 1.018
RMSE for heavy: 1.128
RMSE for moderate: 0.974
RMSE for accidental: 1.327
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;seleção-de-parâmetros&#34;&gt;Seleção de Parâmetros&lt;/h2&gt;

&lt;p&gt;A Seleção de Parâmetros consiste em uma busca executando todas as combinações de valores dos parâmetros. Para limitar a busca, é pré-selecionado um conjunto de valores que faz mais sentido.&lt;/p&gt;

&lt;p&gt;Nessa análise, foi usada uma seleção de valores ainda menor, visando agilizar o processo.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;default_params = dict(num_factors=[5, 10, 20, 50, 100, 200],
                      num_iters=[5, 10, 25],
                      reg = [1e-5, 1e-3, 1e-1, 0.0, 1])

small_params = dict(num_factors=[5, 10, 20],
                    num_iters=[5],
                    reg = [1e-3, 1e-1, 1])

def grid_search(eval_func, params=default_params, verbose=False):
    best_rmse = None
    best_params = None
    for reg in params[&#39;reg&#39;]:
        for num_iters in params[&#39;num_iters&#39;]:
            for num_factors in params[&#39;num_factors&#39;]:
                if verbose:
                    print(&#39;\nParams:&#39;, num_factors, num_iters, reg)
                try:
                    rmse = eval_func(num_factors, num_iters, reg)
                except:
                    rmse = None
                if verbose:
                    print(&#39;RMSE:&#39;,
                          &#39;{:,.3f}&#39;.format(rmse) if rmse is not None else &#39;-&#39;)
                if rmse is not None and (best_rmse is None or rmse &amp;lt; best_rmse):
                    if verbose:
                        print(&#39;best update!&#39;)
                    best_rmse = rmse
                    best_params = (num_factors, num_iters, reg)
    return best_params, best_rmse
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TensorFlow ALS&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def tf_eval(num_factors, num_iters, reg):
    als = ALSRecommender(num_factors=num_factors, num_iters=num_iters, reg=reg)
    model = als.fit(train_data)
    return _rmse(model, valid_eval)

tf_params, tf_score = grid_search(tf_eval, params=small_params, verbose=True)
print()
print(&#39;Best Params:\n\nn_factors={}, n_iters={}, reg={}, RMSE={:.3f}&#39; \
        .format(*tf_params, tf_score))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Saída:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;Params: 5 5 0.001
RMSE: 1.146
best update!

Params: 10 5 0.001
RMSE: 1.309

Params: 20 5 0.001
RMSE: 2.870

Params: 5 5 0.1
RMSE: 1.355

Params: 10 5 0.1
RMSE: 1.438

Params: 20 5 0.1
RMSE: 1.636

Params: 5 5 1
RMSE: 1.487

Params: 10 5 1
RMSE: 1.941

Params: 20 5 1
RMSE: 1.933

Best Params:

n_factors=5, n_iters=5, reg=0.001, RMSE=1.146
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spark ALS&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def spark_eval(num_factors, num_iters, reg):
    als = SparkALS(rank=num_factors, maxIter=num_iters, regParam=reg)
    model = als.fit(train_df)
    return _rmse_spark(model, valid_df)

spark_params, spark_score = grid_search(spark_eval, params=small_params, verbose=True)
print()
print(&#39;Best Params:\n\nn_factors={}, n_iters={}, reg={}, RMSE={:.3f}&#39; \
        .format(*spark_params, spark_score))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Saída:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;Params: 5 5 0.001
RMSE: 1.300
best update!

Params: 10 5 0.001
RMSE: 1.418

Params: 20 5 0.001
RMSE: 1.615

Params: 5 5 0.1
RMSE: 0.981
best update!

Params: 10 5 0.1
RMSE: 1.003

Params: 20 5 0.1
RMSE: 1.033

Params: 5 5 1
RMSE: 1.258

Params: 10 5 1
RMSE: 1.258

Params: 20 5 1
RMSE: 1.258

Best Params:

n_factors=5, n_iters=5, reg=0.1, RMSE=0.981
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;comparação&#34;&gt;Comparação&lt;/h2&gt;

&lt;p&gt;Para a comparação das implementações, a medida de performance é o RMSE dos ratings de avaliação do dataset de Teste com os os melhores parâmetros selecionados na busca.&lt;/p&gt;

&lt;p&gt;O TensorFlow ALS com 5 fatores, 5 iterações e 0.001 de regularização tem RMSE de 1,183 no Teste.&lt;/p&gt;

&lt;p&gt;O Spark ALS com 5 fatores, 5 iterações e 0.1 de regularização tem RMSE de 1,086 no Teste.&lt;/p&gt;

&lt;p&gt;Esse resultado mostra que o Spark tem performance melhor que o TensorFlow no erro final.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TensorFlow ALS&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;als = ALSRecommender(*tf_params)
model = als.fit(train_data)
rmse = _rmse(model, test_eval)
print(&#39;TensorFlow RMSE for test: {:,.3f}&#39;.format(rmse))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Saída:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;TensorFlow RMSE for test: 1.183
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Spark ALS&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;num_factors, num_iters, reg = spark_params
als = SparkALS(rank=num_factors, maxIter=num_iters, regParam=reg)
model = als.fit(train_df)
rmse = _rmse_spark(model, test_df)
print(&#39;Spark RMSE for test: {:,.3f}&#39;.format(rmse))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Saída:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;Spark RMSE for test: 1.086
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusão&#34;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;Apesar do resultado dessa análise ser consistente (várias execuções, pouca variação), não é definitivo. Seria necessário fazer a análise com datasets maiores e verificar se há realmente diferença significativa de performance entre essas implementação.&lt;/p&gt;

&lt;p&gt;Para trabalhos futuros, a ideia completar o trabalho com os passos necessários para colocar esse algoritmo &amp;lsquo;em produção&amp;rsquo;, ou seja, que um sistema de recomendação possa fazer inferência com o modelo treinado com o ALS do TensorFlow. Uma forma de fazer isso é transformar a classe &lt;code&gt;ALSRecommenderModel&lt;/code&gt; em um grafo do TensorFlow que possa ser carregado e executado pelo TensorFlow Serving. Esse pode ser o tema de um próximo artigo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TensorFlow no Jupyter (com notebooks)</title>
      <link>http://cirocavani.github.io/post/tensorflow-no-jupyter-com-notebooks/</link>
      <pubDate>Tue, 13 Sep 2016 21:13:26 -0300</pubDate>
      
      <guid>http://cirocavani.github.io/post/tensorflow-no-jupyter-com-notebooks/</guid>
      <description>

&lt;p&gt;Esse tutorial é sobre o TensorFlow no Jupyter. A princípio, esse projeto pode ser usado para instalar automaticamente o Jupyter Notebook configurado com TensorFlow 0.10 e alguns notebooks de exemplo (tutoriais do TensorFlow). Outro objetivo é servir como base para criação de configurações customizadas isoladas (exemplo um ambiente extra para testar com TensorFlow GPU Python 3 com CUDA 8). O Jupyter é uma ferramenta excelente para testar ideias e prototipar rapidamente com TensorFlow.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Projeto&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-jupyter&#34;&gt;https://github.com/cirocavani/tensorflow-jupyter&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Esse artigo consiste em:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;o procedimento de instalação básico&lt;/li&gt;
&lt;li&gt;a descrição dos notebooks de exemplo&lt;/li&gt;
&lt;li&gt;a explicação de como funciona a instalação&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;instalação&#34;&gt;Instalação&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/cirocavani/tensorflow-jupyter.git
cd tensorflow-jupyter

#bin/setup-linux
bin/setup-mac
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Comandos:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/jupyter
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inicializa o Jupyter que já tem o kernel do TensorFlow configurado.&lt;/p&gt;

&lt;p&gt;Acesso em &lt;a href=&#34;http://localhost:8888/&#34;&gt;http://localhost:8888/&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/tensorboard
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inicializa a ferramenta de visualização do TensorFlow, mostra grafo de execução, valores de medições do treinamento.&lt;/p&gt;

&lt;p&gt;Acesso em &lt;a href=&#34;http://localhost:6006/&#34;&gt;http://localhost:6006/&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;notebooks-exemplo&#34;&gt;Notebooks Exemplo&lt;/h2&gt;

&lt;p&gt;Os notebooks exemplo são baseados nos tutorias disponiveis no site do TensorFlow. Os tutoriais originais estão referenciados no início do notebook. O código de alguns tutoriais foi alterado para usar algumas funcionalidades mais &amp;ldquo;reais&amp;rdquo; (por exemplo: leitura de CSV em batch).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;0 - First Run&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/00%20-%20First%20Run.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Hello World com TensorFlow.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1 - Linear Regression&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/01%20-%20Linear%20Regression.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse exemplo, é feito uma regressão linear para o fit de uma reta em dados gerados sinteticamente pela função y = 0.1x + 0.3, ou seja, o TensorFlow aprende os parâmetros 0.1 e 0.3 de um dataset ruidoso.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2 - MNIST, Softmax Regression&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/02%20-%20MNIST%2C%20Softmax%20Regression.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse exemplo, é feito um classificador com uma regressão softmax para identificação de dígitos 0-9 em uma imagem. Dado na entrada uma imagem de 28x28 pixels de um dígito manuscrito, o classificador retorna 10 valores, cada um indicando a &amp;ldquo;probabilidade&amp;rdquo; de ser um dos dígitos que a variável representa. A acurácia é de 92%.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3 - MNIST, Convolutional Network&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/03%20-%20MNIST%2C%20Convolutional%20Network.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse exemplo, é feito um classificador com uma rede neural convolutiva para identificação de dígitos 0-9 em uma imagem. Dado na entrada uma imagem de 28x28 pixels de um dígito manuscrito, o classificador retorna 10 valores, cada um indicando a &amp;ldquo;probabilidade&amp;rdquo; de ser um dos dígitos que a variável representa. A acurácia é de 99%.&lt;/p&gt;

&lt;p&gt;A rede é formada por duas camadas de convolução, uma camada toda conectada, uma camada de dropout e uma camada de regressão softmax.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4 - MNIST, Feed-forward NN with Log&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/04%20-%20MNIST%2C%20Feed-forward%20NN%20with%20Log.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse exemplo, é feito um classificador com uma rede neural feed-forward para identificação de dígitos 0-9 em uma imagem. Dado na entrada uma imagem de 28x28 pixels de um dígito manuscrito, o classificador retorna 10 valores, cada um indicando a &amp;ldquo;probabilidade&amp;rdquo; de ser um dos dígitos que a variável representa. A acurácia é de 99%.&lt;/p&gt;

&lt;p&gt;A rede é formada por duas camadas toda conectada e uma camada de regressão softmax.&lt;/p&gt;

&lt;p&gt;O modelo treinado nesse notebook pode ser visualizado no TensorBoard.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5 - Iris, DNN Classifier (tf.contrib.learn)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/05%20-%20Iris%2C%20DNN%20Classifier%20%28tf.contrib.learn%29.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse exemplo, é feito um classificador com uma rede neural para classificação de 3 espécies de flor. Dada na entrada as medidas da sépala e da pétala, o classificador retorna a espécie 0 setosa, 1 versicolor e 2 virginica. A acurácia é de 97%.&lt;/p&gt;

&lt;p&gt;A rede é formada por 5 camadas.&lt;/p&gt;

&lt;p&gt;O modelo treinado nesse notebook pode ser visualizado no TensorBoard.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6 - Iris, DNN Classifier with Log (tf.contrib.learn)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/06%20-%20Iris%2C%20DNN%20Classifier%20with%20Log%20%28tf.contrib.learn%29.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse exemplo, é feito um classificador com uma rede neural para classificação de 3 espécies de flor. Dada na entrada as medidas da sépala e da pétala, o classificador retorna a espécie 0 setosa, 1 versicolor e 2 virginica. A acurácia é de 97%.&lt;/p&gt;

&lt;p&gt;A rede é formada por 5 camadas e é feito o monitoramento de métricas que podem ser visualizadas no log do notebook e no TensorBoard.&lt;/p&gt;

&lt;p&gt;O modelo treinado nesse notebook pode ser visualizado no TensorBoard.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;7 - Reading CSV&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/07%20-%20Reading%20CSV.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse exemplo, é feito o pipeline para leitura de dados de um arquivo CSV. O arquivo usado nesse estudo é o mesmo do Census.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;8 - Census, Logistic Regression Classifier (tf.contrib.learn)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/08%20-%20Census%2C%20Logistic%20Regression%20Classifier%20%28tf.contrib.learn%29.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse exemplo, é feito um classificador com uma regressão logística para classificação de rendimento maior que 50 mil dólares. Dada na entrada as informações do Census, o classificador retorna 1 mais de 50 mil e 0 menos de 50 mil. A acurácia é de 87%.&lt;/p&gt;

&lt;p&gt;O modelo treinado nesse notebook pode ser visualizado no TensorBoard.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;9 - Census, Combined Classifier (tf.contrib.learn)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/09%20-%20Census%2C%20Combined%20Classifier%20%28tf.contrib.learn%29.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse exemplo, é feito um classificador com a combinação de uma rede neural e uma regressão logística (treinadas em conjunto) para classificação de rendimento maior que 50 mil dólares. Dada na entrada as informações do Census, o classificador retorna 1 mais de 50 mil e 0 menos de 50 mil. A acurácia é de 93%.&lt;/p&gt;

&lt;p&gt;O modelo treinado nesse notebook pode ser visualizado no TensorBoard.&lt;/p&gt;

&lt;h2 id=&#34;funcionamento&#34;&gt;Funcionamento&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;IMPORTANTE:&lt;/p&gt;

&lt;p&gt;Essa é a descrição de como é feita a configuração do projeto, contudo esse procedimento já está definido no comando &lt;code&gt;bin/setup-linux&lt;/code&gt; (ou &lt;code&gt;bin/setup-mac&lt;/code&gt;) que deve ser executado ao invés desse procedimento.&lt;/p&gt;

&lt;p&gt;Esse passo a passo é para ajudar na customização do Projeto.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;O procedimento de instalação consiste em:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Instalar o Python 2.7 com &lt;code&gt;miniconda&lt;/code&gt; (Linux ou Mac)&lt;/li&gt;
&lt;li&gt;Instalar o Jupyter Notebook 4.2 no &lt;em&gt;environment&lt;/em&gt; &lt;code&gt;default&lt;/code&gt; do &lt;code&gt;conda2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Instalar o TensorFlow 0.10 em um &lt;em&gt;environment&lt;/em&gt; próprio (para Python 2.7)&lt;/li&gt;
&lt;li&gt;Instalar o kernel do Python no &lt;em&gt;environment&lt;/em&gt; do TensorFlow&lt;/li&gt;
&lt;li&gt;Configurar o kernel no Jupyter que é executado no &lt;em&gt;environment&lt;/em&gt; do TensorFlow&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A estrutura do projeto será:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;deps/conda2&lt;/code&gt;: instalação do Python 2.7 (&lt;code&gt;miniconda&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;deps/tensorflow-0.10&lt;/code&gt;: instalação do TensorFlow 0.10 (&lt;em&gt;environment&lt;/em&gt; isolado)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data/kernels/tensorflow-0.10/kernel.json&lt;/code&gt;: configuração do kernel no Jupyter para o TensorFlow&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ao final desse procedimento, a execução do Jupyter consiste de (&lt;code&gt;bin/jupyter&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export JUPYTER_DATA_DIR=`pwd`/data
deps/conda2/bin/jupyter notebook --no-browser --notebook-dir=`pwd`/workspace
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para a criação de uma customização, os passos 3, 4 e 5 devem ser ajustados para um novo &lt;em&gt;environment&lt;/em&gt; com configuração customizada.&lt;/p&gt;

&lt;h3 id=&#34;instalação-do-python&#34;&gt;Instalação do Python&lt;/h3&gt;

&lt;p&gt;A instalação do Python é feita usando o &lt;code&gt;miniconda&lt;/code&gt; para versão 2.7 (para Linux ou Mac).&lt;/p&gt;

&lt;p&gt;Os comandos de Python e Conda ficam disponíveis na pasta &lt;code&gt;deps/conda2/bin&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://conda.pydata.org/miniconda.html&#34;&gt;http://conda.pydata.org/miniconda.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(Linux)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -k -L \
    -O https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh

chmod +x Miniconda2-latest-Linux-x86_64.sh

./Miniconda2-latest-Linux-x86_64.sh -b -f -p deps/conda2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;instalação-do-jupyter&#34;&gt;Instalação do Jupyter&lt;/h3&gt;

&lt;p&gt;O Jupyter tem um meta pacote que depende de todos os componentes, incluindo o Notebook.&lt;/p&gt;

&lt;p&gt;O comando do Jupyter fica disponível em &lt;code&gt;deps/conda2/bin/jupyter&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://jupyter.readthedocs.io/en/latest/install.html&#34;&gt;http://jupyter.readthedocs.io/en/latest/install.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://pypi.python.org/pypi/jupyter&#34;&gt;https://pypi.python.org/pypi/jupyter&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;deps/conda2/bin/pip install --upgrade jupyter
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;instalação-do-tensorflow&#34;&gt;Instalação do TensorFlow&lt;/h3&gt;

&lt;p&gt;O TensorFlow é distribuído como um pacote Wheel e é instalado em um &lt;em&gt;environment&lt;/em&gt; isolado criado no &lt;code&gt;miniconda&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;O comando do TensorBoard fica disponível em &lt;code&gt;deps/tensorflow-0.10/bin/tensorboard&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#anaconda-installation&#34;&gt;https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#anaconda-installation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(Linux)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;deps/conda2/bin/conda create -y -p deps/tensorflow-0.10 python=2.7

deps/tensorflow-0.10/bin/pip install \
    https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;instalação-do-kernel-para-o-tensorflow&#34;&gt;Instalação do Kernel para o TensorFlow&lt;/h3&gt;

&lt;p&gt;No &lt;em&gt;environment&lt;/em&gt; do TensorFlow é instalado o kernel Python que possibilita a conexão a partir do Jupyter (Notebook). Isso torna possível escrever código Python que é executado dentro desse &lt;em&gt;environment&lt;/em&gt; isolado.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://ipython.readthedocs.io/en/stable/install/kernel_install.html&#34;&gt;http://ipython.readthedocs.io/en/stable/install/kernel_install.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://pypi.python.org/pypi/ipykernel&#34;&gt;https://pypi.python.org/pypi/ipykernel&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;deps/tensorflow-0.10/bin/pip install ipykernel
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;configuração-do-kernel-para-o-tensorflow&#34;&gt;Configuração do Kernel para o TensorFlow&lt;/h3&gt;

&lt;p&gt;O Jupyter é configurado com o comando que executa o kernel do Python dentro do &lt;em&gt;environment&lt;/em&gt; que tem o TensorFlow. O kernel é responsável por receber requisições do servidor do Jupyter e executar código Python no processo em que está executando. Esse processo é executado somente com os pacotes do próprio &lt;em&gt;environment&lt;/em&gt; (isolamento) e pacotes adicionais devem ser instalados nesse &lt;em&gt;environment&lt;/em&gt; sem conflito com outros &lt;em&gt;environments&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://jupyter-client.readthedocs.io/en/latest/kernels.html#kernelspecs&#34;&gt;https://jupyter-client.readthedocs.io/en/latest/kernels.html#kernelspecs&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p data/kernels/tensorflow-0.10-py2

echo &amp;quot;{
 \&amp;quot;display_name\&amp;quot;: \&amp;quot;TensorFlow 0.10 (CPU, Python 2)\&amp;quot;,
 \&amp;quot;language\&amp;quot;: \&amp;quot;python\&amp;quot;,
 \&amp;quot;argv\&amp;quot;: [
  \&amp;quot;`pwd`/deps/tensorflow-0.10/bin/python\&amp;quot;,
  \&amp;quot;-c\&amp;quot;,
  \&amp;quot;from ipykernel.kernelapp import main; main()\&amp;quot;,
  \&amp;quot;-f\&amp;quot;,
  \&amp;quot;{connection_file}\&amp;quot;
 ]
}&amp;quot; &amp;gt; data/kernels/tensorflow-0.10-py2/kernel.json
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusão&#34;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;O Jupyter é uma excelente ferramenta para exploração de ideias e desenvolvimento de código rápido. A facilidade de visualização e execução independente de células é muito prático. O desenvolvimento de aplicações mais complexas e com código mais estruturado já não é muito favorável.&lt;/p&gt;

&lt;p&gt;O TensorFlow é uma ferramenta sofisticada para desenvolvimento de algoritmos inteligentes. Algumas APIs podem ser complexas e de difícil entendimento, algumas vezes bem documentadas e outras não. A comunidade é muito engajada e o Google vem produzindo tutoriais, documentação e modelos muito úteis para o aprendizado.&lt;/p&gt;

&lt;p&gt;Aprender TensorFlow no Jupyter é o melhor caminho e essa é a proposta desse Projeto.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Compilação do TensorFlow 0.10 para Linux (com GPU)</title>
      <link>http://cirocavani.github.io/post/compilacao-do-tensorflow-0.10-para-linux-com-gpu/</link>
      <pubDate>Thu, 08 Sep 2016 22:06:49 -0300</pubDate>
      
      <guid>http://cirocavani.github.io/post/compilacao-do-tensorflow-0.10-para-linux-com-gpu/</guid>
      <description>

&lt;p&gt;Esse tutorial é sobre a construção do pacote do TensorFlow 0.10 para Linux com suporte a GPU. Para esse procedimento é usado o Docker com uma imagem do Ubuntu 16.04, GCC 5.4, Python 2.7, Cuda 8.0 (RC) e cuDNN 5.1. A motivação desse trabalho é usar o TensorFlow com as novas gerações de GPUs da Nvidia (&lt;a href=&#34;https://developer.nvidia.com/pascal&#34;&gt;Pascal&lt;/a&gt;). Um segundo objetivo é a criação de um pacote do TensorFlow com capacidades específicas (por exemplo, um &amp;ldquo;Compute Capability&amp;rdquo; específico).&lt;/p&gt;

&lt;p&gt;O procedimento também está disponível como um script para Docker (ainda é necessário fazer o download do Cuda manualmente).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-build&#34;&gt;https://github.com/cirocavani/tensorflow-build&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;compilação&#34;&gt;Compilação&lt;/h2&gt;

&lt;p&gt;O procedimento consiste em:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Instalar o Cuda 8.0rc com o patch para GCC 5.4&lt;/li&gt;
&lt;li&gt;Instalar o cuDNN 5.1 para Cuda 8.0&lt;/li&gt;
&lt;li&gt;Instalar o Java 8&lt;/li&gt;
&lt;li&gt;Instalar o Bazel 0.3&lt;/li&gt;
&lt;li&gt;Construir TensorFlow 0.10&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;O resultado é o pacote do TensorFlow para Python 2 e Linux (com GPU):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tensorflow-0.10.0-py2-none-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Baseado na documentação:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#installing-from-sources&#34;&gt;https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#installing-from-sources&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Um procedimento alternativo:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/tools/docker/Dockerfile.devel-gpu&#34;&gt;https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/tools/docker/Dockerfile.devel-gpu&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;download-do-cuda-8-0rc-cudnn-5-1&#34;&gt;Download do Cuda 8.0rc, cuDNN 5.1&lt;/h3&gt;

&lt;p&gt;É necessário o download dos pacotes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cuda_8.0.27_linux.run
cuda_8.0.27.1_linux.run
cudnn-8.0-linux-x64-v5.1.tgz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Esses pacotes devem ser colocados na pasta &lt;code&gt;build_deps/&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;No momento, a versão mais recente do Cuda é a 8.0 RC e só está disponível para download para membros do &lt;a href=&#34;https://developer.nvidia.com/accelerated-computing-developer&#34;&gt;Accelerated Computing Developer Program&lt;/a&gt; no site da Nvidia (o cadastro é gratuito).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-release-candidate-download&#34;&gt;https://developer.nvidia.com/cuda-release-candidate-download&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Select Target Platform:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Operating System = Linux
Architecture = x86_64
Distribution = Ubuntu
Version = 16.04
Installer Type = runfile (local)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Download:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Base Installer&lt;/strong&gt; - &lt;code&gt;cuda_8.0.27_linux.run&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Patch 1&lt;/strong&gt; - &lt;code&gt;cuda_8.0.27.1_linux.run&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&#34;https://developer.nvidia.com/rdp/cudnn-download&#34;&gt;https://developer.nvidia.com/rdp/cudnn-download&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Selecione:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. I Agree To the Terms of the cuDNN Software License Agreement
2. Download cuDNN v5.1 (August 10, 2016), for CUDA 8.0 RC
3. cuDNN v5.1 Library for Linux
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Download:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cudnn-8.0-linux-x64-v5.1.tgz&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;setup-inicial-no-docker-para-ubuntu-16-04&#34;&gt;Setup inicial no Docker para Ubuntu 16.04&lt;/h3&gt;

&lt;p&gt;Download dos demais pacotes necessários para o build:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd build_deps

curl -k -L \
  -H &amp;quot;Cookie: oraclelicense=accept-securebackup-cookie&amp;quot; \
  -O http://download.oracle.com/otn-pub/java/jdk/8u102-b14/jdk-8u102-linux-x64.tar.gz

curl -k -L \
  -O https://github.com/bazelbuild/bazel/releases/download/0.3.1/bazel-0.3.1-installer-linux-x86_64.sh

chmod +x cuda_8.0.27_linux.run
chmod +x cuda_8.0.27.1_linux.run
chmod +x bazel-0.3.1-installer-linux-x86_64.sh

cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Criação do Container com as dependências:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker create -t --name=tensorflow_build ubuntu:16.04
docker cp build_deps tensorflow_build:/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Execução do Shell no Container:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker start tensorflow_build
docker exec -i -t tensorflow_build /bin/bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Setup do Container:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;echo &#39;debconf debconf/frontend select Noninteractive&#39; | debconf-set-selections
echo &#39;APT::Install-Recommends &amp;quot;0&amp;quot;;&#39; &amp;gt; 01norecommend
mv 01norecommend /etc/apt/apt.conf.d

apt-get update
apt-get upgrade -y

apt-get install -y \
    build-essential \
    python-dev \
    python-wheel \
    python-setuptools \
    python-numpy \
    swig \
    zlib1g-dev \
    unzip \
    file \
    git \
    ca-certificates \
    rsync
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;instalação-do-cuda-8-0rc-e-cudnn-5-1&#34;&gt;Instalação do Cuda 8.0rc e cuDNN 5.1&lt;/h3&gt;

&lt;p&gt;(comandos a serem executados dentro do container)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;/build_deps/cuda_8.0.27_linux.run --silent --toolkit --override

/build_deps/cuda_8.0.27.1_linux.run --silent --accept-eula

tar zxf /build_deps/cudnn-8.0-linux-x64-v5.1.tgz \
    -C /usr/local/cuda-8.0 --strip-components=1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;instalação-do-java-8&#34;&gt;Instalação do Java 8&lt;/h3&gt;

&lt;p&gt;(comandos a serem executados dentro do container)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;tar zxf /build_deps/jdk-8u102-linux-x64.tar.gz -C /opt --no-same-owner

echo &#39;export JAVA_HOME=/opt/jdk1.8.0_102&#39; &amp;gt; /etc/profile.d/java.sh
echo &#39;export PATH=$PATH:$JAVA_HOME/bin&#39; &amp;gt;&amp;gt; /etc/profile.d/java.sh
chmod a+x /etc/profile.d/java.sh

source /etc/profile.d/java.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;instalação-do-bazel-0-3&#34;&gt;Instalação do Bazel 0.3&lt;/h3&gt;

&lt;p&gt;(comandos a serem executados dentro do container)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;/build_deps/bazel-0.3.1-installer-linux-x86_64.sh --prefix=/opt/bazel-0.3.1

echo &#39;export PATH=$PATH:/opt/bazel-0.3.1/bin&#39; &amp;gt; /etc/profile.d/bazel.sh
chmod a+x /etc/profile.d/bazel.sh

source /etc/profile.d/bazel.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;construção-do-tensorflow-0-10&#34;&gt;Construção do TensorFlow 0.10&lt;/h3&gt;

&lt;p&gt;Considerações:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Configuração da GPU&lt;/p&gt;

&lt;p&gt;É necessário definir qual &amp;ldquo;Compute Capability&amp;rdquo; o binário do TensorFlow vai suportar.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-gpus&#34;&gt;https://developer.nvidia.com/cuda-gpus&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Por exemplo:&lt;/p&gt;

&lt;p&gt;A GeForce GT 740M tem Compute Capability 3.0&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export TF_CUDA_COMPUTE_CAPABILITIES=3.0
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Uso de Memória&lt;/p&gt;

&lt;p&gt;O build executa várias tarefas em paralelo e o consumo de memória pode aumentar rapidamente.&lt;/p&gt;

&lt;p&gt;Para limitar o número de execuções paralelas é usada a opção &lt;code&gt;-j 4&lt;/code&gt; no build.&lt;/p&gt;

&lt;p&gt;Em um notebook com 8 cores (HT), 8G de memória é insuficiente.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(comandos a serem executados dentro do container)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;useradd -m tensorflow
passwd -d tensorflow

su - tensorflow

git clone https://github.com/tensorflow/tensorflow.git -b r0.10 ~/tensorflow-0.10

cd ~/tensorflow-0.10

export PYTHON_BIN_PATH=/usr/bin/python
export TF_NEED_GCP=0
export TF_NEED_CUDA=1
export GCC_HOST_COMPILER_PATH=/usr/bin/gcc
export TF_CUDA_VERSION=8.0
export CUDA_TOOLKIT_PATH=/usr/local/cuda-8.0
export TF_CUDNN_VERSION=5
export CUDNN_INSTALL_PATH=/usr/local/cuda-8.0
export TF_CUDA_COMPUTE_CAPABILITIES=3.0
./configure

bazel build -j 4 -c opt --config=cuda \
    //tensorflow/tools/pip_package:build_pip_package

bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOME

mv ~/tensorflow-0.10.0-py2-none-{any,linux_x86_64}.whl

# saindo su
exit

# saindo do container
exit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;Para baixar o pacote (fora do container):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker cp \
    tensorflow_build:/home/tensorflow/tensorflow-0.10.0-py2-none-linux_x86_64.whl \
    .
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusão&#34;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;O procedimento de build do TensorFlow não é complicado, mas pequenas variações podem atingir alguns bugs do build (&lt;a href=&#34;https://github.com/tensorflow/tensorflow/issues/3985&#34;&gt;exemplo&lt;/a&gt;). Com um script bem definido, fica fácil criar o pacote do TensorFlow.&lt;/p&gt;

&lt;p&gt;Com esse pacote, é possível usar o TensorFlow nas GPUs mais recentes da Nvidia.&lt;/p&gt;

&lt;p&gt;No próximo artigo será um tutorial de como configurar um ambiente de desenvolvimento com Jupyter.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TensorFlow: Integração com BigData</title>
      <link>http://cirocavani.github.io/post/tensorflow-integracao-bigdata/</link>
      <pubDate>Mon, 22 Aug 2016 22:00:00 -0300</pubDate>
      
      <guid>http://cirocavani.github.io/post/tensorflow-integracao-bigdata/</guid>
      <description>

&lt;p&gt;Esse artigo é sobre a criação de uma Aplicação com TensorFlow em que o treinamento é feito no YARN (Hadoop), o servidor de inferência é hospedado no Tsuru e as requisições são feitas por Aplicações Java/Scala. Esses são os desafios para colocar em produção na Globo.com aplicações de Inteligência Artificial. Nesse trabalho foram desenvolvidos projetos que são Provas de Conceito de como fazer essa Aplicação TensorFlow integrada com BigData (o código está disponível no GitHub).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Código&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc&#34;&gt;https://github.com/cirocavani/tensorflow-poc&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;motivação&#34;&gt;Motivação&lt;/h2&gt;

&lt;p&gt;O problema começou com: como criar uma Aplicação de AI? A proposta para resolver esse problema foi: TensorFlow.&lt;/p&gt;

&lt;p&gt;O problema passou a ser: o que é o TensorFlow? Como criar uma Aplicação com TensorFlow? como colocar essa Aplicação em Produção? como usar essa Aplicação em um produto para entregar valor à empresa e ao usuário?&lt;/p&gt;

&lt;p&gt;A preparação para resolver esse problema começou a ser feita no início de 2016, no primeiro Hackday em Janeiro e teve um &amp;ldquo;evento crucial&amp;rdquo; no final de Junho, quando o Google publicou em seu blog de pesquisa o uso do TensorFlow para recomendação de app no Google Play.&lt;/p&gt;

&lt;p&gt;O TensorFlow é uma biblioteca usada para treinar algoritmos com dados. Esses &amp;ldquo;algoritmos&amp;rdquo; podem ser &amp;ldquo;executados&amp;rdquo; para gerar resultados que podem ser usados em &amp;ldquo;aplicações inteligentes&amp;rdquo;. Um exemplo de aplicação que pode usar um &amp;ldquo;algoritmo&amp;rdquo; do TensorFlow é Recomendação.&lt;/p&gt;

&lt;p&gt;O problema tornou-se: como usar o TensorFlow em Recomendação?&lt;/p&gt;

&lt;p&gt;Para resolver esse problema, dividi em duas frentes de trabalho: integrar o TensorFlow na infra de BigData, e; desenvolver uma aplicação de TensorFlow para Recomendação.&lt;/p&gt;

&lt;p&gt;A primeira frente de trabalho está feita.&lt;/p&gt;

&lt;h2 id=&#34;por-que-o-tensorflow&#34;&gt;Por que o TensorFlow?&lt;/h2&gt;

&lt;p&gt;Nos últimos anos, a tendência que eu venho observando do mercado é que AI finalmente se tornou a área de diferenciação, inovação e crescimento. Os investidores estão cada vez mais colocando dinheiro nessa área. As grandes empresas estão cada vez mais se posicionando como &amp;ldquo;empresas de AI&amp;rdquo;, sendo os maiores exemplos Google, Facebook e Microsoft, com Apple e Amazon chegando junto. Startups como a que fez a app Prisma valem bilhão de dólares.&lt;/p&gt;

&lt;p&gt;Enfim, AI é importante.&lt;/p&gt;

&lt;p&gt;A minha expectativa é usar Inteligência Artificial na Globo.com, incluindo produção e distribuição de conteúdo, desenvolvimento de produto, controle de infraestrutura, análises de segurança, &amp;hellip; seguir a tendência de Google, Facebook, Microsoft, Amazon, e também ser uma empresa de AI.&lt;/p&gt;

&lt;p&gt;Para o trabalho de fazer AI na Globo.com, a tecnologia escolhida foi o TensorFlow, framework de AI do Google. A estratégia escolhida para acelerar o progresso desse trabalho foi usar a infraestrutura de BigData que já tem bastante poder de processamento. Essa é a proposta inicial, com a expectativa de criar demanda suficiente para uma infraestrutura de AI com GPUs.&lt;/p&gt;

&lt;p&gt;O TensorFlow é um framework para construção de algoritmos que podem ser usados para processar texto, imagem e vídeo, para tratamento de linguagem natural, reconhecimento de objetos e faces, reconhecimento de padrões, construção de chatbots, construção de bots para jogos, &amp;hellip;, ou seja, é uma biblioteca de funções avançadas que o Google está usando para fazer AI e que está ganhando muita popularidade entre os desenvolvedores. Contudo, há um risco nesse controle direto do Google sobre uma tecnologia que está se tornando a &amp;ldquo;base de tudo&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Recentemente, duas reportagens discutiram o TensorFlow.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Here&amp;rsquo;s Why Google Is Open-Sourcing Some Of Its Most Important Technology&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.forbes.com/sites/gregsatell/2016/07/18/heres-why-google-is-open-sourcing-some-of-its-most-important-technology/&#34;&gt;http://www.forbes.com/sites/gregsatell/2016/07/18/heres-why-google-is-open-sourcing-some-of-its-most-important-technology/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Google Sprints Ahead in AI Building Blocks, Leaving Rivals Wary&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.bloomberg.com/news/articles/2016-07-21/google-sprints-ahead-in-ai-building-blocks-leaving-rivals-wary&#34;&gt;http://www.bloomberg.com/news/articles/2016-07-21/google-sprints-ahead-in-ai-building-blocks-leaving-rivals-wary&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Portanto, o primeiro passo na direção de &amp;ldquo;fazer AI na Globo.com&amp;rdquo; está sendo criar uma primeira Aplicação com TensorFlow.&lt;/p&gt;

&lt;p&gt;Esse artigo é sobre como isso foi feito.&lt;/p&gt;

&lt;h3 id=&#34;o-que-é-o-tensorflow&#34;&gt;O que é o TensorFlow?&lt;/h3&gt;

&lt;p&gt;TensorFlow é um framework para processamento de tensores (arrays multidimensionais) em ambientes heterogêneos (CPUs, GPUs, mobile). É um projeto open-source do Google liberado em Novembro/2015. A API principal é Python e a engine de execução é C++, o Google espera que a comunidade desenvolva bindings para outras linguagens.&lt;/p&gt;

&lt;p&gt;Na prática, o TensorFlow permite escrever algoritmos na forma de operações com tensores (arrays) que resultam em um grafo de execução. Esse grafo é otimizado e as operações são distribuídas para serem executadas em CPU ou GPU de acordo com o tipo de operação e a disponibilidade desses recursos. A engine pode ser estendida com novas operações que podem ter implementações para CPU e/ou GPU e serão usadas de acordo com os recursos. Essa capacidade de otimizar o processamento para recursos heterogêneos é o principal benefício do TensorFlow.&lt;/p&gt;

&lt;p&gt;O Google usa o TensorFlow para pesquisa e desenvolvimento de produtos que usam Deep Learning. Contudo, a proposta é que o framework seja usado para Machine Learning em geral. Essa proposta não torna o TensorFlow um framework para processamento geral, mas essa é uma possibilidade para o futuro.&lt;/p&gt;

&lt;p&gt;Atualmente, o Google espera que a comunidade use o TensorFlow para criar algoritmos de Machine Learning que, em um segundo momento, poderão se tornar a biblioteca de algoritmos do TensorFlow (hoje, ainda é escasso comparado aos líderes do mercado, scikit-learn e R/CRAN).&lt;/p&gt;

&lt;p&gt;Comparado com Spark que é um framework para processamento geral e que tem uma biblioteca de Machine Learning com vários algoritmos (com suporte a batch, stream, SQL e grafos), acredito que o TensorFlow tem um mecanismo de execução mais &amp;ldquo;moderno&amp;rdquo;. O processamento otimizado com recursos heterogêneos é um benefício que hoje não está disponível no Spark e pode se tornar indispensável para os processamentos que estão em demanda na atualidade. Ou seja, o TensorFlow parte de um modelo de execução baseado na demanda crescente de algoritmos inteligentes para processar dados enquanto o Spark é a evolução do modelo antigo de paralelização de processamento que pode estar com os dias contados.&lt;/p&gt;

&lt;p&gt;O TensorFlow ainda tem muito o que evoluir para que se torne o framework padrão em processamento de dados e Machine Learning, mas me parece que esse é o futuro para o qual estamos caminhando.&lt;/p&gt;

&lt;p&gt;Por fim, Jeff Dean, criador do MapReduce e muitas outras tecnologias de processamento distribuído e BigData, é um dos líderes do TensorFlow, o que dá ao projeto muita credibilidade.&lt;/p&gt;

&lt;p&gt;Para saber mais sobre o TensorFlow:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;TensorFlow&lt;/strong&gt;&lt;br&gt;
TensorFlow is an Open Source Software Library for Machine Intelligence&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;https://www.tensorflow.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/TensorFlow&#34;&gt;https://en.wikipedia.org/wiki/TensorFlow&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(código)&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorflow&#34;&gt;https://github.com/tensorflow/tensorflow&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(blog)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TensorFlow - Google’s latest machine learning system, open sourced for everyone&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://googleresearch.blogspot.com.br/2015/11/tensorflow-googles-latest-machine_9.html&#34;&gt;http://googleresearch.blogspot.com.br/2015/11/tensorflow-googles-latest-machine_9.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TensorFlow: Open source machine learning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=oZikw5k_2FM&#34;&gt;https://www.youtube.com/watch?v=oZikw5k_2FM&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(Paper)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TensorFlow: Large-scale machine learning on heterogeneous systems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://download.tensorflow.org/paper/whitepaper2015.pdf&#34;&gt;http://download.tensorflow.org/paper/whitepaper2015.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Os dois últimos releases do TensorFlow foram bastante interessantes:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;TensorFlow v0.9 now available with improved mobile support&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://developers.googleblog.com/2016/06/tensorflow-v09-now-available-with.html&#34;&gt;https://developers.googleblog.com/2016/06/tensorflow-v09-now-available-with.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Esse é o último release e destaca o uso do TensorFlow em aplicações Mobile, onde é possível treinar um modelo que executa no smartphone para reconhecimento de objeto usando o vídeo da câmera, em tempo real.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Announcing TensorFlow 0.8 – now with distributed computing support!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://research.googleblog.com/2016/04/announcing-tensorflow-08-now-with.html&#34;&gt;https://research.googleblog.com/2016/04/announcing-tensorflow-08-now-with.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse release, o destaque foi a funcionalidade de treinamento distribuído do modelo, onde antes só era possível usar uma máquina, agora é possível treinar com várias tanto para distribuir processamento quanto para paralelizar configurações distintas do modelo.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;experiência-com-tensorflow&#34;&gt;Experiência com TensorFlow&lt;/h3&gt;

&lt;p&gt;Já há algum tempo, venho trabalhando esporadicamente com o TensorFlow (nos últimos 3 hackdays na Globo.com, quase 9 meses). Recentemente, o Google divulgou um paper em que eles elaboram como usam o TensorFlow para fazer recomendação de apps no Google Play. Essa se tornou a oportunidade que eu encontrei para começar a trazer para a Globo.com a base para crescermos na área de AI.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Wide &amp;amp; Deep Learning: Better Together with TensorFlow&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://research.googleblog.com/2016/06/wide-deep-learning-better-together-with.html&#34;&gt;https://research.googleblog.com/2016/06/wide-deep-learning-better-together-with.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Wide &amp;amp; Deep Learning for Recommender Systems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/1606.07792&#34;&gt;http://arxiv.org/abs/1606.07792&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;De forma prática, o Google disponibilizou no TensorFlow uma API para criar um modelo e um paper que explica como eles usam esse modelo para fazer recomendação. Eles não disponibilizaram o &amp;ldquo;sistema de recomendação&amp;rdquo; propriamente dito que, na verdade, precisa ser &amp;ldquo;construído&amp;rdquo; a partir do TensorFlow.&lt;/p&gt;

&lt;p&gt;Portando, o trabalho consiste em entender a proposta de &amp;ldquo;como&amp;rdquo; fazer recomendação usando o TensorFlow, mapear esse &amp;ldquo;sistema&amp;rdquo; no &amp;ldquo;modus operandi&amp;rdquo; do Ambiente de BigData da Globo.com e avaliar o ganho de ter o TensorFlow como parte dessa plataforma.&lt;/p&gt;

&lt;p&gt;Eu comecei a fazer esse trabalho.&lt;/p&gt;

&lt;p&gt;Estou trabalhando em duas &amp;ldquo;frentes&amp;rdquo;: a primeira é genérica sobre integração do TensorFlow em BigData e a outra é específica sobre fazer recomendação com TensorFlow. Estou trabalhando em ambas, mas o foco desse artigo é na primeira porque acredito que seja mais interessante para todos que ainda não conhecem o TensorFlow.&lt;/p&gt;

&lt;h4 id=&#34;aplicação-tensorflow&#34;&gt;Aplicação TensorFlow&lt;/h4&gt;

&lt;p&gt;A primeira &amp;ldquo;frente&amp;rdquo; de trabalho está sendo ganhar experiência em como o &lt;strong&gt;TensorFlow&lt;/strong&gt; funciona e identificar como é possível acomodar esse processo na plataforma da Globo.com.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;TensorFlow - Google’s latest machine learning system, open sourced for everyone&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://research.googleblog.com/2015/11/tensorflow-googles-latest-machine_9.html&#34;&gt;https://research.googleblog.com/2015/11/tensorflow-googles-latest-machine_9.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;O &lt;strong&gt;TensorFlow&lt;/strong&gt; é uma biblioteca C++ com uma API em Python para criação de modelos (especificação do algoritmo / grafo de operações e parâmetros; processamento de dados / treinamento). O resultado do treinamento (e validação) é um &amp;ldquo;modelo de inferência&amp;rdquo; que deve ser &amp;ldquo;transferido&amp;rdquo; para a &amp;ldquo;aplicação&amp;rdquo; que vai &amp;ldquo;fazer&amp;rdquo; AI.&lt;/p&gt;

&lt;p&gt;Na prática, uma vez definido o grafo de operações e parâmetros, esse &amp;ldquo;programa&amp;rdquo; é alimentado com dados que vão ajustando os parâmetros. No final, esse grafo com parâmetros &amp;ldquo;aprendidos&amp;rdquo; é transformado em um arquivo que pode ser &amp;ldquo;carregado&amp;rdquo; na aplicação e usado para fazer predições / inferência.&lt;/p&gt;

&lt;p&gt;Essa é a fase do treinamento e é praticamente toda em Python.&lt;/p&gt;

&lt;p&gt;Uma vez que o &amp;ldquo;modelo de inferência&amp;rdquo; está feito, a próxima fase é fazer inferência.&lt;/p&gt;

&lt;p&gt;Em Recomendação, &amp;ldquo;inferência&amp;rdquo; seria fazer uma lista das matérias que mais interessam o Usuário. Esse resultado pode ser obtido de duas formas: off-line e on-line. No primeiro caso, uma aplicação pega todas as matérias disponíveis e executa a inferência em batch guardando os melhores resultados para cada um dos Usuário conhecidos (é isso que fazemos com o algoritmo de Fatoração de Matriz / ALS). No segundo caso, o modelo é usado na requisição na API de Conteúdo para gerar a recomendação para um Usuário (é isso que fazemos com o algoritmo do TF-IDF e gostaríamos de fazer com o ALS também). Considero a primeira solução muito custosa computacionalmente e lenta para refletir o interesse imediato / recente do Usuário. Acredito que a solução on-line é melhor e tem alguns casos de uso em que é a única que faz sentido.&lt;/p&gt;

&lt;p&gt;Portanto, a dúvida é: é possível fazer inferência em um modelo do TensorFlow de forma on-line?&lt;/p&gt;

&lt;p&gt;No início do ano, o Google tornou público um segundo projeto chamado &lt;strong&gt;TensorFlow Serving&lt;/strong&gt; que assume esse papel de executar inferências em modelos do TensorFlow de forma on-line.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Running your models in production with TensorFlow Serving&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://research.googleblog.com/2016/02/running-your-models-in-production-with.html&#34;&gt;https://research.googleblog.com/2016/02/running-your-models-in-production-with.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;O &lt;strong&gt;TensorFlow Serving&lt;/strong&gt; é uma biblioteca C++ para construção de &amp;ldquo;servidor&amp;rdquo; de inferência genérico, que já tem suporte para o protocolo HTTP2 usando gRPC (ProtoBuf) e integração com a biblioteca do TensorFlow para fazer inferência (incluindo atualização automática de modelos).&lt;/p&gt;

&lt;p&gt;Na prática, o TF Serving possibilita escrever APIs para modelos do TensorFlow. Uma aplicação C++ que recebe requisições em HTTP2 e executa a inferência (&amp;ldquo;predição&amp;rdquo;) com o modelo treinado. O principal benefício dessa biblioteca é automaticamente carregar novas versões dos modelos conforme eles vão sendo atualizados pelo treinamento.&lt;/p&gt;

&lt;p&gt;Essa é a fase da inferência on-line do modelo e é praticamente toda em C++.&lt;/p&gt;

&lt;h4 id=&#34;algoritmo-tensorflow&#34;&gt;Algoritmo TensorFlow&lt;/h4&gt;

&lt;p&gt;A outra &amp;ldquo;frente&amp;rdquo; de trabalho é paralela a primeira e consistem em desenvolver propriamente o algoritmo de recomendação baseado no paper do Google. Isso significa definir as features que serão usadas, como ler esses dados e alimentar o treinamento do modelo e como executar a inferência usando as matérias de um determinado Produto (Portal).&lt;/p&gt;

&lt;p&gt;Comecei a fazer esse trabalho também, mas vou deixar para discuti-lo em outro artigo.&lt;/p&gt;

&lt;h2 id=&#34;tensorflow-em-bigdata&#34;&gt;TensorFlow em BigData&lt;/h2&gt;

&lt;p&gt;O trabalho consistiu em identificar e resolver todos os requisitos para o desenvolvimento de uma Aplicação TensorFlow em uma infraestrutura de BigData.&lt;/p&gt;

&lt;p&gt;Para esse trabalho, a &amp;ldquo;anatomia&amp;rdquo; de uma Aplicação TensorFlow que foi considerada consiste em dois componentes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;o treinamento (aprendizado): o código é em Python, precisa de acesso a dados e poder de processamento.&lt;/p&gt;

&lt;p&gt;Esse programa deve ser empacotado para rodar no YARN (Hadoop, RedHat EL 6)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;a API de consulta (inferência): o código é em C++, recebe requisições com dados &amp;ldquo;reais&amp;rdquo; e retorna o resultado a partir da versão mais recente de um &amp;ldquo;modelo treinado&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Esse programa deve ser empacotado para rodar no Tsuru (Ubuntu LTS 14.04)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;Para integrar uma aplicação que usa essa &amp;ldquo;Inteligência Artificial&amp;rdquo;, é necessário usar um Cliente que &amp;ldquo;faça requisições&amp;rdquo; ao servidor de inferência (2).&lt;/p&gt;

&lt;p&gt;Essa funcionalidade pode ser implementada em qualquer linguagem e é feita com Python nos exemplos do TensorFlow.&lt;/p&gt;

&lt;p&gt;Para esse trabalho, o interesse é integrar com Aplicações em Scala, logo um cliente Java satisfaz o requisito (Java 7).&lt;/p&gt;

&lt;h2 id=&#34;provas-de-conceito&#34;&gt;Provas de Conceito&lt;/h2&gt;

&lt;p&gt;O trabalho consistiu no desenvolvimento de Provas de Conceito (POC) que exploram os desafios para se criar uma Aplicação TensorFlow.&lt;/p&gt;

&lt;p&gt;Todas as POCs rodam dentro do Docker e foram testadas no Linux e no Mac (usando Docker on Mac).&lt;/p&gt;

&lt;p&gt;Além do Docker, não é necessário mais nada instalado na máquina.&lt;/p&gt;

&lt;p&gt;A POC &lt;a href=&#34;#tflearn-wide-n-deep&#34;&gt;tflearn_wide_n_deep&lt;/a&gt; foi o aquecimento rodando um exemplo do TensorFlow.&lt;/p&gt;

&lt;p&gt;As POCs &lt;a href=&#34;#tfserving-basic&#34;&gt;tfserving_basic&lt;/a&gt;, &lt;a href=&#34;#tfserving-advanced&#34;&gt;tfserving_advanced&lt;/a&gt; e &lt;a href=&#34;#skeleton-project&#34;&gt;skeleton_project&lt;/a&gt; correspondem ao trabalho de criar um projeto que consiste de algoritmo Python para treinamento e servidor C++ para servir o modelo (tem um cliente Python para validar o funcionamento).&lt;/p&gt;

&lt;p&gt;As POCs &lt;a href=&#34;#yarn-training&#34;&gt;yarn_training&lt;/a&gt;, &lt;a href=&#34;#tensorflow-centos6&#34;&gt;tensorflow_centos6&lt;/a&gt;, &lt;a href=&#34;#tensorflow-installer&#34;&gt;tensorflow_installer&lt;/a&gt;, &lt;a href=&#34;#hadoop-centos6&#34;&gt;hadoop_centos6&lt;/a&gt; e &lt;a href=&#34;#hadoop-ubuntu1604&#34;&gt;hadoop_ubuntu1604&lt;/a&gt; correspondem ao trabalho de fazer o treinamento usando YARN (Hadoop) no RedHat EL 6 (produção).&lt;/p&gt;

&lt;p&gt;A POC &lt;a href=&#34;#client-java&#34;&gt;client_java&lt;/a&gt; corresponde ao trabalho de usar em uma aplicação Java/Scala um serviço do TensorFlow.&lt;/p&gt;

&lt;p&gt;A POC &lt;a href=&#34;#server-tsuru&#34;&gt;server_tsuru&lt;/a&gt; corresponde ao trabalho de criar uma app no Tsuru para servir um modelo treinado do TensorFlow.&lt;/p&gt;

&lt;h4 id=&#34;tflearn-wide-n-deep&#34;&gt;tflearn_wide_n_deep&lt;/h4&gt;

&lt;p&gt;A POC é a execução do tutorial sobre o &amp;ldquo;algoritmo&amp;rdquo; usado na recomendação de app do Google Play.&lt;/p&gt;

&lt;p&gt;Na prática, é um classificador binário que responde se uma pessoa ganha mais ou menos de 50 mil dólares baseado em dados do Censo, é uma combinação de Logistic Regression com Rede Neural.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/tflearn_wide_n_deep/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;tfserving-basic&#34;&gt;tfserving_basic&lt;/h4&gt;

&lt;p&gt;A POC é a execução do tutorial sobre o TensorFlow Serving sem versionamento do modelo.&lt;/p&gt;

&lt;p&gt;O &amp;ldquo;algoritmo&amp;rdquo; desse tutorial é um classificador de imagem que faz reconhecimento de dígito usando dataset MNIST e Rede Neural.&lt;/p&gt;

&lt;p&gt;Na prática, consiste de todo o processo de compilação do TensorFlow Serving e do TensorFlow para execução dos três requisitos desse trabalho: treinamento, servidor e cliente (essa &amp;ldquo;arquitetura&amp;rdquo; é o resultado final esperado para uma Aplicação TensorFlow).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/tfserving_basic/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;tfserving-advanced&#34;&gt;tfserving_advanced&lt;/h4&gt;

&lt;p&gt;A POC é a execução do tutorial sobre o TensorFlow Serving com versionamento do modelo.&lt;/p&gt;

&lt;p&gt;O &amp;ldquo;algoritmo&amp;rdquo; desse tutorial é um classificador de imagem que faz reconhecimento de dígito usando dataset MNIST e Rede Neural.&lt;/p&gt;

&lt;p&gt;Na prática, consiste de todo o processo de compilação do TensorFlow Serving e do TensorFlow para execução dos três requisitos desse trabalho: treinamento, servidor e cliente (essa &amp;ldquo;arquitetura&amp;rdquo; é o resultado final esperado para uma Aplicação TensorFlow).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/tfserving_advanced/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;skeleton-project&#34;&gt;skeleton_project&lt;/h4&gt;

&lt;p&gt;(código do exemplo extraído para um projeto fora da árvore do TensorFlow Serving)&lt;/p&gt;

&lt;p&gt;A POC é a criação de um projeto standalone baseado no código do tutorial sobre o TensorFlow Serving com versionamento do modelo.&lt;/p&gt;

&lt;p&gt;O &amp;ldquo;algoritmo&amp;rdquo; desse projeto é um classificador de imagem que faz reconhecimento de dígito usando dataset MNIST e Rede Neural.&lt;/p&gt;

&lt;p&gt;Na prática, consiste em separar o código para treinamento, servidor e cliente em um projeto que depende do TensorFlow Serving e usa a mesma ferramenta de build Bazel (fazendo o processo de compilação do TensorFlow Serving e do TensorFlow).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/skeleton_project/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;yarn-training&#34;&gt;yarn_training&lt;/h4&gt;

&lt;p&gt;(motivação: usar a infraestrutura de armazenamento e processamento do Hadoop para rodar o treinamento com TensorFlow)&lt;/p&gt;

&lt;p&gt;A POC é a criação de uma Aplicação YARN para rodar o treinamento com TensorFlow.&lt;/p&gt;

&lt;p&gt;Na prática, consiste em criar uma aplicação Java baseada no exemplo de execução de shell script distribuído do YARN, essa aplicação é dividida em duas partes, uma que faz submissão do script e a outra que controla dentro do cluster a execução.&lt;/p&gt;

&lt;p&gt;Esse procedimento depende do instalador do TensorFlow criado em &lt;a href=&#34;#tensorflow-installer&#34;&gt;tensorflow_installer&lt;/a&gt; por link simbólico e do container em &lt;a href=&#34;#hadoop-centos6&#34;&gt;hadoop_centos6&lt;/a&gt; estar rodando.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/yarn_training/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;tensorflow-centos6&#34;&gt;tensorflow_centos6&lt;/h4&gt;

&lt;p&gt;(motivação: o TensorFlow não é oficialmente suportado no RedHat EL 6, o binário é compilado para glibc 2.17 e o código C++ 11, ambos requisitos não disponíveis, mas é possível criar um binário do TensorFlow compatível)&lt;/p&gt;

&lt;p&gt;A POC é a construção do binário do TensorFlow compatível com RedHat EL 6.&lt;/p&gt;

&lt;p&gt;Na prática, consiste em instalar a versão mais recente do GCC disponível para o CentOS 6, construir a ferramenta de build Bazel (binário incompatível com a glibc) e construir o TensorFlow (com patch para linkage).&lt;/p&gt;

&lt;p&gt;Essa POC é só para criar um pacote do TensorFlow.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/tensorflow_centos6/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;tensorflow-installer&#34;&gt;tensorflow_installer&lt;/h4&gt;

&lt;p&gt;(motivação: evitar download no ambiente de produção e evitar enviar múltiplos arquivos para o Hadoop)&lt;/p&gt;

&lt;p&gt;A POC é a criação de um instalador para o algoritmo de treinamento com o TensorFlow que tenha todas as dependência e possa ser executado no RedHat EL 6.&lt;/p&gt;

&lt;p&gt;Na prática, consiste em criar um pacote com TensorFlow, Python (conda) e todas as dependências que é embutido em um shell script que faz a instalação e executa o treinamento do TensorFlow.&lt;/p&gt;

&lt;p&gt;Esse procedimento depende do pacote do TensorFlow criado em &lt;a href=&#34;#tensorflow-centos6&#34;&gt;tensorflow_centos6&lt;/a&gt; por link simbólico.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/tensorflow_installer/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;hadoop-centos6&#34;&gt;hadoop_centos6&lt;/h4&gt;

&lt;p&gt;(motivação: essa imagem corresponde a uma &amp;ldquo;aproximação&amp;rdquo; do ambiente de produção que usa RedHat EL 6 com o qual o CentOS6 é binário-compatível - o TensorFlow não é oficialmente suportado nesse sistema)&lt;/p&gt;

&lt;p&gt;A POC é a configuração mínima do Hadoop no CentOS 6 para execução do treinamento com TensorFlow no YARN.&lt;/p&gt;

&lt;p&gt;Na prática, consiste em rodar os servidores do HDFS (NameNode e DataNode) e do YARN (ResourceManager e NodeManager) para poder executar uma aplicação (ApplicationMaster) que instale o TensorFlow e rode o script Python de treinamento.&lt;/p&gt;

&lt;p&gt;Essa POC é só a configuração do Hadoop no CentOS 6.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/hadoop_centos6/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;hadoop-ubuntu1604&#34;&gt;hadoop_ubuntu1604&lt;/h4&gt;

&lt;p&gt;(motivação: essa imagem corresponde ao ambiente em que o TensorFlow é oficialmente suportado, diferente do ambiente de produção RedHat, ou seja, o comportamento nesse ambiente deve representar o &amp;ldquo;funcionamento correto&amp;rdquo;)&lt;/p&gt;

&lt;p&gt;A POC é a configuração mínima do Hadoop no Ubuntu para execução do treinamento com TensorFlow no YARN.&lt;/p&gt;

&lt;p&gt;Na prática, consiste em rodar os servidores do HDFS (NameNode e DataNode) e do YARN (ResourceManager e NodeManager) para poder executar uma aplicação (ApplicationMaster) que instale o TensorFlow e rode o script Python de treinamento.&lt;/p&gt;

&lt;p&gt;Essa POC é só a configuração do Hadoop no Ubuntu 16.04.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/hadoop_ubuntu1604/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;client-java&#34;&gt;client_java&lt;/h4&gt;

&lt;p&gt;A POC é a criação de um cliente Java para fazer inferência em um serviço do TensorFlow.&lt;/p&gt;

&lt;p&gt;Na prática, consiste em gerar o código do protocolo de comunicação usando o gRPC (Protobuf) e usar esse código para acessar o serviço do TensorFlow, a especificação do protocolo faz parte da implementação do serviço.&lt;/p&gt;

&lt;p&gt;O &amp;ldquo;algoritmo&amp;rdquo; desse projeto é um classificador de imagem que faz reconhecimento de dígito usando dataset MNIST e Rede Neural.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/client_java/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;server-tsuru&#34;&gt;server_tsuru&lt;/h4&gt;

&lt;p&gt;A POC é a criação de uma aplicação do Tsuru para rodar o servidor do TensorFlow.&lt;/p&gt;

&lt;p&gt;O &amp;ldquo;algoritmo&amp;rdquo; desse servidor é um classificador de imagem que faz reconhecimento de dígito usando dataset MNIST e Rede Neural.&lt;/p&gt;

&lt;p&gt;Na prática, consiste em fazer o deploy do binário do servidor do TensorFlow em uma app do Tsuru.&lt;/p&gt;

&lt;p&gt;Esse procedimento depende do binário do servidor do TensorFlow criado em &lt;a href=&#34;#skeleton-project&#34;&gt;skeleton_project&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/server_tsuru/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusão&#34;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;Esse trabalho é o preparatório para a criação de Aplicações TensorFlow que serão colocadas em Produção na Globo.com.&lt;/p&gt;

&lt;p&gt;Ainda tem muitos desafios para completar esse trabalho, mas esse é um bom começo.&lt;/p&gt;

&lt;p&gt;O TensorFlow é um projeto fascinante e evolui muito rápido - uma excelente oportunidade de aprendizado e cooperação.&lt;/p&gt;

&lt;p&gt;Para trabalhos futuros, esses são alguns dos desafios:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;TFRecord: formato de dados do TensorFlow&lt;/p&gt;

&lt;p&gt;Como treinar um modelo a partir de dados armazenado em Parquet?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Treinamento Distribuído&lt;/p&gt;

&lt;p&gt;Como treinar um modelo com múltiplos containers no YARN?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tensorboard&lt;/p&gt;

&lt;p&gt;Como rodar o Tensorboard no ApplicationMaster para acompanhar o treinamento?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;TF Serving Source para Swift (OpenStack)&lt;/p&gt;

&lt;p&gt;Como armazenar os modelos como objetos no Swift (OpenStack)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;Em um próximo artigo, pretendo discutir uma Aplicação TensorFlow para Recomendação.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>