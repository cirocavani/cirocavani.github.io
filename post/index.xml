<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Ciro Cavani</title>
    <link>http://cirocavani.github.io/post/index.xml</link>
    <description>Recent content in Posts on Ciro Cavani</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>pt-br</language>
    <copyright>&amp;copy; 2016. All rights reserved.</copyright>
    <lastBuildDate>Wed, 01 Mar 2017 11:31:34 -0300</lastBuildDate>
    <atom:link href="http://cirocavani.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>TensorFlow: Recomendação com ALS (Collaborative Filtering)</title>
      <link>http://cirocavani.github.io/post/tensorflow-recomendacao-com-als-collaborative-filtering/</link>
      <pubDate>Wed, 01 Mar 2017 11:31:34 -0300</pubDate>
      
      <guid>http://cirocavani.github.io/post/tensorflow-recomendacao-com-als-collaborative-filtering/</guid>
      <description>

&lt;p&gt;Esse artigo é sobre a análise do ALS implementado no TensorFlow. O ALS é um método para fatoração de matriz usado como algoritmo de &lt;em&gt;Collaborative Filtering&lt;/em&gt; em Sistemas de Recomendação. A análise consiste no treinamento e &lt;em&gt;tuning&lt;/em&gt; desse algoritmo e a avaliação do erro final. Para comparação, o mesmo algoritmo é implementado com o Spark. A metodologia usada tem características peculiares de como a Recomendação e o ALS funcionam. O resultado mostra que o Spark tem performance melhor que o TensorFlow no erro final.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Código&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Recommendation/ALS.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;motivação&#34;&gt;Motivação&lt;/h2&gt;

&lt;p&gt;Desde que comecei a trabalhar com Recomendação na Globo.com, já coloquei em Produção mais de uma implementação do ALS. É um algoritmo relativamente fácil de entender e que tem excelentes resultados na prática. Atualmente, a implementação que usamos em Produção é a do &lt;a href=&#34;https://youtu.be/Q0VXllYilM0&#34;&gt;Spark 2&lt;/a&gt;. O TensorFlow é uma tecnologia que possibilita a implementação de algoritmos mais sofisticados de Inteligência Artificial que tenho interesse em usar em Produção. Seria ideal que pudesse ser usado nos algoritmos mais comuns que tem boa performance.&lt;/p&gt;

&lt;p&gt;Com base nessa ideia, essa análise é uma primeira comparação entre essas duas implementações do TensorFlow e do Spark.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;IMPORTANTE&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Essa análise foi feita com um dataset pequeno com objetivo de facilitar o desenvolvimento, portanto, os resultados obtidos são apenas para ter uma ideia e não servervem para chegar em &lt;em&gt;conclusões definitivas&lt;/em&gt; sobre essas implementações.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Tomei conhecimento de que o TensorFlow tinha a implementação do ALS a partir de um vídeo do &lt;a href=&#34;https://events.withgoogle.com/tensorflow-dev-summit/&#34;&gt;TensorFlow Dev Summit&lt;/a&gt; que ocorreu em 15/Fevereiro (WALS no tempo 2:20):&lt;/p&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/Tuv5QYKU-MM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;


&lt;h2 id=&#34;introdução&#34;&gt;Introdução&lt;/h2&gt;

&lt;p&gt;A ideia geral é simples:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Usuários dão rating para alguns filmes e o algoritmo gera uma lista de outros filmes que o usuário também daria um bom rating.&lt;/p&gt;

&lt;p&gt;O ALS é um método de fatoração de matriz que é usado para &amp;lsquo;completar&amp;rsquo; os ratings dos filmes que o usuário não deu rating, baseado nos ratings que vários usuários deram aos filmes.&lt;/p&gt;

&lt;p&gt;Cada usuário e filme é transformado em um vetor de números (fatores) que são ajustados para representar o interesse do usuário em uma determinada característica de um filme (cada fator é um &amp;lsquo;peso&amp;rsquo; que indica quanto o usuário gosta e quanto o filme oferece). O produto entre os fatores do usuário e os fatores do filme tem que ser &amp;lsquo;igual&amp;rsquo; ao rating que o usuário deu ao filme.&lt;/p&gt;

&lt;p&gt;No caso dos filmes que o usuário não deu rating (não viu?), esse produto é o &amp;lsquo;rating previsto&amp;rsquo;. Ordenando todos os ratings previstos, os maiores são usados para recomendação.&lt;/p&gt;

&lt;p&gt;Esse é o algoritmo de Collaborative Filtering com ALS.&lt;/p&gt;

&lt;p&gt;Esse é o algoritmo que ficou famoso no prêmio Netflix.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nesse trabalho, a análise do ALS consiste em:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Preparação de Dados: ratings do MovieLens, dataset para treinamento, validação e teste&lt;/li&gt;
&lt;li&gt;Treinamento com TensorFlow: algoritmo que completa a matriz de ratings com ALS do TensorFlow&lt;/li&gt;
&lt;li&gt;Treinamento com Spark: algoritmo que completa a matriz de ratings com ALS do Spark&lt;/li&gt;
&lt;li&gt;Seleção de Parâmetros: busca da combinação com menor erro no dataset de validação&lt;/li&gt;
&lt;li&gt;Comparação: avaliação do erro no dataset de teste da melhor combinação de parâmetros&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;preparação-dos-dados&#34;&gt;Preparação dos Dados&lt;/h2&gt;

&lt;p&gt;Os dados usados nessa análise são do &lt;a href=&#34;https://grouplens.org/datasets/movielens/&#34;&gt;MovieLens&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MovieLens Small&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[ &lt;a href=&#34;http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html&#34;&gt;README&lt;/a&gt; ]
[ &lt;a href=&#34;http://files.grouplens.org/datasets/movielens/ml-latest-small.zip&#34;&gt;ZipFile&lt;/a&gt; ]&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This dataset (ml-latest-small) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 100004 ratings and 1296 tag applications across 9125 movies. These data were created by 671 users between January 09, 1995 and October 16, 2016. This dataset was generated on October 17, 2016.&lt;/p&gt;

&lt;p&gt;Users were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.&lt;/p&gt;

&lt;p&gt;The data are contained in the files links.csv, movies.csv, ratings.csv and tags.csv.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;O dataset consiste de 100.004 ratings registrados por 671 usuários em 9.066 vídeos (o número de vídeos com rating é menor que o número de vídeos com tag, 9.125). Como esperado, a matriz de usuários por vídeos é bastante esparsa: apenas 1,64% de ratings dos 6.083.286 (671 x 9.066) possíveis.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Diferente desse dataset, em que o número de usuários é bem menor que o número de itens (menor que &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;10&lt;/sub&gt;), na recomendação da Globo.com normalmente a proporção é inversa, ou seja, muito mais usuários do que itens - nas nossas próprias análises, essa é uma característica relevante.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Esse dataset é bastante pequeno e serve ao propósito de desenvolvido da análise e não para encontrar &amp;lsquo;grandes verdades&amp;rsquo;.&lt;/p&gt;

&lt;p&gt;A estratégia é dividir esses dados para treinamento, validação e teste. O dataset de treinamento será usado como os dados que o algoritmo conhece do usuário (e deve aprender sobre). O dataset de validação é para ser usado durante o treinamento para medir a performance do algoritmo, verificar overfitting (ou under) e fazer tuning de parâmetros. O dataset de teste será usado uma única vez para medir o desempenho final do algoritmo com os melhores parâmetros.&lt;/p&gt;

&lt;p&gt;O critério usado para dividir os dados é baseado em uma especificidade de Recomendação. No pipeline de Produção, um algoritmo é treinado com os dados históricos e tem sua performance avaliada em tempo real. Desconsiderando o impacto que a própria recomendação possa ter no consumo de itens, essa mesma &amp;lsquo;dinâmica temporal&amp;rsquo; é usada para dividir os dados.&lt;/p&gt;

&lt;p&gt;Os ratings são ordenados pelo timestamp em que foram feitos. Os primeiros 80% desses ratings são designados para treinamento / validação e os últimos 20% são designados para teste. Novamente, o primeiro dataset é ordenado e dividido em 80% para treinamento e 20% para validação. A divisão, portanto, fica 64% para treinamento, 16% para validação e 20% para teste.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Uma variação desse critério: separar por tempo primeiro entre 70% treinamento e 30% validação / teste, depois separar por shuffle 15% de validação e 15% de teste. (Escolhi usar só o critério de tempo porque é mais próximo de Produção)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;O dataset de treinamento tem 64.002 ratings, 435 usuários e 5.668 vídeos.&lt;/p&gt;

&lt;p&gt;O dataset de validação tem 16.001 ratings, 136 usuários e 4.112 vídeos.&lt;/p&gt;

&lt;p&gt;O dataset de teste tem 20.001 ratings, 147 usuários e 4.753 vídeos.&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;A medida de performance usada nesse análise é o RMSE (&lt;a href=&#34;https://en.wikipedia.org/wiki/Root-mean-square_error&#34;&gt;Root Mean Square Error&lt;/a&gt;) onde o &amp;lsquo;erro&amp;rsquo; é a diferença entre o rating atribuído pelo usuário a um vídeo e o rating calulado pelo produto entre o vetor de fatores desse usuário e o vetor de fatores desse vídeo. O RMSE é uma medida aproximada de quanto o algoritmo pode errar a predição de rating, para mais ou para menos. A expectativa é que esse valor seja muito pequeno para o dataset de treinamento (o ALS minimiza um função similar ao RMSE).&lt;/p&gt;

&lt;p&gt;Para efeito de avaliação de performance, temos uma especificidade do ALS. Uma vez que é necessário ter o vetor de fatores tanto do usuário quanto do vídeo para estimar o rating, apenas usuários e vídeos que estão simultaneamente no dataset de treinamento e validação (ou teste) podem ser considerados para o cálculo do RMSE. Nesse caso, estamos avaliando a capacidade de predição do algoritmo e ignorando a cobertura (tanto em usuários ou vídeos).&lt;/p&gt;

&lt;p&gt;Apenas um subconjunto do dataset de validação e teste é usado para avaliação.&lt;/p&gt;

&lt;p&gt;(Todo o dataset de treinamento pode ser usado na avaliação)&lt;/p&gt;

&lt;p&gt;A avaliação com o dataset de validação tem 944 ratings, 23 usuários e 2.424 vídeos.&lt;/p&gt;

&lt;p&gt;A avaliação com o dataset de teste tem 278 ratings, 5 usuários e 2.332 vídeos.&lt;/p&gt;

&lt;h2 id=&#34;treinamento-com-tensorflow&#34;&gt;Treinamento com TensorFlow&lt;/h2&gt;

&lt;p&gt;A implementação do algoritmo foi baseada na documentação da classe WALS do TensorFlow e nos testes dessa classe.&lt;/p&gt;

&lt;p&gt;Documentação da implementação do WALS:&lt;/p&gt;

&lt;p&gt;[ &lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/v1.0.0/tensorflow/contrib/factorization/python/ops/factorization_ops.py#L53-L166&#34;&gt;GitHub: factorization_ops.py#L53-L166&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;Documentação dos parâmetros do WALS:&lt;/p&gt;

&lt;p&gt;[ &lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/v1.0.0/tensorflow/contrib/factorization/python/ops/factorization_ops.py#L181-L214&#34;&gt;GitHub: factorization_ops.py#L181-L214&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;Código do teste da classe WALS:&lt;/p&gt;

&lt;p&gt;[ &lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/v1.0.0/tensorflow/contrib/factorization/python/ops/factorization_ops_test.py#L534-L576&#34;&gt;GitHub: factorization_ops_test.py#L534-L576&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;Código da loss function:&lt;/p&gt;

&lt;p&gt;[ &lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/df5d3cd42335e31bccb6c796169d000d73c747d3/tensorflow/contrib/factorization/python/ops/factorization_ops_test.py#L105-L158&#34;&gt;GitHub: factorization_ops_test.py#L105-L158&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;Rascunho do algoritmo:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Recommendation/ALS%20draft.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;O algoritmo é implementado em duas classes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;ALSRecommender&lt;/code&gt;: classe responsável pelo treinamento (recebe um dataset com ratings, calcula os fatores dos usuários e vídeos com o ALS e retorna o modelo com esses fatores)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ALSRecommenderModel&lt;/code&gt;: classe responsável pela inferência (recebe um par usuário e vídeo e retorna a predição do rating ou recebe um usuário e retorna os vídeos com maior rating para esse usuário)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;ALSRecommender&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A classe &lt;code&gt;ALSRecommender&lt;/code&gt; recebe três parâmetros:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;num_factors&lt;/code&gt; (default 10): número de fatores em que cada usuário e vídeo devem ser representados (valor muito grande pode resultar em overfitting, muito pequeno em underfitting; custo computacional, tamanho da matriz de usuários e vídeos)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;num_iters&lt;/code&gt; (default 10): número de repetições do método do ALS (a convergência normalmente é rápida, portando um número muito grande pode não ajudar muito; custo computacional)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;reg&lt;/code&gt; (default 1e-1): fator de regularização (impacto na convergência, valor muito grande pode resultar em instabilidade e um valor muito pequeno pode resultar em overfitting)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;O treinamento é implementado no método &lt;code&gt;fit&lt;/code&gt; e consiste em três passos: transformação dos dados em matriz esparsa, criação do ALS e execução do ALS.&lt;/p&gt;

&lt;p&gt;No final é retornanda uma instância do &lt;code&gt;ALSRecommenderModel&lt;/code&gt; com a matriz de usuários e matriz de vídeos (itens).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def fit(self, dataset, verbose=False):
    with tf.Graph().as_default(), tf.Session() as sess:
        input_matrix, mapping = self.sparse_input(dataset)
        model = self.als_model(dataset)
        self.train(model, input_matrix, verbose)
        row_factor = model.row_factors[0].eval()
        col_factor = model.col_factors[0].eval()
        return ALSRecommenderModel(row_factor, col_factor, mapping)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;O primeiro passo é a transformação de uma lista de ratings em uma matriz esparsa de usuários por vídeos, implementado no método &lt;code&gt;sparse_input&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sparse_input(self, dataset):
    mapping = new_mapping(dataset)

    indices = [(mapping.users_to_idx[r.user_id],
                mapping.items_to_idx[r.item_id])
               for r in dataset.ratings]
    values = [r.rating for r in dataset.ratings]
    shape = (dataset.n_users, dataset.n_items)

    return tf.SparseTensor(indices, values, shape), mapping
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;O segundo passo é a construção do ALS para calcular os fatores e &amp;lsquo;completar&amp;rsquo; os ratings, implementado no método &lt;code&gt;als_model&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def als_model(self, dataset):
    return WALSModel(
        dataset.n_users,
        dataset.n_items,
        self.num_factors,
        regularization=self.regularization,
        unobserved_weight=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;O tercero passo é a execução do ALS em si, que consiste na repetição de dois passos: mantem a matriz de vídeos constante e altera a matriz de usuários; mantem a matriz de usuários constante e altera a matriz de vídeos. A cada passo, o erro entre os ratings do input e os ratings aproximados deve diminuir.&lt;/p&gt;

&lt;p&gt;Execução do ALS implementada no método &lt;code&gt;train&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def train(self, model, input_matrix, verbose=False):
    rmse_op = self.rmse_op(model, input_matrix) if verbose else None

    row_update_op = model.update_row_factors(sp_input=input_matrix)[1]
    col_update_op = model.update_col_factors(sp_input=input_matrix)[1]

    model.initialize_op.run()
    model.worker_init.run()
    for _ in range(self.num_iters):
        # Update Users
        model.row_update_prep_gramian_op.run()
        model.initialize_row_update_op.run()
        row_update_op.run()
        # Update Items
        model.col_update_prep_gramian_op.run()
        model.initialize_col_update_op.run()
        col_update_op.run()

        if verbose:
            print(&#39;RMSE: {:,.3f}&#39;.format(rmse_op.eval()))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;ALSRecommenderModel&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A classe &lt;code&gt;ALSRecommenderModel&lt;/code&gt; recebe três parâmetros:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;user_factors&lt;/code&gt;: matriz densa de usuários por número de fatores&lt;/li&gt;
&lt;li&gt;&lt;code&gt;item_factors&lt;/code&gt;: matriz densa de vídeos por número de fatores&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mapping&lt;/code&gt;: objeto que converte &lt;code&gt;user_id&lt;/code&gt; para / de índice em &lt;code&gt;user_factors&lt;/code&gt;, &lt;code&gt;item_id&lt;/code&gt; para / de índice em &lt;code&gt;item_factors&lt;/code&gt; (vídeos)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A classe &lt;code&gt;ALSRecommenderModel&lt;/code&gt; implementa dois métodos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;transform&lt;/code&gt;: recebe uma lista de &lt;code&gt;(user_id, item_id)&lt;/code&gt; e retorna a predição do rating&lt;/li&gt;
&lt;li&gt;&lt;code&gt;recommend&lt;/code&gt;: recebe um &lt;code&gt;user_id&lt;/code&gt; e retorna a lista de &lt;code&gt;(item_id, rating)&lt;/code&gt; ordenada com os maiores ratings primeiro&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;O método &lt;code&gt;transform&lt;/code&gt; é o produto dos fatores do usuário e do vídeo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def transform(self, x):
    for user_id, item_id in x:
        if user_id not in self.mapping.users_to_idx \
            or item_id not in self.mapping.items_to_idx:
            yield (user_id, item_id), 0.0
            continue
        i = self.mapping.users_to_idx[user_id]
        j = self.mapping.items_to_idx[item_id]
        u = self.user_factors[i]
        v = self.item_factors[j]
        r = np.dot(u, v)
        yield (user_id, item_id), r
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;O método &lt;code&gt;recommend&lt;/code&gt; é o produto da matriz de vídeos pelo vetor de fatores de um usuário:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def recommend(self, user_id, num_items=10, items_exclude=set()):
    i = self.mapping.users_to_idx[user_id]
    u = self.user_factors[i]
    V = self.item_factors
    P = np.dot(V, u)
    rank = sorted(enumerate(P), key=lambda p: p[1], reverse=True)

    top = list()
    k = 0
    while k &amp;lt; len(rank) and len(top) &amp;lt; num_items:
        j, r = rank[k]
        k += 1

        item_id = self.mapping.items_from_idx[j]
        if item_id in items_exclude:
            continue

        top.append((item_id, r))

    return top
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Execução&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A execução consiste em instanciar a classe &lt;code&gt;ALSRecommender&lt;/code&gt;, fazer o treinamento com o método &lt;code&gt;fit&lt;/code&gt; e fazer inferências com a instância da classe &lt;code&gt;ALSRecommenderModel&lt;/code&gt; retornada.&lt;/p&gt;

&lt;p&gt;Nesse exemplo, a inferência é executada para todos os ratings de avaliação como definido na Preparação de Dados. Com o rating da inferência, é calculado o RMSE de cada conjunto de avaliação.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;als = ALSRecommender(num_factors=10, num_iters=10, reg=0.1)
print(&#39;Training...\n&#39;)
als_model = als.fit(train_data, verbose=True)
print(&#39;\nEvaluation...\n&#39;)
eval_rmse(als_model)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Saída:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;Training...

RMSE: 1.729
RMSE: 0.765
RMSE: 0.631
RMSE: 0.588
RMSE: 0.565
RMSE: 0.550
RMSE: 0.540
RMSE: 0.532
RMSE: 0.526
RMSE: 0.521

Evaluation...

RMSE (train): 0.521
RMSE (validation): 1.688
RMSE for heavy: 2.444
RMSE for moderate: 1.465
RMSE for accidental: 1.926
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;treinamento-com-spark&#34;&gt;Treinamento com Spark&lt;/h2&gt;

&lt;p&gt;Documentação da implementação:&lt;/p&gt;

&lt;p&gt;[ &lt;a href=&#34;http://spark.apache.org/docs/2.1.0/ml-collaborative-filtering.html&#34;&gt;Manual&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;Documentação dos parâmetros do ALS:&lt;/p&gt;

&lt;p&gt;[ &lt;a href=&#34;http://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#module-pyspark.ml.recommendation&#34;&gt;Python API&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;Exemplo do ALS:&lt;/p&gt;

&lt;p&gt;[ &lt;a href=&#34;https://github.com/apache/spark/blob/v2.1.0/examples/src/main/python/ml/als_example.py&#34;&gt;GitHub: als_example.py&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;A execução consiste em instanciar a classe &lt;code&gt;ALS&lt;/code&gt;, fazer o treinamento com o método &lt;code&gt;fit&lt;/code&gt; e fazer inferências com a instância da classe &lt;code&gt;ALSModel&lt;/code&gt; retornada.&lt;/p&gt;

&lt;p&gt;Nesse exemplo, a inferência é executada para todos os ratings de avaliação como definido na Preparação de Dados. Com o rating da inferência, é calculado o RMSE de cada conjunto de avaliação.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.ml.recommendation import ALS as SparkALS

spark_als = SparkALS(rank=10, maxIter=10, regParam=0.1)
spark_model = spark_als.fit(train_df)
eval_rmse_spark(spark_model)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Saída:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;RMSE (train): 0.601
RMSE (validation): 1.018
RMSE for heavy: 1.128
RMSE for moderate: 0.974
RMSE for accidental: 1.327
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;seleção-de-parâmetros&#34;&gt;Seleção de Parâmetros&lt;/h2&gt;

&lt;p&gt;A Seleção de Parâmetros consiste em uma busca executando todas as combinações de valores dos parâmetros. Para limitar a busca, é pré-selecionado um conjunto de valores que faz mais sentido.&lt;/p&gt;

&lt;p&gt;Nessa análise, foi usada uma seleção de valores ainda menor, visando agilizar o processo.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;default_params = dict(num_factors=[5, 10, 20, 50, 100, 200],
                      num_iters=[5, 10, 25],
                      reg = [1e-5, 1e-3, 1e-1, 0.0, 1])

small_params = dict(num_factors=[5, 10, 20],
                    num_iters=[5],
                    reg = [1e-3, 1e-1, 1])

def grid_search(eval_func, params=default_params, verbose=False):
    best_rmse = None
    best_params = None
    for reg in params[&#39;reg&#39;]:
        for num_iters in params[&#39;num_iters&#39;]:
            for num_factors in params[&#39;num_factors&#39;]:
                if verbose:
                    print(&#39;\nParams:&#39;, num_factors, num_iters, reg)
                try:
                    rmse = eval_func(num_factors, num_iters, reg)
                except:
                    rmse = None
                if verbose:
                    print(&#39;RMSE:&#39;,
                          &#39;{:,.3f}&#39;.format(rmse) if rmse is not None else &#39;-&#39;)
                if rmse is not None and (best_rmse is None or rmse &amp;lt; best_rmse):
                    if verbose:
                        print(&#39;best update!&#39;)
                    best_rmse = rmse
                    best_params = (num_factors, num_iters, reg)
    return best_params, best_rmse
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TensorFlow ALS&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def tf_eval(num_factors, num_iters, reg):
    als = ALSRecommender(num_factors=num_factors, num_iters=num_iters, reg=reg)
    model = als.fit(train_data)
    return _rmse(model, valid_eval)

tf_params, tf_score = grid_search(tf_eval, params=small_params, verbose=True)
print()
print(&#39;Best Params:\n\nn_factors={}, n_iters={}, reg={}, RMSE={:.3f}&#39; \
        .format(*tf_params, tf_score))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Saída:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;Params: 5 5 0.001
RMSE: 1.146
best update!

Params: 10 5 0.001
RMSE: 1.309

Params: 20 5 0.001
RMSE: 2.870

Params: 5 5 0.1
RMSE: 1.355

Params: 10 5 0.1
RMSE: 1.438

Params: 20 5 0.1
RMSE: 1.636

Params: 5 5 1
RMSE: 1.487

Params: 10 5 1
RMSE: 1.941

Params: 20 5 1
RMSE: 1.933

Best Params:

n_factors=5, n_iters=5, reg=0.001, RMSE=1.146
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spark ALS&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def spark_eval(num_factors, num_iters, reg):
    als = SparkALS(rank=num_factors, maxIter=num_iters, regParam=reg)
    model = als.fit(train_df)
    return _rmse_spark(model, valid_df)

spark_params, spark_score = grid_search(spark_eval, params=small_params, verbose=True)
print()
print(&#39;Best Params:\n\nn_factors={}, n_iters={}, reg={}, RMSE={:.3f}&#39; \
        .format(*spark_params, spark_score))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Saída:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;Params: 5 5 0.001
RMSE: 1.300
best update!

Params: 10 5 0.001
RMSE: 1.418

Params: 20 5 0.001
RMSE: 1.615

Params: 5 5 0.1
RMSE: 0.981
best update!

Params: 10 5 0.1
RMSE: 1.003

Params: 20 5 0.1
RMSE: 1.033

Params: 5 5 1
RMSE: 1.258

Params: 10 5 1
RMSE: 1.258

Params: 20 5 1
RMSE: 1.258

Best Params:

n_factors=5, n_iters=5, reg=0.1, RMSE=0.981
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;comparação&#34;&gt;Comparação&lt;/h2&gt;

&lt;p&gt;Para a comparação das implementações, a medida de performance é o RMSE dos ratings de avaliação do dataset de Teste com os os melhores parâmetros selecionados na busca.&lt;/p&gt;

&lt;p&gt;O TensorFlow ALS com 5 fatores, 5 iterações e 0.001 de regularização tem RMSE de 1,183 no Teste.&lt;/p&gt;

&lt;p&gt;O Spark ALS com 5 fatores, 5 iterações e 0.1 de regularização tem RMSE de 1,086 no Teste.&lt;/p&gt;

&lt;p&gt;Esse resultado mostra que o Spark tem performance melhor que o TensorFlow no erro final.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TensorFlow ALS&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;als = ALSRecommender(*tf_params)
model = als.fit(train_data)
rmse = _rmse(model, test_eval)
print(&#39;TensorFlow RMSE for test: {:,.3f}&#39;.format(rmse))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Saída:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;TensorFlow RMSE for test: 1.183
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Spark ALS&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;num_factors, num_iters, reg = spark_params
als = SparkALS(rank=num_factors, maxIter=num_iters, regParam=reg)
model = als.fit(train_df)
rmse = _rmse_spark(model, test_df)
print(&#39;Spark RMSE for test: {:,.3f}&#39;.format(rmse))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Saída:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;Spark RMSE for test: 1.086
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusão&#34;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;Apesar do resultado dessa análise ser consistente (várias execuções, pouca variação), não é definitivo. Seria necessário fazer a análise com datasets maiores e verificar se há realmente diferença significativa de performance entre essas implementação.&lt;/p&gt;

&lt;p&gt;Para trabalhos futuros, a ideia completar o trabalho com os passos necessários para colocar esse algoritmo &amp;lsquo;em produção&amp;rsquo;, ou seja, que um sistema de recomendação possa fazer inferência com o modelo treinado com o ALS do TensorFlow. Uma forma de fazer isso é transformar a classe &lt;code&gt;ALSRecommenderModel&lt;/code&gt; em um grafo do TensorFlow que possa ser carregado e executado pelo TensorFlow Serving. Esse pode ser o tema de um próximo artigo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TensorFlow no Jupyter (com notebooks)</title>
      <link>http://cirocavani.github.io/post/tensorflow-no-jupyter-com-notebooks/</link>
      <pubDate>Tue, 13 Sep 2016 21:13:26 -0300</pubDate>
      
      <guid>http://cirocavani.github.io/post/tensorflow-no-jupyter-com-notebooks/</guid>
      <description>

&lt;p&gt;Esse tutorial é sobre o TensorFlow no Jupyter. A princípio, esse projeto pode ser usado para instalar automaticamente o Jupyter Notebook configurado com TensorFlow 0.10 e alguns notebooks de exemplo (tutoriais do TensorFlow). Outro objetivo é servir como base para criação de configurações customizadas isoladas (exemplo um ambiente extra para testar com TensorFlow GPU Python 3 com CUDA 8). O Jupyter é uma ferramenta excelente para testar ideias e prototipar rapidamente com TensorFlow.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Projeto&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-jupyter&#34;&gt;https://github.com/cirocavani/tensorflow-jupyter&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Esse artigo consiste em:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;o procedimento de instalação básico&lt;/li&gt;
&lt;li&gt;a descrição dos notebooks de exemplo&lt;/li&gt;
&lt;li&gt;a explicação de como funciona a instalação&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;instalação&#34;&gt;Instalação&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/cirocavani/tensorflow-jupyter.git
cd tensorflow-jupyter

#bin/setup-linux
bin/setup-mac
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Comandos:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/jupyter
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inicializa o Jupyter que já tem o kernel do TensorFlow configurado.&lt;/p&gt;

&lt;p&gt;Acesso em &lt;a href=&#34;http://localhost:8888/&#34;&gt;http://localhost:8888/&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/tensorboard
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inicializa a ferramenta de visualização do TensorFlow, mostra grafo de execução, valores de medições do treinamento.&lt;/p&gt;

&lt;p&gt;Acesso em &lt;a href=&#34;http://localhost:6006/&#34;&gt;http://localhost:6006/&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;notebooks-exemplo&#34;&gt;Notebooks Exemplo&lt;/h2&gt;

&lt;p&gt;Os notebooks exemplo são baseados nos tutorias disponiveis no site do TensorFlow. Os tutoriais originais estão referenciados no início do notebook. O código de alguns tutoriais foi alterado para usar algumas funcionalidades mais &amp;ldquo;reais&amp;rdquo; (por exemplo: leitura de CSV em batch).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;0 - First Run&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/00%20-%20First%20Run.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Hello World com TensorFlow.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1 - Linear Regression&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/01%20-%20Linear%20Regression.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse exemplo, é feito uma regressão linear para o fit de uma reta em dados gerados sinteticamente pela função y = 0.1x + 0.3, ou seja, o TensorFlow aprende os parâmetros 0.1 e 0.3 de um dataset ruidoso.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2 - MNIST, Softmax Regression&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/02%20-%20MNIST%2C%20Softmax%20Regression.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse exemplo, é feito um classificador com uma regressão softmax para identificação de dígitos 0-9 em uma imagem. Dado na entrada uma imagem de 28x28 pixels de um dígito manuscrito, o classificador retorna 10 valores, cada um indicando a &amp;ldquo;probabilidade&amp;rdquo; de ser um dos dígitos que a variável representa. A acurácia é de 92%.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3 - MNIST, Convolutional Network&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/03%20-%20MNIST%2C%20Convolutional%20Network.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse exemplo, é feito um classificador com uma rede neural convolutiva para identificação de dígitos 0-9 em uma imagem. Dado na entrada uma imagem de 28x28 pixels de um dígito manuscrito, o classificador retorna 10 valores, cada um indicando a &amp;ldquo;probabilidade&amp;rdquo; de ser um dos dígitos que a variável representa. A acurácia é de 99%.&lt;/p&gt;

&lt;p&gt;A rede é formada por duas camadas de convolução, uma camada toda conectada, uma camada de dropout e uma camada de regressão softmax.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4 - MNIST, Feed-forward NN with Log&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/04%20-%20MNIST%2C%20Feed-forward%20NN%20with%20Log.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse exemplo, é feito um classificador com uma rede neural feed-forward para identificação de dígitos 0-9 em uma imagem. Dado na entrada uma imagem de 28x28 pixels de um dígito manuscrito, o classificador retorna 10 valores, cada um indicando a &amp;ldquo;probabilidade&amp;rdquo; de ser um dos dígitos que a variável representa. A acurácia é de 99%.&lt;/p&gt;

&lt;p&gt;A rede é formada por duas camadas toda conectada e uma camada de regressão softmax.&lt;/p&gt;

&lt;p&gt;O modelo treinado nesse notebook pode ser visualizado no TensorBoard.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5 - Iris, DNN Classifier (tf.contrib.learn)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/05%20-%20Iris%2C%20DNN%20Classifier%20%28tf.contrib.learn%29.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse exemplo, é feito um classificador com uma rede neural para classificação de 3 espécies de flor. Dada na entrada as medidas da sépala e da pétala, o classificador retorna a espécie 0 setosa, 1 versicolor e 2 virginica. A acurácia é de 97%.&lt;/p&gt;

&lt;p&gt;A rede é formada por 5 camadas.&lt;/p&gt;

&lt;p&gt;O modelo treinado nesse notebook pode ser visualizado no TensorBoard.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6 - Iris, DNN Classifier with Log (tf.contrib.learn)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/06%20-%20Iris%2C%20DNN%20Classifier%20with%20Log%20%28tf.contrib.learn%29.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse exemplo, é feito um classificador com uma rede neural para classificação de 3 espécies de flor. Dada na entrada as medidas da sépala e da pétala, o classificador retorna a espécie 0 setosa, 1 versicolor e 2 virginica. A acurácia é de 97%.&lt;/p&gt;

&lt;p&gt;A rede é formada por 5 camadas e é feito o monitoramento de métricas que podem ser visualizadas no log do notebook e no TensorBoard.&lt;/p&gt;

&lt;p&gt;O modelo treinado nesse notebook pode ser visualizado no TensorBoard.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;7 - Reading CSV&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/07%20-%20Reading%20CSV.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse exemplo, é feito o pipeline para leitura de dados de um arquivo CSV. O arquivo usado nesse estudo é o mesmo do Census.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;8 - Census, Logistic Regression Classifier (tf.contrib.learn)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/08%20-%20Census%2C%20Logistic%20Regression%20Classifier%20%28tf.contrib.learn%29.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse exemplo, é feito um classificador com uma regressão logística para classificação de rendimento maior que 50 mil dólares. Dada na entrada as informações do Census, o classificador retorna 1 mais de 50 mil e 0 menos de 50 mil. A acurácia é de 87%.&lt;/p&gt;

&lt;p&gt;O modelo treinado nesse notebook pode ser visualizado no TensorBoard.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;9 - Census, Combined Classifier (tf.contrib.learn)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/cirocavani/tensorflow-jupyter/blob/master/workspace/Examples/09%20-%20Census%2C%20Combined%20Classifier%20%28tf.contrib.learn%29.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse exemplo, é feito um classificador com a combinação de uma rede neural e uma regressão logística (treinadas em conjunto) para classificação de rendimento maior que 50 mil dólares. Dada na entrada as informações do Census, o classificador retorna 1 mais de 50 mil e 0 menos de 50 mil. A acurácia é de 93%.&lt;/p&gt;

&lt;p&gt;O modelo treinado nesse notebook pode ser visualizado no TensorBoard.&lt;/p&gt;

&lt;h2 id=&#34;funcionamento&#34;&gt;Funcionamento&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;IMPORTANTE:&lt;/p&gt;

&lt;p&gt;Essa é a descrição de como é feita a configuração do projeto, contudo esse procedimento já está definido no comando &lt;code&gt;bin/setup-linux&lt;/code&gt; (ou &lt;code&gt;bin/setup-mac&lt;/code&gt;) que deve ser executado ao invés desse procedimento.&lt;/p&gt;

&lt;p&gt;Esse passo a passo é para ajudar na customização do Projeto.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;O procedimento de instalação consiste em:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Instalar o Python 2.7 com &lt;code&gt;miniconda&lt;/code&gt; (Linux ou Mac)&lt;/li&gt;
&lt;li&gt;Instalar o Jupyter Notebook 4.2 no &lt;em&gt;environment&lt;/em&gt; &lt;code&gt;default&lt;/code&gt; do &lt;code&gt;conda2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Instalar o TensorFlow 0.10 em um &lt;em&gt;environment&lt;/em&gt; próprio (para Python 2.7)&lt;/li&gt;
&lt;li&gt;Instalar o kernel do Python no &lt;em&gt;environment&lt;/em&gt; do TensorFlow&lt;/li&gt;
&lt;li&gt;Configurar o kernel no Jupyter que é executado no &lt;em&gt;environment&lt;/em&gt; do TensorFlow&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A estrutura do projeto será:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;deps/conda2&lt;/code&gt;: instalação do Python 2.7 (&lt;code&gt;miniconda&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;deps/tensorflow-0.10&lt;/code&gt;: instalação do TensorFlow 0.10 (&lt;em&gt;environment&lt;/em&gt; isolado)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data/kernels/tensorflow-0.10/kernel.json&lt;/code&gt;: configuração do kernel no Jupyter para o TensorFlow&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ao final desse procedimento, a execução do Jupyter consiste de (&lt;code&gt;bin/jupyter&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export JUPYTER_DATA_DIR=`pwd`/data
deps/conda2/bin/jupyter notebook --no-browser --notebook-dir=`pwd`/workspace
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para a criação de uma customização, os passos 3, 4 e 5 devem ser ajustados para um novo &lt;em&gt;environment&lt;/em&gt; com configuração customizada.&lt;/p&gt;

&lt;h3 id=&#34;instalação-do-python&#34;&gt;Instalação do Python&lt;/h3&gt;

&lt;p&gt;A instalação do Python é feita usando o &lt;code&gt;miniconda&lt;/code&gt; para versão 2.7 (para Linux ou Mac).&lt;/p&gt;

&lt;p&gt;Os comandos de Python e Conda ficam disponíveis na pasta &lt;code&gt;deps/conda2/bin&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://conda.pydata.org/miniconda.html&#34;&gt;http://conda.pydata.org/miniconda.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(Linux)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -k -L \
    -O https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh

chmod +x Miniconda2-latest-Linux-x86_64.sh

./Miniconda2-latest-Linux-x86_64.sh -b -f -p deps/conda2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;instalação-do-jupyter&#34;&gt;Instalação do Jupyter&lt;/h3&gt;

&lt;p&gt;O Jupyter tem um meta pacote que depende de todos os componentes, incluindo o Notebook.&lt;/p&gt;

&lt;p&gt;O comando do Jupyter fica disponível em &lt;code&gt;deps/conda2/bin/jupyter&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://jupyter.readthedocs.io/en/latest/install.html&#34;&gt;http://jupyter.readthedocs.io/en/latest/install.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://pypi.python.org/pypi/jupyter&#34;&gt;https://pypi.python.org/pypi/jupyter&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;deps/conda2/bin/pip install --upgrade jupyter
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;instalação-do-tensorflow&#34;&gt;Instalação do TensorFlow&lt;/h3&gt;

&lt;p&gt;O TensorFlow é distribuído como um pacote Wheel e é instalado em um &lt;em&gt;environment&lt;/em&gt; isolado criado no &lt;code&gt;miniconda&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;O comando do TensorBoard fica disponível em &lt;code&gt;deps/tensorflow-0.10/bin/tensorboard&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#anaconda-installation&#34;&gt;https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#anaconda-installation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(Linux)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;deps/conda2/bin/conda create -y -p deps/tensorflow-0.10 python=2.7

deps/tensorflow-0.10/bin/pip install \
    https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;instalação-do-kernel-para-o-tensorflow&#34;&gt;Instalação do Kernel para o TensorFlow&lt;/h3&gt;

&lt;p&gt;No &lt;em&gt;environment&lt;/em&gt; do TensorFlow é instalado o kernel Python que possibilita a conexão a partir do Jupyter (Notebook). Isso torna possível escrever código Python que é executado dentro desse &lt;em&gt;environment&lt;/em&gt; isolado.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://ipython.readthedocs.io/en/stable/install/kernel_install.html&#34;&gt;http://ipython.readthedocs.io/en/stable/install/kernel_install.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://pypi.python.org/pypi/ipykernel&#34;&gt;https://pypi.python.org/pypi/ipykernel&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;deps/tensorflow-0.10/bin/pip install ipykernel
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;configuração-do-kernel-para-o-tensorflow&#34;&gt;Configuração do Kernel para o TensorFlow&lt;/h3&gt;

&lt;p&gt;O Jupyter é configurado com o comando que executa o kernel do Python dentro do &lt;em&gt;environment&lt;/em&gt; que tem o TensorFlow. O kernel é responsável por receber requisições do servidor do Jupyter e executar código Python no processo em que está executando. Esse processo é executado somente com os pacotes do próprio &lt;em&gt;environment&lt;/em&gt; (isolamento) e pacotes adicionais devem ser instalados nesse &lt;em&gt;environment&lt;/em&gt; sem conflito com outros &lt;em&gt;environments&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://jupyter-client.readthedocs.io/en/latest/kernels.html#kernelspecs&#34;&gt;https://jupyter-client.readthedocs.io/en/latest/kernels.html#kernelspecs&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p data/kernels/tensorflow-0.10-py2

echo &amp;quot;{
 \&amp;quot;display_name\&amp;quot;: \&amp;quot;TensorFlow 0.10 (CPU, Python 2)\&amp;quot;,
 \&amp;quot;language\&amp;quot;: \&amp;quot;python\&amp;quot;,
 \&amp;quot;argv\&amp;quot;: [
  \&amp;quot;`pwd`/deps/tensorflow-0.10/bin/python\&amp;quot;,
  \&amp;quot;-c\&amp;quot;,
  \&amp;quot;from ipykernel.kernelapp import main; main()\&amp;quot;,
  \&amp;quot;-f\&amp;quot;,
  \&amp;quot;{connection_file}\&amp;quot;
 ]
}&amp;quot; &amp;gt; data/kernels/tensorflow-0.10-py2/kernel.json
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusão&#34;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;O Jupyter é uma excelente ferramenta para exploração de ideias e desenvolvimento de código rápido. A facilidade de visualização e execução independente de células é muito prático. O desenvolvimento de aplicações mais complexas e com código mais estruturado já não é muito favorável.&lt;/p&gt;

&lt;p&gt;O TensorFlow é uma ferramenta sofisticada para desenvolvimento de algoritmos inteligentes. Algumas APIs podem ser complexas e de difícil entendimento, algumas vezes bem documentadas e outras não. A comunidade é muito engajada e o Google vem produzindo tutoriais, documentação e modelos muito úteis para o aprendizado.&lt;/p&gt;

&lt;p&gt;Aprender TensorFlow no Jupyter é o melhor caminho e essa é a proposta desse Projeto.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Compilação do TensorFlow 0.10 para Linux (com GPU)</title>
      <link>http://cirocavani.github.io/post/compilacao-do-tensorflow-0.10-para-linux-com-gpu/</link>
      <pubDate>Thu, 08 Sep 2016 22:06:49 -0300</pubDate>
      
      <guid>http://cirocavani.github.io/post/compilacao-do-tensorflow-0.10-para-linux-com-gpu/</guid>
      <description>

&lt;p&gt;Esse tutorial é sobre a construção do pacote do TensorFlow 0.10 para Linux com suporte a GPU. Para esse procedimento é usado o Docker com uma imagem do Ubuntu 16.04, GCC 5.4, Python 2.7, Cuda 8.0 (RC) e cuDNN 5.1. A motivação desse trabalho é usar o TensorFlow com as novas gerações de GPUs da Nvidia (&lt;a href=&#34;https://developer.nvidia.com/pascal&#34;&gt;Pascal&lt;/a&gt;). Um segundo objetivo é a criação de um pacote do TensorFlow com capacidades específicas (por exemplo, um &amp;ldquo;Compute Capability&amp;rdquo; específico).&lt;/p&gt;

&lt;p&gt;O procedimento também está disponível como um script para Docker (ainda é necessário fazer o download do Cuda manualmente).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-build&#34;&gt;https://github.com/cirocavani/tensorflow-build&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;compilação&#34;&gt;Compilação&lt;/h2&gt;

&lt;p&gt;O procedimento consiste em:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Instalar o Cuda 8.0rc com o patch para GCC 5.4&lt;/li&gt;
&lt;li&gt;Instalar o cuDNN 5.1 para Cuda 8.0&lt;/li&gt;
&lt;li&gt;Instalar o Java 8&lt;/li&gt;
&lt;li&gt;Instalar o Bazel 0.3&lt;/li&gt;
&lt;li&gt;Construir TensorFlow 0.10&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;O resultado é o pacote do TensorFlow para Python 2 e Linux (com GPU):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tensorflow-0.10.0-py2-none-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Baseado na documentação:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#installing-from-sources&#34;&gt;https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#installing-from-sources&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Um procedimento alternativo:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/tools/docker/Dockerfile.devel-gpu&#34;&gt;https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/tools/docker/Dockerfile.devel-gpu&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;download-do-cuda-8-0rc-cudnn-5-1&#34;&gt;Download do Cuda 8.0rc, cuDNN 5.1&lt;/h3&gt;

&lt;p&gt;É necessário o download dos pacotes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cuda_8.0.27_linux.run
cuda_8.0.27.1_linux.run
cudnn-8.0-linux-x64-v5.1.tgz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Esses pacotes devem ser colocados na pasta &lt;code&gt;build_deps/&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;No momento, a versão mais recente do Cuda é a 8.0 RC e só está disponível para download para membros do &lt;a href=&#34;https://developer.nvidia.com/accelerated-computing-developer&#34;&gt;Accelerated Computing Developer Program&lt;/a&gt; no site da Nvidia (o cadastro é gratuito).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-release-candidate-download&#34;&gt;https://developer.nvidia.com/cuda-release-candidate-download&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Select Target Platform:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Operating System = Linux
Architecture = x86_64
Distribution = Ubuntu
Version = 16.04
Installer Type = runfile (local)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Download:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Base Installer&lt;/strong&gt; - &lt;code&gt;cuda_8.0.27_linux.run&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Patch 1&lt;/strong&gt; - &lt;code&gt;cuda_8.0.27.1_linux.run&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&#34;https://developer.nvidia.com/rdp/cudnn-download&#34;&gt;https://developer.nvidia.com/rdp/cudnn-download&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Selecione:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. I Agree To the Terms of the cuDNN Software License Agreement
2. Download cuDNN v5.1 (August 10, 2016), for CUDA 8.0 RC
3. cuDNN v5.1 Library for Linux
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Download:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cudnn-8.0-linux-x64-v5.1.tgz&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;setup-inicial-no-docker-para-ubuntu-16-04&#34;&gt;Setup inicial no Docker para Ubuntu 16.04&lt;/h3&gt;

&lt;p&gt;Download dos demais pacotes necessários para o build:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd build_deps

curl -k -L \
  -H &amp;quot;Cookie: oraclelicense=accept-securebackup-cookie&amp;quot; \
  -O http://download.oracle.com/otn-pub/java/jdk/8u102-b14/jdk-8u102-linux-x64.tar.gz

curl -k -L \
  -O https://github.com/bazelbuild/bazel/releases/download/0.3.1/bazel-0.3.1-installer-linux-x86_64.sh

chmod +x cuda_8.0.27_linux.run
chmod +x cuda_8.0.27.1_linux.run
chmod +x bazel-0.3.1-installer-linux-x86_64.sh

cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Criação do Container com as dependências:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker create -t --name=tensorflow_build ubuntu:16.04
docker cp build_deps tensorflow_build:/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Execução do Shell no Container:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker start tensorflow_build
docker exec -i -t tensorflow_build /bin/bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Setup do Container:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;echo &#39;debconf debconf/frontend select Noninteractive&#39; | debconf-set-selections
echo &#39;APT::Install-Recommends &amp;quot;0&amp;quot;;&#39; &amp;gt; 01norecommend
mv 01norecommend /etc/apt/apt.conf.d

apt-get update
apt-get upgrade -y

apt-get install -y \
    build-essential \
    python-dev \
    python-wheel \
    python-setuptools \
    python-numpy \
    swig \
    zlib1g-dev \
    unzip \
    file \
    git \
    ca-certificates \
    rsync
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;instalação-do-cuda-8-0rc-e-cudnn-5-1&#34;&gt;Instalação do Cuda 8.0rc e cuDNN 5.1&lt;/h3&gt;

&lt;p&gt;(comandos a serem executados dentro do container)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;/build_deps/cuda_8.0.27_linux.run --silent --toolkit --override

/build_deps/cuda_8.0.27.1_linux.run --silent --accept-eula

tar zxf /build_deps/cudnn-8.0-linux-x64-v5.1.tgz \
    -C /usr/local/cuda-8.0 --strip-components=1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;instalação-do-java-8&#34;&gt;Instalação do Java 8&lt;/h3&gt;

&lt;p&gt;(comandos a serem executados dentro do container)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;tar zxf /build_deps/jdk-8u102-linux-x64.tar.gz -C /opt --no-same-owner

echo &#39;export JAVA_HOME=/opt/jdk1.8.0_102&#39; &amp;gt; /etc/profile.d/java.sh
echo &#39;export PATH=$PATH:$JAVA_HOME/bin&#39; &amp;gt;&amp;gt; /etc/profile.d/java.sh
chmod a+x /etc/profile.d/java.sh

source /etc/profile.d/java.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;instalação-do-bazel-0-3&#34;&gt;Instalação do Bazel 0.3&lt;/h3&gt;

&lt;p&gt;(comandos a serem executados dentro do container)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;/build_deps/bazel-0.3.1-installer-linux-x86_64.sh --prefix=/opt/bazel-0.3.1

echo &#39;export PATH=$PATH:/opt/bazel-0.3.1/bin&#39; &amp;gt; /etc/profile.d/bazel.sh
chmod a+x /etc/profile.d/bazel.sh

source /etc/profile.d/bazel.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;construção-do-tensorflow-0-10&#34;&gt;Construção do TensorFlow 0.10&lt;/h3&gt;

&lt;p&gt;Considerações:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Configuração da GPU&lt;/p&gt;

&lt;p&gt;É necessário definir qual &amp;ldquo;Compute Capability&amp;rdquo; o binário do TensorFlow vai suportar.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-gpus&#34;&gt;https://developer.nvidia.com/cuda-gpus&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Por exemplo:&lt;/p&gt;

&lt;p&gt;A GeForce GT 740M tem Compute Capability 3.0&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export TF_CUDA_COMPUTE_CAPABILITIES=3.0
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Uso de Memória&lt;/p&gt;

&lt;p&gt;O build executa várias tarefas em paralelo e o consumo de memória pode aumentar rapidamente.&lt;/p&gt;

&lt;p&gt;Para limitar o número de execuções paralelas é usada a opção &lt;code&gt;-j 4&lt;/code&gt; no build.&lt;/p&gt;

&lt;p&gt;Em um notebook com 8 cores (HT), 8G de memória é insuficiente.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(comandos a serem executados dentro do container)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;useradd -m tensorflow
passwd -d tensorflow

su - tensorflow

git clone https://github.com/tensorflow/tensorflow.git -b r0.10 ~/tensorflow-0.10

cd ~/tensorflow-0.10

export PYTHON_BIN_PATH=/usr/bin/python
export TF_NEED_GCP=0
export TF_NEED_CUDA=1
export GCC_HOST_COMPILER_PATH=/usr/bin/gcc
export TF_CUDA_VERSION=8.0
export CUDA_TOOLKIT_PATH=/usr/local/cuda-8.0
export TF_CUDNN_VERSION=5
export CUDNN_INSTALL_PATH=/usr/local/cuda-8.0
export TF_CUDA_COMPUTE_CAPABILITIES=3.0
./configure

bazel build -j 4 -c opt --config=cuda \
    //tensorflow/tools/pip_package:build_pip_package

bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOME

mv ~/tensorflow-0.10.0-py2-none-{any,linux_x86_64}.whl

# saindo su
exit

# saindo do container
exit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;Para baixar o pacote (fora do container):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker cp \
    tensorflow_build:/home/tensorflow/tensorflow-0.10.0-py2-none-linux_x86_64.whl \
    .
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusão&#34;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;O procedimento de build do TensorFlow não é complicado, mas pequenas variações podem atingir alguns bugs do build (&lt;a href=&#34;https://github.com/tensorflow/tensorflow/issues/3985&#34;&gt;exemplo&lt;/a&gt;). Com um script bem definido, fica fácil criar o pacote do TensorFlow.&lt;/p&gt;

&lt;p&gt;Com esse pacote, é possível usar o TensorFlow nas GPUs mais recentes da Nvidia.&lt;/p&gt;

&lt;p&gt;No próximo artigo será um tutorial de como configurar um ambiente de desenvolvimento com Jupyter.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TensorFlow: Integração com BigData</title>
      <link>http://cirocavani.github.io/post/tensorflow-integracao-bigdata/</link>
      <pubDate>Mon, 22 Aug 2016 22:00:00 -0300</pubDate>
      
      <guid>http://cirocavani.github.io/post/tensorflow-integracao-bigdata/</guid>
      <description>

&lt;p&gt;Esse artigo é sobre a criação de uma Aplicação com TensorFlow em que o treinamento é feito no YARN (Hadoop), o servidor de inferência é hospedado no Tsuru e as requisições são feitas por Aplicações Java/Scala. Esses são os desafios para colocar em produção na Globo.com aplicações de Inteligência Artificial. Nesse trabalho foram desenvolvidos projetos que são Provas de Conceito de como fazer essa Aplicação TensorFlow integrada com BigData (o código está disponível no GitHub).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Código&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc&#34;&gt;https://github.com/cirocavani/tensorflow-poc&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;motivação&#34;&gt;Motivação&lt;/h2&gt;

&lt;p&gt;O problema começou com: como criar uma Aplicação de AI? A proposta para resolver esse problema foi: TensorFlow.&lt;/p&gt;

&lt;p&gt;O problema passou a ser: o que é o TensorFlow? Como criar uma Aplicação com TensorFlow? como colocar essa Aplicação em Produção? como usar essa Aplicação em um produto para entregar valor à empresa e ao usuário?&lt;/p&gt;

&lt;p&gt;A preparação para resolver esse problema começou a ser feita no início de 2016, no primeiro Hackday em Janeiro e teve um &amp;ldquo;evento crucial&amp;rdquo; no final de Junho, quando o Google publicou em seu blog de pesquisa o uso do TensorFlow para recomendação de app no Google Play.&lt;/p&gt;

&lt;p&gt;O TensorFlow é uma biblioteca usada para treinar algoritmos com dados. Esses &amp;ldquo;algoritmos&amp;rdquo; podem ser &amp;ldquo;executados&amp;rdquo; para gerar resultados que podem ser usados em &amp;ldquo;aplicações inteligentes&amp;rdquo;. Um exemplo de aplicação que pode usar um &amp;ldquo;algoritmo&amp;rdquo; do TensorFlow é Recomendação.&lt;/p&gt;

&lt;p&gt;O problema tornou-se: como usar o TensorFlow em Recomendação?&lt;/p&gt;

&lt;p&gt;Para resolver esse problema, dividi em duas frentes de trabalho: integrar o TensorFlow na infra de BigData, e; desenvolver uma aplicação de TensorFlow para Recomendação.&lt;/p&gt;

&lt;p&gt;A primeira frente de trabalho está feita.&lt;/p&gt;

&lt;h2 id=&#34;por-que-o-tensorflow&#34;&gt;Por que o TensorFlow?&lt;/h2&gt;

&lt;p&gt;Nos últimos anos, a tendência que eu venho observando do mercado é que AI finalmente se tornou a área de diferenciação, inovação e crescimento. Os investidores estão cada vez mais colocando dinheiro nessa área. As grandes empresas estão cada vez mais se posicionando como &amp;ldquo;empresas de AI&amp;rdquo;, sendo os maiores exemplos Google, Facebook e Microsoft, com Apple e Amazon chegando junto. Startups como a que fez a app Prisma valem bilhão de dólares.&lt;/p&gt;

&lt;p&gt;Enfim, AI é importante.&lt;/p&gt;

&lt;p&gt;A minha expectativa é usar Inteligência Artificial na Globo.com, incluindo produção e distribuição de conteúdo, desenvolvimento de produto, controle de infraestrutura, análises de segurança, &amp;hellip; seguir a tendência de Google, Facebook, Microsoft, Amazon, e também ser uma empresa de AI.&lt;/p&gt;

&lt;p&gt;Para o trabalho de fazer AI na Globo.com, a tecnologia escolhida foi o TensorFlow, framework de AI do Google. A estratégia escolhida para acelerar o progresso desse trabalho foi usar a infraestrutura de BigData que já tem bastante poder de processamento. Essa é a proposta inicial, com a expectativa de criar demanda suficiente para uma infraestrutura de AI com GPUs.&lt;/p&gt;

&lt;p&gt;O TensorFlow é um framework para construção de algoritmos que podem ser usados para processar texto, imagem e vídeo, para tratamento de linguagem natural, reconhecimento de objetos e faces, reconhecimento de padrões, construção de chatbots, construção de bots para jogos, &amp;hellip;, ou seja, é uma biblioteca de funções avançadas que o Google está usando para fazer AI e que está ganhando muita popularidade entre os desenvolvedores. Contudo, há um risco nesse controle direto do Google sobre uma tecnologia que está se tornando a &amp;ldquo;base de tudo&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Recentemente, duas reportagens discutiram o TensorFlow.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Here&amp;rsquo;s Why Google Is Open-Sourcing Some Of Its Most Important Technology&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.forbes.com/sites/gregsatell/2016/07/18/heres-why-google-is-open-sourcing-some-of-its-most-important-technology/&#34;&gt;http://www.forbes.com/sites/gregsatell/2016/07/18/heres-why-google-is-open-sourcing-some-of-its-most-important-technology/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Google Sprints Ahead in AI Building Blocks, Leaving Rivals Wary&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.bloomberg.com/news/articles/2016-07-21/google-sprints-ahead-in-ai-building-blocks-leaving-rivals-wary&#34;&gt;http://www.bloomberg.com/news/articles/2016-07-21/google-sprints-ahead-in-ai-building-blocks-leaving-rivals-wary&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Portanto, o primeiro passo na direção de &amp;ldquo;fazer AI na Globo.com&amp;rdquo; está sendo criar uma primeira Aplicação com TensorFlow.&lt;/p&gt;

&lt;p&gt;Esse artigo é sobre como isso foi feito.&lt;/p&gt;

&lt;h3 id=&#34;o-que-é-o-tensorflow&#34;&gt;O que é o TensorFlow?&lt;/h3&gt;

&lt;p&gt;TensorFlow é um framework para processamento de tensores (arrays multidimensionais) em ambientes heterogêneos (CPUs, GPUs, mobile). É um projeto open-source do Google liberado em Novembro/2015. A API principal é Python e a engine de execução é C++, o Google espera que a comunidade desenvolva bindings para outras linguagens.&lt;/p&gt;

&lt;p&gt;Na prática, o TensorFlow permite escrever algoritmos na forma de operações com tensores (arrays) que resultam em um grafo de execução. Esse grafo é otimizado e as operações são distribuídas para serem executadas em CPU ou GPU de acordo com o tipo de operação e a disponibilidade desses recursos. A engine pode ser estendida com novas operações que podem ter implementações para CPU e/ou GPU e serão usadas de acordo com os recursos. Essa capacidade de otimizar o processamento para recursos heterogêneos é o principal benefício do TensorFlow.&lt;/p&gt;

&lt;p&gt;O Google usa o TensorFlow para pesquisa e desenvolvimento de produtos que usam Deep Learning. Contudo, a proposta é que o framework seja usado para Machine Learning em geral. Essa proposta não torna o TensorFlow um framework para processamento geral, mas essa é uma possibilidade para o futuro.&lt;/p&gt;

&lt;p&gt;Atualmente, o Google espera que a comunidade use o TensorFlow para criar algoritmos de Machine Learning que, em um segundo momento, poderão se tornar a biblioteca de algoritmos do TensorFlow (hoje, ainda é escasso comparado aos líderes do mercado, scikit-learn e R/CRAN).&lt;/p&gt;

&lt;p&gt;Comparado com Spark que é um framework para processamento geral e que tem uma biblioteca de Machine Learning com vários algoritmos (com suporte a batch, stream, SQL e grafos), acredito que o TensorFlow tem um mecanismo de execução mais &amp;ldquo;moderno&amp;rdquo;. O processamento otimizado com recursos heterogêneos é um benefício que hoje não está disponível no Spark e pode se tornar indispensável para os processamentos que estão em demanda na atualidade. Ou seja, o TensorFlow parte de um modelo de execução baseado na demanda crescente de algoritmos inteligentes para processar dados enquanto o Spark é a evolução do modelo antigo de paralelização de processamento que pode estar com os dias contados.&lt;/p&gt;

&lt;p&gt;O TensorFlow ainda tem muito o que evoluir para que se torne o framework padrão em processamento de dados e Machine Learning, mas me parece que esse é o futuro para o qual estamos caminhando.&lt;/p&gt;

&lt;p&gt;Por fim, Jeff Dean, criador do MapReduce e muitas outras tecnologias de processamento distribuído e BigData, é um dos líderes do TensorFlow, o que dá ao projeto muita credibilidade.&lt;/p&gt;

&lt;p&gt;Para saber mais sobre o TensorFlow:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;TensorFlow&lt;/strong&gt;&lt;br&gt;
TensorFlow is an Open Source Software Library for Machine Intelligence&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;https://www.tensorflow.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/TensorFlow&#34;&gt;https://en.wikipedia.org/wiki/TensorFlow&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(código)&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorflow&#34;&gt;https://github.com/tensorflow/tensorflow&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(blog)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TensorFlow - Google’s latest machine learning system, open sourced for everyone&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://googleresearch.blogspot.com.br/2015/11/tensorflow-googles-latest-machine_9.html&#34;&gt;http://googleresearch.blogspot.com.br/2015/11/tensorflow-googles-latest-machine_9.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TensorFlow: Open source machine learning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=oZikw5k_2FM&#34;&gt;https://www.youtube.com/watch?v=oZikw5k_2FM&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(Paper)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TensorFlow: Large-scale machine learning on heterogeneous systems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://download.tensorflow.org/paper/whitepaper2015.pdf&#34;&gt;http://download.tensorflow.org/paper/whitepaper2015.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Os dois últimos releases do TensorFlow foram bastante interessantes:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;TensorFlow v0.9 now available with improved mobile support&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://developers.googleblog.com/2016/06/tensorflow-v09-now-available-with.html&#34;&gt;https://developers.googleblog.com/2016/06/tensorflow-v09-now-available-with.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Esse é o último release e destaca o uso do TensorFlow em aplicações Mobile, onde é possível treinar um modelo que executa no smartphone para reconhecimento de objeto usando o vídeo da câmera, em tempo real.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Announcing TensorFlow 0.8 – now with distributed computing support!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://research.googleblog.com/2016/04/announcing-tensorflow-08-now-with.html&#34;&gt;https://research.googleblog.com/2016/04/announcing-tensorflow-08-now-with.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nesse release, o destaque foi a funcionalidade de treinamento distribuído do modelo, onde antes só era possível usar uma máquina, agora é possível treinar com várias tanto para distribuir processamento quanto para paralelizar configurações distintas do modelo.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;experiência-com-tensorflow&#34;&gt;Experiência com TensorFlow&lt;/h3&gt;

&lt;p&gt;Já há algum tempo, venho trabalhando esporadicamente com o TensorFlow (nos últimos 3 hackdays na Globo.com, quase 9 meses). Recentemente, o Google divulgou um paper em que eles elaboram como usam o TensorFlow para fazer recomendação de apps no Google Play. Essa se tornou a oportunidade que eu encontrei para começar a trazer para a Globo.com a base para crescermos na área de AI.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Wide &amp;amp; Deep Learning: Better Together with TensorFlow&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://research.googleblog.com/2016/06/wide-deep-learning-better-together-with.html&#34;&gt;https://research.googleblog.com/2016/06/wide-deep-learning-better-together-with.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Wide &amp;amp; Deep Learning for Recommender Systems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/1606.07792&#34;&gt;http://arxiv.org/abs/1606.07792&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;De forma prática, o Google disponibilizou no TensorFlow uma API para criar um modelo e um paper que explica como eles usam esse modelo para fazer recomendação. Eles não disponibilizaram o &amp;ldquo;sistema de recomendação&amp;rdquo; propriamente dito que, na verdade, precisa ser &amp;ldquo;construído&amp;rdquo; a partir do TensorFlow.&lt;/p&gt;

&lt;p&gt;Portando, o trabalho consiste em entender a proposta de &amp;ldquo;como&amp;rdquo; fazer recomendação usando o TensorFlow, mapear esse &amp;ldquo;sistema&amp;rdquo; no &amp;ldquo;modus operandi&amp;rdquo; do Ambiente de BigData da Globo.com e avaliar o ganho de ter o TensorFlow como parte dessa plataforma.&lt;/p&gt;

&lt;p&gt;Eu comecei a fazer esse trabalho.&lt;/p&gt;

&lt;p&gt;Estou trabalhando em duas &amp;ldquo;frentes&amp;rdquo;: a primeira é genérica sobre integração do TensorFlow em BigData e a outra é específica sobre fazer recomendação com TensorFlow. Estou trabalhando em ambas, mas o foco desse artigo é na primeira porque acredito que seja mais interessante para todos que ainda não conhecem o TensorFlow.&lt;/p&gt;

&lt;h4 id=&#34;aplicação-tensorflow&#34;&gt;Aplicação TensorFlow&lt;/h4&gt;

&lt;p&gt;A primeira &amp;ldquo;frente&amp;rdquo; de trabalho está sendo ganhar experiência em como o &lt;strong&gt;TensorFlow&lt;/strong&gt; funciona e identificar como é possível acomodar esse processo na plataforma da Globo.com.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;TensorFlow - Google’s latest machine learning system, open sourced for everyone&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://research.googleblog.com/2015/11/tensorflow-googles-latest-machine_9.html&#34;&gt;https://research.googleblog.com/2015/11/tensorflow-googles-latest-machine_9.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;O &lt;strong&gt;TensorFlow&lt;/strong&gt; é uma biblioteca C++ com uma API em Python para criação de modelos (especificação do algoritmo / grafo de operações e parâmetros; processamento de dados / treinamento). O resultado do treinamento (e validação) é um &amp;ldquo;modelo de inferência&amp;rdquo; que deve ser &amp;ldquo;transferido&amp;rdquo; para a &amp;ldquo;aplicação&amp;rdquo; que vai &amp;ldquo;fazer&amp;rdquo; AI.&lt;/p&gt;

&lt;p&gt;Na prática, uma vez definido o grafo de operações e parâmetros, esse &amp;ldquo;programa&amp;rdquo; é alimentado com dados que vão ajustando os parâmetros. No final, esse grafo com parâmetros &amp;ldquo;aprendidos&amp;rdquo; é transformado em um arquivo que pode ser &amp;ldquo;carregado&amp;rdquo; na aplicação e usado para fazer predições / inferência.&lt;/p&gt;

&lt;p&gt;Essa é a fase do treinamento e é praticamente toda em Python.&lt;/p&gt;

&lt;p&gt;Uma vez que o &amp;ldquo;modelo de inferência&amp;rdquo; está feito, a próxima fase é fazer inferência.&lt;/p&gt;

&lt;p&gt;Em Recomendação, &amp;ldquo;inferência&amp;rdquo; seria fazer uma lista das matérias que mais interessam o Usuário. Esse resultado pode ser obtido de duas formas: off-line e on-line. No primeiro caso, uma aplicação pega todas as matérias disponíveis e executa a inferência em batch guardando os melhores resultados para cada um dos Usuário conhecidos (é isso que fazemos com o algoritmo de Fatoração de Matriz / ALS). No segundo caso, o modelo é usado na requisição na API de Conteúdo para gerar a recomendação para um Usuário (é isso que fazemos com o algoritmo do TF-IDF e gostaríamos de fazer com o ALS também). Considero a primeira solução muito custosa computacionalmente e lenta para refletir o interesse imediato / recente do Usuário. Acredito que a solução on-line é melhor e tem alguns casos de uso em que é a única que faz sentido.&lt;/p&gt;

&lt;p&gt;Portanto, a dúvida é: é possível fazer inferência em um modelo do TensorFlow de forma on-line?&lt;/p&gt;

&lt;p&gt;No início do ano, o Google tornou público um segundo projeto chamado &lt;strong&gt;TensorFlow Serving&lt;/strong&gt; que assume esse papel de executar inferências em modelos do TensorFlow de forma on-line.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Running your models in production with TensorFlow Serving&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://research.googleblog.com/2016/02/running-your-models-in-production-with.html&#34;&gt;https://research.googleblog.com/2016/02/running-your-models-in-production-with.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;O &lt;strong&gt;TensorFlow Serving&lt;/strong&gt; é uma biblioteca C++ para construção de &amp;ldquo;servidor&amp;rdquo; de inferência genérico, que já tem suporte para o protocolo HTTP2 usando gRPC (ProtoBuf) e integração com a biblioteca do TensorFlow para fazer inferência (incluindo atualização automática de modelos).&lt;/p&gt;

&lt;p&gt;Na prática, o TF Serving possibilita escrever APIs para modelos do TensorFlow. Uma aplicação C++ que recebe requisições em HTTP2 e executa a inferência (&amp;ldquo;predição&amp;rdquo;) com o modelo treinado. O principal benefício dessa biblioteca é automaticamente carregar novas versões dos modelos conforme eles vão sendo atualizados pelo treinamento.&lt;/p&gt;

&lt;p&gt;Essa é a fase da inferência on-line do modelo e é praticamente toda em C++.&lt;/p&gt;

&lt;h4 id=&#34;algoritmo-tensorflow&#34;&gt;Algoritmo TensorFlow&lt;/h4&gt;

&lt;p&gt;A outra &amp;ldquo;frente&amp;rdquo; de trabalho é paralela a primeira e consistem em desenvolver propriamente o algoritmo de recomendação baseado no paper do Google. Isso significa definir as features que serão usadas, como ler esses dados e alimentar o treinamento do modelo e como executar a inferência usando as matérias de um determinado Produto (Portal).&lt;/p&gt;

&lt;p&gt;Comecei a fazer esse trabalho também, mas vou deixar para discuti-lo em outro artigo.&lt;/p&gt;

&lt;h2 id=&#34;tensorflow-em-bigdata&#34;&gt;TensorFlow em BigData&lt;/h2&gt;

&lt;p&gt;O trabalho consistiu em identificar e resolver todos os requisitos para o desenvolvimento de uma Aplicação TensorFlow em uma infraestrutura de BigData.&lt;/p&gt;

&lt;p&gt;Para esse trabalho, a &amp;ldquo;anatomia&amp;rdquo; de uma Aplicação TensorFlow que foi considerada consiste em dois componentes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;o treinamento (aprendizado): o código é em Python, precisa de acesso a dados e poder de processamento.&lt;/p&gt;

&lt;p&gt;Esse programa deve ser empacotado para rodar no YARN (Hadoop, RedHat EL 6)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;a API de consulta (inferência): o código é em C++, recebe requisições com dados &amp;ldquo;reais&amp;rdquo; e retorna o resultado a partir da versão mais recente de um &amp;ldquo;modelo treinado&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Esse programa deve ser empacotado para rodar no Tsuru (Ubuntu LTS 14.04)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;Para integrar uma aplicação que usa essa &amp;ldquo;Inteligência Artificial&amp;rdquo;, é necessário usar um Cliente que &amp;ldquo;faça requisições&amp;rdquo; ao servidor de inferência (2).&lt;/p&gt;

&lt;p&gt;Essa funcionalidade pode ser implementada em qualquer linguagem e é feita com Python nos exemplos do TensorFlow.&lt;/p&gt;

&lt;p&gt;Para esse trabalho, o interesse é integrar com Aplicações em Scala, logo um cliente Java satisfaz o requisito (Java 7).&lt;/p&gt;

&lt;h2 id=&#34;provas-de-conceito&#34;&gt;Provas de Conceito&lt;/h2&gt;

&lt;p&gt;O trabalho consistiu no desenvolvimento de Provas de Conceito (POC) que exploram os desafios para se criar uma Aplicação TensorFlow.&lt;/p&gt;

&lt;p&gt;Todas as POCs rodam dentro do Docker e foram testadas no Linux e no Mac (usando Docker on Mac).&lt;/p&gt;

&lt;p&gt;Além do Docker, não é necessário mais nada instalado na máquina.&lt;/p&gt;

&lt;p&gt;A POC &lt;a href=&#34;#tflearn-wide-n-deep&#34;&gt;tflearn_wide_n_deep&lt;/a&gt; foi o aquecimento rodando um exemplo do TensorFlow.&lt;/p&gt;

&lt;p&gt;As POCs &lt;a href=&#34;#tfserving-basic&#34;&gt;tfserving_basic&lt;/a&gt;, &lt;a href=&#34;#tfserving-advanced&#34;&gt;tfserving_advanced&lt;/a&gt; e &lt;a href=&#34;#skeleton-project&#34;&gt;skeleton_project&lt;/a&gt; correspondem ao trabalho de criar um projeto que consiste de algoritmo Python para treinamento e servidor C++ para servir o modelo (tem um cliente Python para validar o funcionamento).&lt;/p&gt;

&lt;p&gt;As POCs &lt;a href=&#34;#yarn-training&#34;&gt;yarn_training&lt;/a&gt;, &lt;a href=&#34;#tensorflow-centos6&#34;&gt;tensorflow_centos6&lt;/a&gt;, &lt;a href=&#34;#tensorflow-installer&#34;&gt;tensorflow_installer&lt;/a&gt;, &lt;a href=&#34;#hadoop-centos6&#34;&gt;hadoop_centos6&lt;/a&gt; e &lt;a href=&#34;#hadoop-ubuntu1604&#34;&gt;hadoop_ubuntu1604&lt;/a&gt; correspondem ao trabalho de fazer o treinamento usando YARN (Hadoop) no RedHat EL 6 (produção).&lt;/p&gt;

&lt;p&gt;A POC &lt;a href=&#34;#client-java&#34;&gt;client_java&lt;/a&gt; corresponde ao trabalho de usar em uma aplicação Java/Scala um serviço do TensorFlow.&lt;/p&gt;

&lt;p&gt;A POC &lt;a href=&#34;#server-tsuru&#34;&gt;server_tsuru&lt;/a&gt; corresponde ao trabalho de criar uma app no Tsuru para servir um modelo treinado do TensorFlow.&lt;/p&gt;

&lt;h4 id=&#34;tflearn-wide-n-deep&#34;&gt;tflearn_wide_n_deep&lt;/h4&gt;

&lt;p&gt;A POC é a execução do tutorial sobre o &amp;ldquo;algoritmo&amp;rdquo; usado na recomendação de app do Google Play.&lt;/p&gt;

&lt;p&gt;Na prática, é um classificador binário que responde se uma pessoa ganha mais ou menos de 50 mil dólares baseado em dados do Censo, é uma combinação de Logistic Regression com Rede Neural.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/tflearn_wide_n_deep/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;tfserving-basic&#34;&gt;tfserving_basic&lt;/h4&gt;

&lt;p&gt;A POC é a execução do tutorial sobre o TensorFlow Serving sem versionamento do modelo.&lt;/p&gt;

&lt;p&gt;O &amp;ldquo;algoritmo&amp;rdquo; desse tutorial é um classificador de imagem que faz reconhecimento de dígito usando dataset MNIST e Rede Neural.&lt;/p&gt;

&lt;p&gt;Na prática, consiste de todo o processo de compilação do TensorFlow Serving e do TensorFlow para execução dos três requisitos desse trabalho: treinamento, servidor e cliente (essa &amp;ldquo;arquitetura&amp;rdquo; é o resultado final esperado para uma Aplicação TensorFlow).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/tfserving_basic/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;tfserving-advanced&#34;&gt;tfserving_advanced&lt;/h4&gt;

&lt;p&gt;A POC é a execução do tutorial sobre o TensorFlow Serving com versionamento do modelo.&lt;/p&gt;

&lt;p&gt;O &amp;ldquo;algoritmo&amp;rdquo; desse tutorial é um classificador de imagem que faz reconhecimento de dígito usando dataset MNIST e Rede Neural.&lt;/p&gt;

&lt;p&gt;Na prática, consiste de todo o processo de compilação do TensorFlow Serving e do TensorFlow para execução dos três requisitos desse trabalho: treinamento, servidor e cliente (essa &amp;ldquo;arquitetura&amp;rdquo; é o resultado final esperado para uma Aplicação TensorFlow).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/tfserving_advanced/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;skeleton-project&#34;&gt;skeleton_project&lt;/h4&gt;

&lt;p&gt;(código do exemplo extraído para um projeto fora da árvore do TensorFlow Serving)&lt;/p&gt;

&lt;p&gt;A POC é a criação de um projeto standalone baseado no código do tutorial sobre o TensorFlow Serving com versionamento do modelo.&lt;/p&gt;

&lt;p&gt;O &amp;ldquo;algoritmo&amp;rdquo; desse projeto é um classificador de imagem que faz reconhecimento de dígito usando dataset MNIST e Rede Neural.&lt;/p&gt;

&lt;p&gt;Na prática, consiste em separar o código para treinamento, servidor e cliente em um projeto que depende do TensorFlow Serving e usa a mesma ferramenta de build Bazel (fazendo o processo de compilação do TensorFlow Serving e do TensorFlow).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/skeleton_project/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;yarn-training&#34;&gt;yarn_training&lt;/h4&gt;

&lt;p&gt;(motivação: usar a infraestrutura de armazenamento e processamento do Hadoop para rodar o treinamento com TensorFlow)&lt;/p&gt;

&lt;p&gt;A POC é a criação de uma Aplicação YARN para rodar o treinamento com TensorFlow.&lt;/p&gt;

&lt;p&gt;Na prática, consiste em criar uma aplicação Java baseada no exemplo de execução de shell script distribuído do YARN, essa aplicação é dividida em duas partes, uma que faz submissão do script e a outra que controla dentro do cluster a execução.&lt;/p&gt;

&lt;p&gt;Esse procedimento depende do instalador do TensorFlow criado em &lt;a href=&#34;#tensorflow-installer&#34;&gt;tensorflow_installer&lt;/a&gt; por link simbólico e do container em &lt;a href=&#34;#hadoop-centos6&#34;&gt;hadoop_centos6&lt;/a&gt; estar rodando.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/yarn_training/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;tensorflow-centos6&#34;&gt;tensorflow_centos6&lt;/h4&gt;

&lt;p&gt;(motivação: o TensorFlow não é oficialmente suportado no RedHat EL 6, o binário é compilado para glibc 2.17 e o código C++ 11, ambos requisitos não disponíveis, mas é possível criar um binário do TensorFlow compatível)&lt;/p&gt;

&lt;p&gt;A POC é a construção do binário do TensorFlow compatível com RedHat EL 6.&lt;/p&gt;

&lt;p&gt;Na prática, consiste em instalar a versão mais recente do GCC disponível para o CentOS 6, construir a ferramenta de build Bazel (binário incompatível com a glibc) e construir o TensorFlow (com patch para linkage).&lt;/p&gt;

&lt;p&gt;Essa POC é só para criar um pacote do TensorFlow.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/tensorflow_centos6/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;tensorflow-installer&#34;&gt;tensorflow_installer&lt;/h4&gt;

&lt;p&gt;(motivação: evitar download no ambiente de produção e evitar enviar múltiplos arquivos para o Hadoop)&lt;/p&gt;

&lt;p&gt;A POC é a criação de um instalador para o algoritmo de treinamento com o TensorFlow que tenha todas as dependência e possa ser executado no RedHat EL 6.&lt;/p&gt;

&lt;p&gt;Na prática, consiste em criar um pacote com TensorFlow, Python (conda) e todas as dependências que é embutido em um shell script que faz a instalação e executa o treinamento do TensorFlow.&lt;/p&gt;

&lt;p&gt;Esse procedimento depende do pacote do TensorFlow criado em &lt;a href=&#34;#tensorflow-centos6&#34;&gt;tensorflow_centos6&lt;/a&gt; por link simbólico.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/tensorflow_installer/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;hadoop-centos6&#34;&gt;hadoop_centos6&lt;/h4&gt;

&lt;p&gt;(motivação: essa imagem corresponde a uma &amp;ldquo;aproximação&amp;rdquo; do ambiente de produção que usa RedHat EL 6 com o qual o CentOS6 é binário-compatível - o TensorFlow não é oficialmente suportado nesse sistema)&lt;/p&gt;

&lt;p&gt;A POC é a configuração mínima do Hadoop no CentOS 6 para execução do treinamento com TensorFlow no YARN.&lt;/p&gt;

&lt;p&gt;Na prática, consiste em rodar os servidores do HDFS (NameNode e DataNode) e do YARN (ResourceManager e NodeManager) para poder executar uma aplicação (ApplicationMaster) que instale o TensorFlow e rode o script Python de treinamento.&lt;/p&gt;

&lt;p&gt;Essa POC é só a configuração do Hadoop no CentOS 6.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/hadoop_centos6/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;hadoop-ubuntu1604&#34;&gt;hadoop_ubuntu1604&lt;/h4&gt;

&lt;p&gt;(motivação: essa imagem corresponde ao ambiente em que o TensorFlow é oficialmente suportado, diferente do ambiente de produção RedHat, ou seja, o comportamento nesse ambiente deve representar o &amp;ldquo;funcionamento correto&amp;rdquo;)&lt;/p&gt;

&lt;p&gt;A POC é a configuração mínima do Hadoop no Ubuntu para execução do treinamento com TensorFlow no YARN.&lt;/p&gt;

&lt;p&gt;Na prática, consiste em rodar os servidores do HDFS (NameNode e DataNode) e do YARN (ResourceManager e NodeManager) para poder executar uma aplicação (ApplicationMaster) que instale o TensorFlow e rode o script Python de treinamento.&lt;/p&gt;

&lt;p&gt;Essa POC é só a configuração do Hadoop no Ubuntu 16.04.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/hadoop_ubuntu1604/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;client-java&#34;&gt;client_java&lt;/h4&gt;

&lt;p&gt;A POC é a criação de um cliente Java para fazer inferência em um serviço do TensorFlow.&lt;/p&gt;

&lt;p&gt;Na prática, consiste em gerar o código do protocolo de comunicação usando o gRPC (Protobuf) e usar esse código para acessar o serviço do TensorFlow, a especificação do protocolo faz parte da implementação do serviço.&lt;/p&gt;

&lt;p&gt;O &amp;ldquo;algoritmo&amp;rdquo; desse projeto é um classificador de imagem que faz reconhecimento de dígito usando dataset MNIST e Rede Neural.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/client_java/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;server-tsuru&#34;&gt;server_tsuru&lt;/h4&gt;

&lt;p&gt;A POC é a criação de uma aplicação do Tsuru para rodar o servidor do TensorFlow.&lt;/p&gt;

&lt;p&gt;O &amp;ldquo;algoritmo&amp;rdquo; desse servidor é um classificador de imagem que faz reconhecimento de dígito usando dataset MNIST e Rede Neural.&lt;/p&gt;

&lt;p&gt;Na prática, consiste em fazer o deploy do binário do servidor do TensorFlow em uma app do Tsuru.&lt;/p&gt;

&lt;p&gt;Esse procedimento depende do binário do servidor do TensorFlow criado em &lt;a href=&#34;#skeleton-project&#34;&gt;skeleton_project&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cirocavani/tensorflow-poc/blob/master/server_tsuru/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusão&#34;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;Esse trabalho é o preparatório para a criação de Aplicações TensorFlow que serão colocadas em Produção na Globo.com.&lt;/p&gt;

&lt;p&gt;Ainda tem muitos desafios para completar esse trabalho, mas esse é um bom começo.&lt;/p&gt;

&lt;p&gt;O TensorFlow é um projeto fascinante e evolui muito rápido - uma excelente oportunidade de aprendizado e cooperação.&lt;/p&gt;

&lt;p&gt;Para trabalhos futuros, esses são alguns dos desafios:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;TFRecord: formato de dados do TensorFlow&lt;/p&gt;

&lt;p&gt;Como treinar um modelo a partir de dados armazenado em Parquet?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Treinamento Distribuído&lt;/p&gt;

&lt;p&gt;Como treinar um modelo com múltiplos containers no YARN?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tensorboard&lt;/p&gt;

&lt;p&gt;Como rodar o Tensorboard no ApplicationMaster para acompanhar o treinamento?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;TF Serving Source para Swift (OpenStack)&lt;/p&gt;

&lt;p&gt;Como armazenar os modelos como objetos no Swift (OpenStack)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;Em um próximo artigo, pretendo discutir uma Aplicação TensorFlow para Recomendação.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Otimização dos parâmetros do Spark ALS (Collaborative Filtering) usando MOE</title>
      <link>http://cirocavani.github.io/post/otimizacao-dos-parametros-do-spark-als-collaborative-filtering-usando-moe/</link>
      <pubDate>Thu, 24 Sep 2015 19:44:08 -0300</pubDate>
      
      <guid>http://cirocavani.github.io/post/otimizacao-dos-parametros-do-spark-als-collaborative-filtering-usando-moe/</guid>
      <description>&lt;p&gt;Esse tutorial é sobre otimização de parâmetros em modelos de machine learning. Para esse tutorial, a ferramenta utilizada é o MOE, Metric Optimization Engine, desenvolvido pelo Yelp que implementa o algoritmo de busca usando Gaussian Process. O algoritmo escolhido para ter os parâmetros otimizados é o Collaborative Filtering baseado na fatoração da matriz de preferências. De forma genérica, esse é um processo que pode ser facilmente adaptado para outros algoritmos e permite sistematizar a árdua tarefa de escolher os melhores parâmetros para um modelo.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://yelp.github.io/MOE/&#34;&gt;http://yelp.github.io/MOE/&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;MOE (Metric Optimization Engine) is an efficient way to optimize a system’s parameters, when evaluating parameters is time-consuming or expensive.&lt;/p&gt;

&lt;p&gt;How does MOE work?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Build a Gaussian Process (GP) with the historical data&lt;/li&gt;
&lt;li&gt;Optimize the hyperparameters of the Gaussian Process&lt;/li&gt;
&lt;li&gt;Find the point(s) of highest Expected Improvement (EI)&lt;/li&gt;
&lt;li&gt;Return the point(s) to sample, then repeat&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;Primeiramente, é feita a instalação do MOE. Nesse processo, é necessário configurar o ambiente para compilar as dependências do projeto e o código que é composto por Python e C++. No final desse procedimento, o serviço do MOE estará disponível como um servidor REST e a API Python que pode ser usada para definir o procedimento de otimização.&lt;/p&gt;

&lt;p&gt;O procedimento de instalação em detalhes é descrito aqui:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://yelp.github.io/MOE/install.html#install-from-source&#34;&gt;http://yelp.github.io/MOE/install.html#install-from-source&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;
mkdir grandesdados-opt
cd grandesdados-opt

virtualenv --no-site-packages --python=python2.7 moe-env

&amp;gt; Running virtualenv with interpreter /usr/bin/python2.7
&amp;gt; New python executable in moe-env/bin/python2.7
&amp;gt; Also creating executable in moe-env/bin/python
&amp;gt; Installing setuptools, pip, wheel...done.

source moe-env/bin/activate

git clone https://github.com/Yelp/MOE.git
cd MOE

pip install -r requirements.txt

&amp;gt; (...)
&amp;gt; Successfully installed (...)

python setup.py install

&amp;gt; (...)

pserve --reload development.ini

&amp;gt; (...)
&amp;gt; Starting server in PID 23232.
&amp;gt; serving on 0.0.0.0:6543 view at http://127.0.0.1:6543

# (nesse momento, esse terminal fica &#39;preso&#39; mostrando o log do servidor do MOE)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;O próximo passo é instalar o algoritmo que tem parâmetros que precisam ser otimizados.&lt;/p&gt;

&lt;p&gt;Nesse tutorial, será usado o algoritmo de Collaborative Filtering baseado na fatoração da matriz de preferências que gera um vetor para cada usuário e item da matriz original. Nesse algoritmo, os parâmetros são a dimensão do vetor a ser gerado (fatores latentes), o número de iterações para fatoração da matriz e o parâmetro de regularização usado na fatoração.&lt;/p&gt;

&lt;p&gt;O DataSet usado é um sample MovieLens que já vem na distribuição do Spark. São 1501 ratings, 30 usuários e 100 filmes.&lt;/p&gt;

&lt;p&gt;O código pode ser análisado aqui:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/apache/spark/blob/v1.5.0/examples/src/main/scala/org/apache/spark/examples/ml/MovieLensALS.scala&#34;&gt;https://github.com/apache/spark/blob/v1.5.0/examples/src/main/scala/org/apache/spark/examples/ml/MovieLensALS.scala&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(procedimento na mesma pasta anterior &lt;code&gt;grandesdados-opt&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -L -O http://ftp.unicamp.br/pub/apache/spark/spark-1.5.0/spark-1.5.0-bin-hadoop2.6.tgz
tar zxf spark-1.5.0-bin-hadoop2.6.tgz
cd spark-1.5.0-bin-hadoop2.6/

cp conf/log4j.properties{.template,}
sed -i s/log4j\.rootCategory\=INFO/log4j\.rootCategory\=ERROR/1 conf/log4j.properties

echo &amp;quot;spark.ui.showConsoleProgress=false&amp;quot; &amp;gt; conf/spark-defaults.conf

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Executando o exemplo do MovieLens (a saída são os parâmetros):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;
./bin/run-example ml.MovieLensALS

&amp;gt; Error: Missing option --ratings
&amp;gt; Error: Missing option --movies
&amp;gt; MovieLensALS: an example app for ALS on MovieLens data.
&amp;gt; Usage: MovieLensALS [options]
&amp;gt;
&amp;gt;   --ratings &amp;lt;value&amp;gt;
&amp;gt;         path to a MovieLens dataset of ratings
&amp;gt;   --movies &amp;lt;value&amp;gt;
&amp;gt;         path to a MovieLens dataset of movies
&amp;gt;   --rank &amp;lt;value&amp;gt;
&amp;gt;         rank, default: 10
&amp;gt;   --maxIter &amp;lt;value&amp;gt;
&amp;gt;         max number of iterations, default: 10
&amp;gt;   --regParam &amp;lt;value&amp;gt;
&amp;gt;         regularization parameter, default: 0.1
&amp;gt;   --numBlocks &amp;lt;value&amp;gt;
&amp;gt;         number of blocks, default: 10
&amp;gt;
&amp;gt; Example command line to run this app:
&amp;gt;
&amp;gt;  bin/spark-submit --class org.apache.spark.examples.ml.MovieLensALS \
&amp;gt;   examples/target/scala-*/spark-examples-*.jar \
&amp;gt;   --rank 10 --maxIter 15 --regParam 0.1 \
&amp;gt;   --movies data/mllib/als/sample_movielens_movies.txt \
&amp;gt;   --ratings data/mllib/als/sample_movielens_ratings.txt

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fazendo uma execução:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;
time ./bin/run-example ml.MovieLensALS \
--rank 10 --maxIter 15 --regParam 0.1 \
--movies data/mllib/als/sample_movielens_movies.txt \
--ratings data/mllib/als/sample_movielens_ratings.txt

&amp;gt; Got 1501 ratings from 30 users on 100 movies.                                   
&amp;gt; Training: 1169, test: 332.
&amp;gt; Test RMSE = 0.9815785141168548.                                                 
&amp;gt; Found 0 false positives                                                         
&amp;gt;
&amp;gt; real	0m22.441s
&amp;gt; user	0m56.320s
&amp;gt; sys	0m1.847s

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para fazer a otimização, o MOE requer que o problema seja modelado como uma função do vetor de parâmetros para um valor escalar. O objetivo da ferramenta é minimizar essa função.&lt;/p&gt;

&lt;p&gt;Para o problema do ALS, por simplicidade, vamos aproveitar que o exemplo já calcula o RMSE e usar a função que mapeia o vetor do Número de Fatores Latentes, Número de Iterações e Regularização para o RMSE. Faz sentido o objetivo ser minimizar o RMSE.&lt;/p&gt;

&lt;p&gt;Na pasta &lt;code&gt;grandesdados-opt&lt;/code&gt;, crie o arquivo &lt;code&gt;func.sh&lt;/code&gt; que mapeia a função desejada:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;#!/bin/bash

cd spark-1.5.0-bin-hadoop2.6/

./bin/run-example ml.MovieLensALS \
--rank $1 --maxIter $2 --regParam $3 \
--movies data/mllib/als/sample_movielens_movies.txt \
--ratings data/mllib/als/sample_movielens_ratings.txt 2&amp;gt;&amp;amp;1 \
| sed -n &#39;s/\(Test RMSE =\) \([0-9]*\.[0-9]*\)\./\2/p&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Dessa forma, teremos:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;chmod +x func.sh

./func.sh 10 15 0.1

&amp;gt; 0.9815785141168546
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Agora podemos definir o procedimento de otimização como um experimento do MOE.&lt;/p&gt;

&lt;p&gt;O experimento é criado com o domínio dos parâmetros que estamos querendo otimizar. Dado que estamos trabalhando com um DataSet limitado, podemos restringir os valores.&lt;/p&gt;

&lt;p&gt;Nesse exemplo, estamos usando:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Número de Fatores Latentes: entre 5 e 50&lt;/li&gt;
&lt;li&gt;Número de Iterações da Fatoração: entre 5 e 20&lt;/li&gt;
&lt;li&gt;Regularização da Fatoração: entre 0.001 e 1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Vamos assumir como primeiro ponto &amp;lsquo;ótimo&amp;rsquo; o que vem documentado no exemplo, ou seja, 10, 15 e 0,1.&lt;/p&gt;

&lt;p&gt;A busca por uma solução muito boa pode envolver muitas iterações do processo de otimização, nesse exemplo vamos usar 20, mas poderia ser 100 ou 400 para uma busca mais completa.&lt;/p&gt;

&lt;p&gt;Na pasta &lt;code&gt;grandesdados-opt&lt;/code&gt;, crie o arquivo &lt;code&gt;opt.py&lt;/code&gt; que define o procedimento de otimização:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import subprocess

from moe.easy_interface.experiment import Experiment
from moe.easy_interface.simple_endpoint import gp_next_points
from moe.optimal_learning.python.data_containers import SamplePoint

def function_to_minimize(x):
    f = &amp;quot;./func.sh {0:} {1:} {2:}&amp;quot;.format(int(x[0]), int(x[1]), x[2])
    print f
    y = subprocess.Popen(f, shell=True, stdout=subprocess.PIPE).stdout.read().strip()
    if y: print y
    return float(y)

if __name__ == &#39;__main__&#39;:
    exp = Experiment([[5, 50], [5, 20], [0.001, 1]])

    xmin = []
    ymin = 0.0

    for i in range(20):
        print &amp;quot;Sample {0:}&amp;quot;.format(i)
        try:
            x = [10.0, 15.0, 0.1] if i == 0 else gp_next_points(exp)[0]
            y = function_to_minimize(x)
            exp.historical_data.append_sample_points([
                SamplePoint(x, y, 0.05),
            ])
            if not xmin or y &amp;lt; ymin:
                xmin, ymin = x, y
        except ValueError:
            print &amp;quot;error&amp;quot;
        print

    print str(xmin)
    print str(ymin)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Por fim, o resultado:
&lt;br/&gt;(necessário estar dentro do virtualenv onde o MOE foi instalado)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python2 opt.py

&amp;gt; Sample 0
&amp;gt; ./func.sh 10 15 0.1
&amp;gt; 0.9815785141168545
&amp;gt; (...)
&amp;gt; Sample 19
&amp;gt; ./func.sh 16 19 0.41989952735
&amp;gt; error
&amp;gt;
&amp;gt; [46.6604641336, 19.9410400182, 0.0409527639665]
&amp;gt; 0.977384860516

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Como podemos ver, os parâmetros 46 para Número de Fatores Latentes, 19 para Iterações da Fatoração e 0,041 para Regularização resultaram em um erro menor nesse dataset de exemplo.&lt;/p&gt;

&lt;p&gt;Para mais informações sobre otimização usando o MOE, consulte a documentação.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Configuração do Hadoop, HBase e Kafka na Máquina Local com Docker</title>
      <link>http://cirocavani.github.io/post/configuracao-do-hadoop-hbase-e-kafka-na-maquina-local-com-docker/</link>
      <pubDate>Wed, 16 Sep 2015 23:26:07 -0300</pubDate>
      
      <guid>http://cirocavani.github.io/post/configuracao-do-hadoop-hbase-e-kafka-na-maquina-local-com-docker/</guid>
      <description>

&lt;p&gt;Esse tutorial é sobre a criação de uma imagem do Docker com a configuração local do Hadoop, HBase e Kafka. Nesse procedimento, o Hadoop é configurado no modo pseudo-distribuído com cada serviço rodando em uma instância própria da JVM, mas todas na mesma máquina. O HBase e o Kafka também rodam em modo &amp;lsquo;distribuído&amp;rsquo; compartilhando uma instância separada do ZooKeeper. Esse procedimento é muito útil para testar funcionalidades desses serviços e aprendizado, mas não é uma solução completa para uso em produção.&lt;/p&gt;

&lt;h2 id=&#34;pré-requisito&#34;&gt;Pré-requisito&lt;/h2&gt;

&lt;p&gt;Nesse procedimento, é necessário que o Docker esteja instalado e funcionando; também é necessário acesso à Internet.&lt;/p&gt;

&lt;p&gt;Originalmente, esse procedimento foi testado no ArchLinux atualizado até final de Agosto/2015.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://wiki.archlinux.org/index.php/Docker&#34;&gt;https://wiki.archlinux.org/index.php/Docker&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo docker version

&amp;gt; Client:
&amp;gt;  Version:      1.8.1
&amp;gt;  API version:  1.20
&amp;gt;  Go version:   go1.4.2
&amp;gt;  Git commit:   d12ea79
&amp;gt;  Built:        Sat Aug 15 17:29:10 UTC 2015
&amp;gt;  OS/Arch:      linux/amd64
&amp;gt;
&amp;gt; Server:
&amp;gt;  Version:      1.8.1
&amp;gt;  API version:  1.20
&amp;gt;  Go version:   go1.4.2
&amp;gt;  Git commit:   d12ea79
&amp;gt;  Built:        Sat Aug 15 17:29:10 UTC 2015
&amp;gt;  OS/Arch:      linux/amd64
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;configuração&#34;&gt;Configuração&lt;/h2&gt;

&lt;p&gt;Hadoop, ZooKeeper, HBase e Kafka.&lt;/p&gt;

&lt;h3 id=&#34;container&#34;&gt;Container&lt;/h3&gt;

&lt;p&gt;Começamos com a criação de um conainer do Docker com a imagem do CentOS6.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Importante: para os endereços com &lt;code&gt;grandesdados-hadoop&lt;/code&gt; funcionarem fora do container, direto na máquina host, é necessário colocar no &lt;code&gt;/etc/hosts&lt;/code&gt; da máquina host o endereço IP do container do Docker para esse nome.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Ao executar o comando &lt;code&gt;run&lt;/code&gt;, o Docker automaticamente fará o download da imagem e a shell será inicializada dentro de um novo container.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo docker run -i -t --name=grandesdados-hadoop --hostname=grandesdados-hadoop centos:6 /bin/bash

&amp;gt; Unable to find image &#39;centos:6&#39; locally
&amp;gt; 6: Pulling from library/centos
&amp;gt;
&amp;gt; f1b10cd84249: Pull complete
&amp;gt; fb9cc58bde0c: Pull complete
&amp;gt; a005304e4e74: Already exists
&amp;gt; library/centos:6: The image you are pulling has been verified. Important: image verification is a tech preview feature and should not be relied on to provide security.
&amp;gt;
&amp;gt; Digest: sha256:25d94c55b37cb7a33ad706d5f440e36376fec20f59e57d16fe02c64698b531c1
&amp;gt; Status: Downloaded newer image for centos:6
&amp;gt; [root@grandesdados-hadoop /]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Já dentro do container criamos um usuário e local que serão usados para a instalação e execução dos processos.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;adduser -m -d /hadoop hadoop
cd hadoop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A versão usada nesse procedimento é o Java 8, atual versão estável da Oracle.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -k -L -H &amp;quot;Cookie: oraclelicense=accept-securebackup-cookie&amp;quot; -O http://download.oracle.com/otn-pub/java/jdk/8u60-b27/jdk-8u60-linux-x64.rpm
rpm -i jdk-8u60-linux-x64.rpm

echo &#39;export JAVA_HOME=&amp;quot;/usr/java/jdk1.8.0_60&amp;quot;&#39; &amp;gt; /etc/profile.d/java.sh
source /etc/profile.d/java.sh

echo $JAVA_HOME

&amp;gt; /usr/java/jdk1.8.0_60

java -version

&amp;gt; java version &amp;quot;1.8.0_60&amp;quot;
&amp;gt; Java(TM) SE Runtime Environment (build 1.8.0_60-b27)
&amp;gt; Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para completar o ambiente de execução, instalamos os serviços e bibliotecas necessárias.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;yum install -y tar openssh-clients openssh-server rsync gzip zlib openssl fuse bzip2 snappy

&amp;gt; (...)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(configuração do SSH para acesso sem senha)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;service sshd start
chkconfig sshd on

su - hadoop

ssh-keygen -C hadoop -P &#39;&#39; -f ~/.ssh/id_rsa
cp ~/.ssh/{id_rsa.pub,authorized_keys}

ssh-keyscan grandesdados-hadoop &amp;gt;&amp;gt;  ~/.ssh/known_hosts
ssh-keyscan localhost &amp;gt;&amp;gt; ~/.ssh/known_hosts
ssh-keyscan 127.0.0.1 &amp;gt;&amp;gt; ~/.ssh/known_hosts
ssh-keyscan 0.0.0.0 &amp;gt;&amp;gt; ~/.ssh/known_hosts

ssh grandesdados-hadoop

&amp;gt; Warning: Permanently added the RSA host key for IP address &#39;172.17.0.12&#39; to the list of known hosts.
&amp;gt; (nova shell, sem login nem confirmação)

# (sair do shell do ssh)
exit
# (sair do shell do su)
exit

whoami

&amp;gt; root
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;hadoop&#34;&gt;Hadoop&lt;/h3&gt;

&lt;p&gt;Procedimento para configuração local do Hadoop em modo pseudo-distribuído com uma JVM por serviço.&lt;/p&gt;

&lt;p&gt;Esse procedimento é baseado na &lt;a href=&#34;http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/SingleCluster.html&#34;&gt;documentação do Hadoop&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Serviços:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;HDFS: NameNode, SecondaryNameNode, DataNode&lt;/li&gt;
&lt;li&gt;YARN: ResouceManager, NodeManager&lt;/li&gt;
&lt;li&gt;MR: HistoryServer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instalação&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;O pacote usado nesse procedimento é o Hadoop 2.7.1 para CentOS6 descrito outro &lt;a href=&#34;http://cirocavani.github.io/post/compilacao-do-hadoop-para-centos6-rhel6-usando-docker/&#34;&gt;artigo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Primeiramente, colocamos o pacote dentro do container.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# (shell fora do container)
sudo docker cp hadoop-2.7.1.tar.gz grandesdados-hadoop:/hadoop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;De volta ao container como usuário root.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;tar zxf hadoop-2.7.1.tar.gz -C /opt
chown hadoop:hadoop -R /opt/hadoop-2.7.1

echo &#39;export PATH=$PATH:/opt/hadoop-2.7.1/bin:/opt/hadoop-2.7.1/sbin&#39; &amp;gt; /etc/profile.d/hadoop.sh
source /etc/profile.d/hadoop.sh

hadoop version

&amp;gt; Hadoop 2.7.1
&amp;gt; Subversion Unknown -r Unknown
&amp;gt; Compiled by hadoop on 2015-09-01T00:30Z
&amp;gt; Compiled with protoc 2.5.0
&amp;gt; From source with checksum fc0a1a23fc1868e4d5ee7fa2b28a58a
&amp;gt; This command was run using /opt/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar

mkdir -p /data/hadoop
chown hadoop:hadoop /data/hadoop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Configuração&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(para a configuração, deve ser usado o usuário hadoop: &lt;code&gt;su - hadoop&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;Editar &lt;code&gt;/opt/hadoop-2.7.1/etc/hadoop/core-site.xml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;/data/hadoop&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hdfs://grandesdados-hadoop&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Editar &lt;code&gt;/opt/hadoop-2.7.1/etc/hadoop/hdfs-site.xml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.blocksize&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;8M&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Editar &lt;code&gt;/opt/hadoop-2.7.1/etc/hadoop/yarn-site.xml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Editar &lt;code&gt;/opt/hadoop-2.7.1/etc/hadoop/mapred-site.xml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;mapreduce.jobtracker.staging.root.dir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;/user&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Setup Inicial (antes da primeira inicialização).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;hdfs namenode -format

&amp;gt; 15/09/16 02:12:03 INFO namenode.NameNode: STARTUP_MSG:
&amp;gt; /************************************************************
&amp;gt; STARTUP_MSG: Starting NameNode
&amp;gt; STARTUP_MSG:   host = grandesdados-hadoop/172.17.0.12
&amp;gt; STARTUP_MSG:   args = [-format]
&amp;gt; STARTUP_MSG:   version = 2.7.1
&amp;gt; (...)
&amp;gt; INFO namenode.NameNode: createNameNode [-format]
&amp;gt; Formatting using clusterid: CID-5daa32a0-3ab6-405e-bfd2-05c0a6e1e7e6
&amp;gt; (...)
&amp;gt; INFO common.Storage: Storage directory /data/hadoop/dfs/name has been successfully formatted.
&amp;gt; (...)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;HDFS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(como usuário hadoop &lt;code&gt;su - hadoop&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;start-dfs.sh

&amp;gt; Starting namenodes on [grandesdados-hadoop]
&amp;gt; grandesdados-hadoop: starting namenode, logging to /opt/hadoop-2.7.1/logs/hadoop-hadoop-namenode-grandesdados-hadoop.out
&amp;gt; localhost: starting datanode, logging to /opt/hadoop-2.7.1/logs/hadoop-hadoop-datanode-grandesdados-hadoop.out
&amp;gt; Starting secondary namenodes [0.0.0.0]
&amp;gt; 0.0.0.0: starting secondarynamenode, logging to /opt/hadoop-2.7.1/logs/hadoop-hadoop-secondarynamenode-grandesdados-hadoop.out

# criação do diretório do usuário hadoop
hdfs dfs -mkdir -p /user/hadoop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interface Web do Name Node:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://grandesdados-hadoop:50070/&#34;&gt;http://grandesdados-hadoop:50070/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Interface Web do Data Node (vazia):&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://grandesdados-hadoop:50075/&#34;&gt;http://grandesdados-hadoop:50075/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Interface Web do Secondary Name Node:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://grandesdados-hadoop:50090/&#34;&gt;http://grandesdados-hadoop:50090/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Para parar o serviço:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;stop-dfs.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;YARN&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(como usuário hadoop &lt;code&gt;su - hadoop&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;start-yarn.sh

&amp;gt; starting yarn daemons
&amp;gt; starting resourcemanager, logging to /opt/hadoop-2.7.1/logs/yarn-hadoop-resourcemanager-grandesdados-hadoop.out
&amp;gt; localhost: starting nodemanager, logging to /opt/hadoop-2.7.1/logs/yarn-hadoop-nodemanager-grandesdados-hadoop.out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interface Web do Resource Manager:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://grandesdados-hadoop:8088/&#34;&gt;http://grandesdados-hadoop:8088/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Interface Web do Node Manager:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://grandesdados-hadoop:8042/&#34;&gt;http://grandesdados-hadoop:8042/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Para parar o serviço:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;stop-yarn.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;History Server&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(como usuário hadoop &lt;code&gt;su - hadoop&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mr-jobhistory-daemon.sh start historyserver

&amp;gt; starting historyserver, logging to /opt/hadoop-2.7.1/logs/mapred-hadoop-historyserver-grandesdados-hadoop.out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interface Web do History Server:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://grandesdados-hadoop:19888/&#34;&gt;http://grandesdados-hadoop:19888/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Para parar o serviço:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mr-jobhistory-daemon.sh stop historyserver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Teste&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(para os testes, deve ser usado o usuário hadoop: &lt;code&gt;su - hadoop&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;Processos:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ps x

&amp;gt;   PID TTY      STAT   TIME COMMAND
&amp;gt;  5162 ?        S      0:00 -bash
&amp;gt;  5327 ?        Sl     0:08 /usr/java/jdk1.8.0_60/bin/java -Dproc_namenode (...)
&amp;gt;  5423 ?        Sl     0:07 /usr/java/jdk1.8.0_60/bin/java -Dproc_datanode (...)
&amp;gt;  5612 ?        Sl     0:06 /usr/java/jdk1.8.0_60/bin/java -Dproc_secondarynamenode (...)
&amp;gt;  5772 ?        Sl     0:08 /usr/java/jdk1.8.0_60/bin/java -Dproc_resourcemanager (...)
&amp;gt;  5870 ?        Sl     0:07 /usr/java/jdk1.8.0_60/bin/java -Dproc_nodemanager (...)
&amp;gt;  6189 ?        Sl     0:08 /usr/java/jdk1.8.0_60/bin/java -Dproc_historyserver (...)
&amp;gt;  6273 ?        R+     0:00 ps x
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Exemplos de MapReduce:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;yarn jar /opt/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar

&amp;gt; An example program must be given as the first argument.
&amp;gt; Valid program names are:
&amp;gt;   aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.
&amp;gt;   aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.
&amp;gt;   bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.
&amp;gt;   dbcount: An example job that count the pageview counts from a database.
&amp;gt;   distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.
&amp;gt;   grep: A map/reduce program that counts the matches of a regex in the input.
&amp;gt;   join: A job that effects a join over sorted, equally partitioned datasets
&amp;gt;   multifilewc: A job that counts words from several files.
&amp;gt;   pentomino: A map/reduce tile laying program to find solutions to pentomino problems.
&amp;gt;   pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.
&amp;gt;   randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.
&amp;gt;   randomwriter: A map/reduce program that writes 10GB of random data per node.
&amp;gt;   secondarysort: An example defining a secondary sort to the reduce.
&amp;gt;   sort: A map/reduce program that sorts the data written by the random writer.
&amp;gt;   sudoku: A sudoku solver.
&amp;gt;   teragen: Generate data for the terasort
&amp;gt;   terasort: Run the terasort
&amp;gt;   teravalidate: Checking results of terasort
&amp;gt;   wordcount: A map/reduce program that counts the words in the input files.
&amp;gt;   wordmean: A map/reduce program that counts the average length of the words in the input files.
&amp;gt;   wordmedian: A map/reduce program that counts the median length of the words in the input files.
&amp;gt;   wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Rodando o Cálculo do Pi com MapReduce:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;yarn jar /opt/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar pi 16 100000

&amp;gt; Number of Maps  = 16
&amp;gt; Samples per Map = 100000
&amp;gt; (...)
&amp;gt; INFO impl.YarnClientImpl: Submitted application application_1442439610364_0001
&amp;gt; INFO mapreduce.Job: The url to track the job: http://grandesdados-hadoop:8088/proxy/application_1442439610364_0001/
&amp;gt; INFO mapreduce.Job: Running job: job_1442439610364_0001
&amp;gt; (...)
&amp;gt; Job Finished in 48.333 seconds
&amp;gt; Estimated value of Pi is 3.14157500000000000000
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;zookeeper&#34;&gt;ZooKeeper&lt;/h3&gt;

&lt;p&gt;Esse procedimento é baseado na &lt;a href=&#34;https://zookeeper.apache.org/doc/r3.4.6/zookeeperStarted.html&#34;&gt;documentação do ZooKeeper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Dentro do container como usuário root:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -L -O http://archive.apache.org/dist/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz
tar zxf zookeeper-3.4.6.tar.gz -C /opt
chown hadoop:hadoop -R /opt/zookeeper-3.4.6

echo &#39;export PATH=$PATH:/opt/zookeeper-3.4.6/bin&#39; &amp;gt; /etc/profile.d/zookeeper.sh
source /etc/profile.d/zookeeper.sh

mkdir -p /data/zookeeper
chown hadoop:hadoop /data/zookeeper
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Editar &lt;code&gt;/opt/zookeeper-3.4.6/conf/zoo.cfg&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;tickTime=6000
dataDir=/data/zookeeper
clientPort=2181
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inicializando o serviço:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;su - hadoop
zkServer.sh start

&amp;gt; JMX enabled by default
&amp;gt; Using config: /opt/zookeeper-3.4.6/bin/../conf/zoo.cfg
&amp;gt; Starting zookeeper ... STARTED
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para parar o serviço:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;zkServer.sh stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Teste&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(para os testes, deve ser usado o usuário hadoop: &lt;code&gt;su - hadoop&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;Processo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ps x | grep zoo

&amp;gt; 8246 ?        Sl     0:00 /usr/java/jdk1.8.0_60/bin/java (...) org.apache.zookeeper.server.quorum.QuorumPeerMain (...)
&amp;gt; 8291 ?        S+     0:00 grep zoo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Telnet:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;echo &#39;ruok&#39; |  curl telnet://grandesdados-hadoop:2181

&amp;gt; imok
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cliente:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;zkCli.sh -server grandesdados-hadoop:2181

&amp;gt; Connecting to grandesdados-hadoop:2181
&amp;gt; ...
&amp;gt; [zk: grandesdados-hadoop:2181(CONNECTED) 0] ls /
&amp;gt; [zookeeper]
&amp;gt; [zk: grandesdados-hadoop:2181(CONNECTED) 1] help
&amp;gt; ZooKeeper -server host:port cmd args
&amp;gt; 	stat path [watch]
&amp;gt; 	set path data [version]
&amp;gt; 	ls path [watch]
&amp;gt; 	delquota [-n|-b] path
&amp;gt; 	ls2 path [watch]
&amp;gt; 	setAcl path acl
&amp;gt; 	setquota -n|-b val path
&amp;gt; 	history
&amp;gt; 	redo cmdno
&amp;gt; 	printwatches on|off
&amp;gt; 	delete path [version]
&amp;gt; 	sync path
&amp;gt; 	listquota path
&amp;gt; 	rmr path
&amp;gt; 	get path [watch]
&amp;gt; 	create [-s] [-e] path data acl
&amp;gt; 	addauth scheme auth
&amp;gt; 	quit
&amp;gt; 	getAcl path
&amp;gt; 	close
&amp;gt; 	connect host:port
&amp;gt; [zk: grandesdados-hadoop:2181(CONNECTED) 3] quit
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;hbase&#34;&gt;HBase&lt;/h3&gt;

&lt;p&gt;Esse procedimento é baseado na &lt;a href=&#34;http://hbase.apache.org/book.html#quickstart&#34;&gt;documentação do HBase&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Dentro do container como usuário root:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -L -O http://archive.apache.org/dist/hbase/1.1.2/hbase-1.1.2-bin.tar.gz
tar zxf hbase-1.1.2-bin.tar.gz -C /opt
chown hadoop:hadoop -R /opt/hbase-1.1.2

echo &#39;export PATH=$PATH:/opt/hbase-1.1.2/bin&#39; &amp;gt; /etc/profile.d/hbase.sh
source /etc/profile.d/hbase.sh

mkdir -p /data/hbase/tmp
chown hadoop:hadoop -R /data/hbase
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Editar &lt;code&gt;/opt/hbase-1.1.2/conf/hbase-site.xml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.cluster.distributed&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hdfs:///hbase&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.tmp.dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/data/hbase/tmp&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.zookeeper.quorum&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;grandesdados-hadoop&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Editar &lt;code&gt;/opt/hbase-1.1.2/conf/hbase-env.sh&lt;/code&gt;:
&lt;br/&gt;(manter conteúdo original, só alterar os valores abaixo)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export HBASE_OPTS=&amp;quot;-XX:+UseConcMarkSweepGC -Djava.net.preferIPv4Stack=true&amp;quot;
export HBASE_MANAGES_ZK=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inicializando o serviço:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;su - hadoop
start-hbase.sh

&amp;gt; starting master, logging to /opt/hbase-1.1.2/bin/../logs/hbase-hadoop-master-grandesdados-hadoop.out
&amp;gt; Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0
&amp;gt; Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0
&amp;gt; starting regionserver, logging to /opt/hbase-1.1.2/bin/../logs/hbase-hadoop-1-regionserver-grandesdados-hadoop.out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interface Web do Master:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://grandesdados-hadoop:16010/&#34;&gt;http://grandesdados-hadoop:16010/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Interface Web do Region Server:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://grandesdados-hadoop:16301/&#34;&gt;http://grandesdados-hadoop:16301/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Para parar o serviço:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;stop-hbase.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Teste&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(para os testes, deve ser usado o usuário hadoop: &lt;code&gt;su - hadoop&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;Processo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ps x | grep hbase

&amp;gt; 8790 ?        S      0:00 bash /opt/hbase-1.1.2/bin/hbase-daemon.sh --config /opt/hbase-1.1.2/bin/../conf foreground_start master
&amp;gt; 8804 ?        Sl     0:14 /usr/java/jdk1.8.0_60/bin/java -Dproc_master (...)
&amp;gt; 8915 ?        S      0:00 bash /opt/hbase-1.1.2/bin/hbase-daemon.sh --config /opt/hbase-1.1.2/bin/../conf foreground_start regionserver -D hbase.regionserver.port=16201 -D hbase.regionserver.info.port=16301
&amp;gt; 8929 ?        Sl     0:14 /usr/java/jdk1.8.0_60/bin/java -Dproc_regionserver (...)
&amp;gt; 9329 ?        S+     0:00 grep hbase
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cliente:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;hbase shell

&amp;gt; HBase Shell; enter &#39;help&amp;lt;RETURN&amp;gt;&#39; for list of supported commands.
&amp;gt; Type &amp;quot;exit&amp;lt;RETURN&amp;gt;&amp;quot; to leave the HBase Shell
&amp;gt; Version 1.1.2, rcc2b70cf03e3378800661ec5cab11eb43fafe0fc, Wed Aug 26 20:11:27 PDT 2015
&amp;gt;
&amp;gt; hbase(main):001:0&amp;gt; status
&amp;gt; 1 servers, 0 dead, 2.0000 average load
&amp;gt;
&amp;gt; hbase(main):002:0&amp;gt; help
&amp;gt; HBase Shell, version 1.1.2, rcc2b70cf03e3378800661ec5cab11eb43fafe0fc, Wed Aug 26 20:11:27 PDT 2015
&amp;gt; Type &#39;help &amp;quot;COMMAND&amp;quot;&#39;, (e.g. &#39;help &amp;quot;get&amp;quot;&#39; -- the quotes are necessary) for help on a specific command.
&amp;gt; Commands are grouped. Type &#39;help &amp;quot;COMMAND_GROUP&amp;quot;&#39;, (e.g. &#39;help &amp;quot;general&amp;quot;&#39;) for help on a command group.
&amp;gt; (...)
&amp;gt; hbase(main):004:0&amp;gt; exit
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;kafka&#34;&gt;Kafka&lt;/h3&gt;

&lt;p&gt;Esse procedimento é baseado na &lt;a href=&#34;http://kafka.apache.org/documentation.html#quickstart&#34;&gt;documentação do Kafka&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Dentro do container como usuário root:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -L -O http://archive.apache.org/dist/kafka/0.8.2.1/kafka_2.10-0.8.2.1.tgz
tar zxf kafka_2.10-0.8.2.1.tgz -C /opt
chown hadoop:hadoop -R /opt/kafka_2.10-0.8.2.1

echo &#39;export PATH=$PATH:/opt/kafka_2.10-0.8.2.1/bin&#39; &amp;gt; /etc/profile.d/kafka.sh
source /etc/profile.d/kafka.sh

mkdir -p /data/kafka
chown hadoop:hadoop /data/kafka
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Editar &lt;code&gt;/opt/kafka_2.10-0.8.2.1/config/server.properties&lt;/code&gt;:
&lt;br/&gt;(manter conteúdo original, só alterar os valores abaixo)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;log.dirs=/data/kafka
zookeeper.connect=grandesdados-hadoop:2181
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inicializando o serviço:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;su - hadoop
kafka-server-start.sh /opt/kafka_2.10-0.8.2.1/config/server.properties &amp;amp;&amp;gt; kafka.out &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para parar o serviço:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kafka-server-stop.sh /opt/kafka_2.10-0.8.2.1/config/server.properties
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Teste&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(para os testes, deve ser usado o usuário hadoop: &lt;code&gt;su - hadoop&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;Processo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ps x | grep kafka

&amp;gt; 9818 ?        Sl     0:03 /usr/java/jdk1.8.0_60/bin/java (...) kafka.Kafka /opt/kafka_2.10-0.8.2.1/config/server.properties
&amp;gt; 9928 ?        S+     0:00 grep kafka
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Enviando e recebendo mensagens:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kafka-topics.sh \
--create \
--zookeeper grandesdados-hadoop:2181 \
--replication-factor 1 \
--partitions 1 \
--topic test

&amp;gt; Created topic &amp;quot;test&amp;quot;.

echo &#39;Primeira mensagem de teste&#39; | kafka-console-producer.sh --broker-list grandesdados-hadoop:9092 --topic test

&amp;gt; (...)

kafka-console-consumer.sh --zookeeper grandesdados-hadoop:2181 --topic test --from-beginning

&amp;gt; Primeira mensagem de teste
&amp;gt; ^CConsumed 1 messages
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusão&#34;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;A revolução em BigData é um fenômeno da tecnologia desenvolvida ao longo dos últimos anos focada na manipulação de um grande volume de dados em máquinas de baixo custo. Essa é a tecnologia que torna possível combinar uma solução de dados escalável com processos para geração de resultados relevantes, tanto no desenvolvimento de produtos quanto na evolução do conhecimento. O importante é entender como essa tecnologia pode ser usada para agregar valor ao negócio e permitir imaginar soluções inovadoras.&lt;/p&gt;

&lt;p&gt;Esse artigo documenta o passo-a-passo de uma configuração local do Hadoop, ZooKeeper, HBase e Kafka. Esses são os serviços essenciais em uma plataforma de BigData que, juntamente com o Spark, possibilitam o desenvolvimento de soluções tanto para processamento batch quanto para tempo real.&lt;/p&gt;

&lt;p&gt;Em artigos futuros, entrarei usando essa solução para desenvolver e testar algumas aplicações de BigData usando Spark.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Compilação do Spark 1.5 (com bugfix)</title>
      <link>http://cirocavani.github.io/post/compilacao-do-spark-15-com-bugfix/</link>
      <pubDate>Fri, 11 Sep 2015 08:10:00 -0300</pubDate>
      
      <guid>http://cirocavani.github.io/post/compilacao-do-spark-15-com-bugfix/</guid>
      <description>

&lt;p&gt;Aproveitando que foi feito o lançamento da versão 1.5.0 do Spark, esse tutorial é sobre a construção do pacote do Spark usando o branch atualizado. O branch foi criado para fazer a estabilização do código que deu origem ao primeiro release. Esse branch continua recebendo atualizações importantes que farão parte de releases bugfix no futuro. Com esse procedimento, é possível gerar o pacote com essas últimas atualizações (e até customizar com alterações próprias) antecipando correções que podem ajudar em produção. Importante entender que ao usar uma versão que não passou pelo release implica em riscos que devem ser mitigados com muitos testes.&lt;/p&gt;

&lt;p&gt;Mais informações sobre a construção do Spark podem ser obtidas na documentação &lt;a href=&#34;http://spark.apache.org/docs/latest/building-spark.html&#34;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Mais informações sobre a última versão Spark 1.5.0 no &lt;a href=&#34;http://spark.apache.org/releases/spark-release-1-5-0.html&#34;&gt;Release Notes&lt;/a&gt; e no blog da Databricks &lt;a href=&#34;https://databricks.com/blog/2015/09/09/announcing-spark-1-5.html&#34;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;pré-requisito&#34;&gt;Pré-requisito&lt;/h2&gt;

&lt;p&gt;O procedimento consiste em: provisionar o ambiente; fazer uma cópia do branch estável da última versão, e; gerar o pacote binário e os artefatos do Maven.&lt;/p&gt;

&lt;p&gt;As ferramentas necessárias para construção são git, Java 7 e Maven 3.3.&lt;/p&gt;

&lt;p&gt;Todo o procedimento é executado na linha de comando do terminal.&lt;/p&gt;

&lt;p&gt;(é assumido que o git já está instalado)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Java&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A versão usada nesse procedimento é o Java 7 para o qual a Oracle já terminou o ciclo de desenvolvimento das releases públicas (gratuitas). Contudo, essa é a versão que tem melhor suporte nas ferramentas que estaremos usando com Spark.&lt;/p&gt;

&lt;p&gt;(também tem suporte para o Java 8, mas o interesse é usar esse pacote no Hadoop 2.7 que ainda não suporta oficialmente essa versão)&lt;/p&gt;

&lt;p&gt;Segue o procedimento para Linux e MacOSX.&lt;/p&gt;

&lt;p&gt;(Linux)&lt;/p&gt;

&lt;p&gt;No Linux, para o Java, é usado o JDK da Oracle.&lt;/p&gt;

&lt;p&gt;(nesse procedimento, foi usado o ArchLinux atualizado até essa primeira semana de Setembro)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;wget --no-check-certificate --no-cookies --header &amp;quot;Cookie: oraclelicense=accept-securebackup-cookie&amp;quot; http://download.oracle.com/otn-pub/java/jdk/7u80-b15/jdk-7u80-linux-x64.tar.gz

tar zxf jdk-7u80-linux-x64.tar.gz

export JAVA_HOME=`pwd`/jdk1.7.0_80
export PATH=$JAVA_HOME/bin:$PATH

java -version

&amp;gt; java version &amp;quot;1.7.0_80&amp;quot;
&amp;gt; Java(TM) SE Runtime Environment (build 1.7.0_80-b15)
&amp;gt; Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(OSX)&lt;/p&gt;

&lt;p&gt;No MacOSX, é necessário baixar o pacote no site da Oracle e fazer a instalação.&lt;/p&gt;

&lt;p&gt;Download do Java 7 &lt;a href=&#34;http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html#jdk-7u80-oth-JPR&#34;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;No terminal, a versão específica do Java pode ser configurada ajustando a variável de ambiente:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export JAVA_HOME=&amp;quot;$(/usr/libexec/java_home -v 1.7)&amp;quot;

java -version

&amp;gt; java version &amp;quot;1.7.0_80&amp;quot;
&amp;gt; Java(TM) SE Runtime Environment (build 1.7.0_80-b15)
&amp;gt; Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Maven&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A construção do Spark depende da versão 3.3 do Maven.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;wget http://archive.apache.org/dist/maven/maven-3/3.3.3/binaries/apache-maven-3.3.3-bin.tar.gz

tar zxf apache-maven-3.3.3-bin.tar.gz

export PATH=`pwd`/apache-maven-3.3.3/bin

mvn -version

&amp;gt; Apache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T08:57:37-03:00)
&amp;gt; Maven home: /home/cavani/Software/apache-maven-3.3.3
&amp;gt; Java version: 1.7.0_80, vendor: Oracle Corporation
&amp;gt; Java home: /home/cavani/Software/jdk1.7.0_80/jre
&amp;gt; Default locale: en_US, platform encoding: UTF-8
&amp;gt; OS name: &amp;quot;linux&amp;quot;, version: &amp;quot;4.0.4-2-arch&amp;quot;, arch: &amp;quot;amd64&amp;quot;, family: &amp;quot;unix&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;compilação&#34;&gt;Compilação&lt;/h2&gt;

&lt;p&gt;Primeiramente é criado um clone local do repositório do Spark no qual é desenvolvido a versão 1.5 (estável).&lt;/p&gt;

&lt;p&gt;(use &lt;code&gt;--depth 1&lt;/code&gt; para baixar apenas os arquivos finais, sem o histórico de mudanças, diminui o download)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/apache/spark.git --branch branch-1.5 spark-1.5

&amp;gt; Cloning into &#39;spark-1.5&#39;...
&amp;gt; remote: Counting objects: 256928, done.
&amp;gt; remote: Total 256928 (delta 0), reused 0 (delta 0), pack-reused 256928
&amp;gt; Receiving objects: 100% (256928/256928), 121.38 MiB | 1.23 MiB/s, done.
&amp;gt; Resolving deltas: 100% (108225/108225), done.
&amp;gt; Checking connectivity... done.

cd spark-1.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A partir desse branch serão criados todos os releases 1.5.x.&lt;/p&gt;

&lt;p&gt;Já foi feito o release da tag v1.5.0 e está aberto o desenvolvimento da versão 1.5.1 (ou seja, a versão corrente no branch é a 1.5.1-SNAPSHOT).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git log --oneline -30

&amp;gt; 89d351b Revert &amp;quot;[SPARK-6350] [MESOS] Fine-grained mode scheduler respects mesosExecutor.cores&amp;quot;
&amp;gt; (...)
&amp;gt; 2b270a1 Preparing development version 1.5.1-SNAPSHOT
&amp;gt; 908e37b Preparing Spark release v1.5.0-rc3
&amp;gt; 1c752b8 [SPARK-10341] [SQL] fix memory starving in unsafe SMJ
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;O versionamento será com base na versão do último release e a identificação dos bugfix será feita no nome do pacote, preservado a substituição transparente da versão oficial pela atualizada.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mvn help:evaluate -Dexpression=project.version | grep -v INFO | grep -v WARNING | grep -v Download

&amp;gt; 1.5.1-SNAPSHOT

mvn versions:set -DnewVersion=1.5.0 -DgenerateBackupPoms=false

&amp;gt; (...)
&amp;gt; [INFO] Reactor Summary:
&amp;gt; [INFO]
&amp;gt; [INFO] Spark Project Parent POM ........................... SUCCESS [  4.559 s]
&amp;gt; (...)
&amp;gt; [INFO] BUILD SUCCESS
&amp;gt; (...)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Por fim, a construção do pacote.&lt;/p&gt;

&lt;p&gt;Nesse caso estaremos construindo um pacote com suporte ao YARN no Hadoop 2.7.1, suporte Hive com JDBC.&lt;/p&gt;

&lt;p&gt;Estamos colocando no nome do pacote o número do último commit.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export MAVEN_OPTS=&amp;quot;-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m&amp;quot;

./make-distribution.sh --name 89d351b --tgz --skip-java-test -Phadoop-2.6 -Pyarn -Phive -Phive-thriftserver -Dhadoop.version=2.7.1

&amp;gt; (...)
&amp;gt; [INFO] Reactor Summary:
&amp;gt; [INFO]
&amp;gt; [INFO] Spark Project Parent POM ........................... SUCCESS [  3.841 s]
&amp;gt; [INFO] Spark Project Launcher ............................. SUCCESS [ 12.819 s]
&amp;gt; [INFO] Spark Project Networking ........................... SUCCESS [ 10.980 s]
&amp;gt; [INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  6.876 s]
&amp;gt; [INFO] Spark Project Unsafe ............................... SUCCESS [ 15.828 s]
&amp;gt; [INFO] Spark Project Core ................................. SUCCESS [03:19 min]
&amp;gt; [INFO] Spark Project Bagel ................................ SUCCESS [  7.048 s]
&amp;gt; [INFO] Spark Project GraphX ............................... SUCCESS [ 18.493 s]
&amp;gt; [INFO] Spark Project Streaming ............................ SUCCESS [ 41.120 s]
&amp;gt; [INFO] Spark Project Catalyst ............................. SUCCESS [01:01 min]
&amp;gt; [INFO] Spark Project SQL .................................. SUCCESS [01:22 min]
&amp;gt; [INFO] Spark Project ML Library ........................... SUCCESS [01:13 min]
&amp;gt; [INFO] Spark Project Tools ................................ SUCCESS [  2.460 s]
&amp;gt; [INFO] Spark Project Hive ................................. SUCCESS [ 58.477 s]
&amp;gt; [INFO] Spark Project REPL ................................. SUCCESS [ 11.646 s]
&amp;gt; [INFO] Spark Project YARN ................................. SUCCESS [ 14.443 s]
&amp;gt; [INFO] Spark Project Hive Thrift Server ................... SUCCESS [ 11.609 s]
&amp;gt; [INFO] Spark Project Assembly ............................. SUCCESS [02:02 min]
&amp;gt; [INFO] Spark Project External Twitter ..................... SUCCESS [  8.653 s]
&amp;gt; [INFO] Spark Project External Flume Sink .................. SUCCESS [  5.997 s]
&amp;gt; [INFO] Spark Project External Flume ....................... SUCCESS [ 12.408 s]
&amp;gt; [INFO] Spark Project External Flume Assembly .............. SUCCESS [  3.959 s]
&amp;gt; [INFO] Spark Project External MQTT ........................ SUCCESS [ 22.884 s]
&amp;gt; [INFO] Spark Project External MQTT Assembly ............... SUCCESS [  8.830 s]
&amp;gt; [INFO] Spark Project External ZeroMQ ...................... SUCCESS [  8.407 s]
&amp;gt; [INFO] Spark Project External Kafka ....................... SUCCESS [ 14.933 s]
&amp;gt; [INFO] Spark Project Examples ............................. SUCCESS [01:52 min]
&amp;gt; [INFO] Spark Project External Kafka Assembly .............. SUCCESS [  7.171 s]
&amp;gt; [INFO] Spark Project YARN Shuffle Service ................. SUCCESS [  7.010 s]
&amp;gt; [INFO] ------------------------------------------------------------------------
&amp;gt; [INFO] BUILD SUCCESS
&amp;gt; [INFO] ------------------------------------------------------------------------
&amp;gt; [INFO] Total time: 16:08 min
&amp;gt; [INFO] Finished at: 2015-09-11T06:44:55-03:00
&amp;gt; [INFO] Final Memory: 417M/1553M
&amp;gt; [INFO] ------------------------------------------------------------------------
&amp;gt; (...)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Resultado:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;spark-1.5.0-bin-89d351b.tgz&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;(Artefatos do Maven)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;rm -rf ~/.m2/repository/org/apache/spark

mvn install -Phadoop-2.6 -Pyarn -Phive -Phive-thriftserver -Dhadoop.version=2.7.1 -DskipTests

&amp;gt; (...)
&amp;gt; [INFO] Reactor Summary:
&amp;gt; [INFO]
&amp;gt; [INFO] Spark Project Parent POM ........................... SUCCESS [  4.339 s]
&amp;gt; [INFO] Spark Project Launcher ............................. SUCCESS [ 14.078 s]
&amp;gt; [INFO] Spark Project Networking ........................... SUCCESS [  8.555 s]
&amp;gt; [INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  3.540 s]
&amp;gt; [INFO] Spark Project Unsafe ............................... SUCCESS [  3.395 s]
&amp;gt; [INFO] Spark Project Core ................................. SUCCESS [01:22 min]
&amp;gt; [INFO] Spark Project Bagel ................................ SUCCESS [  7.293 s]
&amp;gt; [INFO] Spark Project GraphX ............................... SUCCESS [ 15.367 s]
&amp;gt; [INFO] Spark Project Streaming ............................ SUCCESS [ 26.005 s]
&amp;gt; [INFO] Spark Project Catalyst ............................. SUCCESS [ 49.232 s]
&amp;gt; [INFO] Spark Project SQL .................................. SUCCESS [ 48.866 s]
&amp;gt; [INFO] Spark Project ML Library ........................... SUCCESS [01:01 min]
&amp;gt; [INFO] Spark Project Tools ................................ SUCCESS [  8.979 s]
&amp;gt; [INFO] Spark Project Hive ................................. SUCCESS [ 29.601 s]
&amp;gt; [INFO] Spark Project REPL ................................. SUCCESS [ 19.661 s]
&amp;gt; [INFO] Spark Project YARN ................................. SUCCESS [ 16.976 s]
&amp;gt; [INFO] Spark Project Hive Thrift Server ................... SUCCESS [ 13.583 s]
&amp;gt; [INFO] Spark Project Assembly ............................. SUCCESS [02:01 min]
&amp;gt; [INFO] Spark Project External Twitter ..................... SUCCESS [  9.734 s]
&amp;gt; [INFO] Spark Project External Flume Sink .................. SUCCESS [ 10.291 s]
&amp;gt; [INFO] Spark Project External Flume ....................... SUCCESS [ 12.282 s]
&amp;gt; [INFO] Spark Project External Flume Assembly .............. SUCCESS [  4.252 s]
&amp;gt; [INFO] Spark Project External MQTT ........................ SUCCESS [ 21.910 s]
&amp;gt; [INFO] Spark Project External MQTT Assembly ............... SUCCESS [  8.383 s]
&amp;gt; [INFO] Spark Project External ZeroMQ ...................... SUCCESS [  7.677 s]
&amp;gt; [INFO] Spark Project External Kafka ....................... SUCCESS [ 13.317 s]
&amp;gt; [INFO] Spark Project Examples ............................. SUCCESS [01:45 min]
&amp;gt; [INFO] Spark Project External Kafka Assembly .............. SUCCESS [  6.813 s]
&amp;gt; [INFO] Spark Project YARN Shuffle Service ................. SUCCESS [  7.544 s]
&amp;gt; [INFO] ------------------------------------------------------------------------
&amp;gt; [INFO] BUILD SUCCESS
&amp;gt; [INFO] ------------------------------------------------------------------------
&amp;gt; [INFO] Total time: 12:24 min
&amp;gt; [INFO] Finished at: 2015-09-11T07:52:35-03:00
&amp;gt; [INFO] Final Memory: 102M/1349M
&amp;gt; [INFO] ------------------------------------------------------------------------

cd ~/.m2/repository
tar cf spark-1.5.0-m2-89d351b.tar org/apache/spark

cd -
mv ~/.m2/repository/spark-1.5.0-m2-89d351b.tar .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Resultado:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;spark-1.5.0-m2-89d351b.tar&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusão&#34;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;O Spark é um framework que vem evoluindo rapidamente, com contribuições das mais diversas origem. Praticamente todos os grandes de BigData estão contribuindo com o Spark. Muitas vezes, surgem novas funcionalidades que podem agregar muito valor nas suas aplicações. Outras vezes, são bugs corrigidos que contribuem para a estabilidade de uma aplicação que já existe. Também tem o &amp;lsquo;prazer&amp;rsquo; de ser um &amp;lsquo;early adopter&amp;rsquo;. Seja qual for o motivo, esse procedimento mostra que o trabalho para ter um pacote do Spark é bem fácil e, por experiência, esse é um fator bastante relevante para ganhar tempo e gerar máximo valor.&lt;/p&gt;

&lt;p&gt;Nos próximos artigos, vou falar mais de como usar o Spark para desenvolver um aplicação que roda no Hadoop.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Compilação do Hadoop para CentOS6 / RHEL6 usando Docker</title>
      <link>http://cirocavani.github.io/post/compilacao-do-hadoop-para-centos6-rhel6-usando-docker/</link>
      <pubDate>Mon, 31 Aug 2015 22:45:23 -0300</pubDate>
      
      <guid>http://cirocavani.github.io/post/compilacao-do-hadoop-para-centos6-rhel6-usando-docker/</guid>
      <description>

&lt;p&gt;Esse tutorial é sobre a construção do pacote do Hadoop 2.7.1 para o CentOS6 / RHEL6 usando Docker. Esse procedimento é necessário para gerar as bibliotecas nativas compatíveis. O principal objetivo que motivou esse trabalho foi configurar o FairScheduler do YARN usando CGroups rodando no Red Hat Enterprise Linux 6 (RHEL6). O pacote Hadoop distribuído pela Apache tem executável binário que não é compatível com a Glibc que faz parte do CentOS6/RHEL6.&lt;/p&gt;

&lt;p&gt;O RHEL6 é o sistema operacional homologado para as máquinas do cluster que usamos na Globo.com e foi necessário criar uma distribuição própria do Hadoop para que pudéssemos fazer uso do &lt;a href=&#34;http://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/FairScheduler.html&#34;&gt;FairScheduler&lt;/a&gt; juntamente com o &lt;a href=&#34;http://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/NodeManagerCgroups.html&#34;&gt;CGroups&lt;/a&gt; para limitar o uso de processamento entre as aplicações rodando nos mesmos NodeManagers.&lt;/p&gt;

&lt;p&gt;Esse trabalho de configuração do Hadoop para uso compartilhado será assunto de outro artigo.&lt;/p&gt;

&lt;p&gt;Nesse artigo, o foco é um passo a passo de como usar o Docker para gerar um pacote do Hadoop adaptado para o Red Hat Enterprise Linux 6 (RHEL6) usando CentOS6.&lt;/p&gt;

&lt;h2 id=&#34;pré-requisito&#34;&gt;Pré-requisito&lt;/h2&gt;

&lt;p&gt;Nesse procedimento, é necessário que o Docker esteja instalado e funcionando; também é necessário acesso à Internet.&lt;/p&gt;

&lt;p&gt;Originalmente, esse procedimento foi testado no ArchLinux atualizado até final de Agosto/2015.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://wiki.archlinux.org/index.php/Docker&#34;&gt;https://wiki.archlinux.org/index.php/Docker&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo docker version

&amp;gt; Client:
&amp;gt;  Version:      1.8.1
&amp;gt;  API version:  1.20
&amp;gt;  Go version:   go1.4.2
&amp;gt;  Git commit:   d12ea79
&amp;gt;  Built:        Sat Aug 15 17:29:10 UTC 2015
&amp;gt;  OS/Arch:      linux/amd64
&amp;gt;
&amp;gt; Server:
&amp;gt;  Version:      1.8.1
&amp;gt;  API version:  1.20
&amp;gt;  Go version:   go1.4.2
&amp;gt;  Git commit:   d12ea79
&amp;gt;  Built:        Sat Aug 15 17:29:10 UTC 2015
&amp;gt;  OS/Arch:      linux/amd64
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;compilação&#34;&gt;Compilação&lt;/h2&gt;

&lt;p&gt;Documento com instruções de build do Hadoop &lt;a href=&#34;https://github.com/apache/hadoop/blob/release-2.7.1/BUILDING.txt&#34;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;O resultado desse procedimento é um pacote do Hadoop com os executáveis e bibliotecas nativas compilados para o CentOS6 que rodam no RHEL6.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;/hadoop/hadoop-2.7.1-src/hadoop-dist/target/hadoop-2.7.1.tar.gz&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;Começamos com a criação de um conainer do Docker com a imagem do CentOS6.&lt;/p&gt;

&lt;p&gt;Ao executar o comando &lt;code&gt;run&lt;/code&gt;, o Docker automaticamente fará o download da imagem e a shell será inicializada dentro de um novo container.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo docker run -i -t centos:6 /bin/bash

&amp;gt; Unable to find image &#39;centos:6&#39; locally
&amp;gt; 6: Pulling from library/centos
&amp;gt;
&amp;gt; f1b10cd84249: Pull complete
&amp;gt; fb9cc58bde0c: Pull complete
&amp;gt; a005304e4e74: Already exists
&amp;gt; library/centos:6: The image you are pulling has been verified. Important: image verification is a tech preview feature and should not be relied on to provide security.
&amp;gt;
&amp;gt; Digest: sha256:25d94c55b37cb7a33ad706d5f440e36376fec20f59e57d16fe02c64698b531c1
&amp;gt; Status: Downloaded newer image for centos:6
&amp;gt; [root@3cc2bc5e593b /]#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Já dentro do container criamos um usuário e local que serão usados na compilação e geração do pacote.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;adduser -m -d /hadoop hadoop
cd hadoop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para a compilação das bibliotecas nativas é necessária a instalação do compilador C e mais alguns pacotes de desenvolvimento (cabeçalhos das bibliotecas usadas pelo Hadoop).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;yum install -y tar gzip gcc-c++ cmake zlib zlib-devel openssl openssl-devel fuse fuse-devel bzip2 bzip2-devel snappy snappy-devel

&amp;gt; (...)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;O Hadoop ainda depende de duas outras bibliotecas que precisam ser instaladas manualmente no CentOS: Google ProtoBuf 2.5 (RPC), Jansson (JSON).&lt;/p&gt;

&lt;p&gt;Para instalar o ProtoBuf, é necessário baixar o pacote, configurar para as pastas do CentOS (64 bits) e instalar.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -L -O https://github.com/google/protobuf/releases/download/v2.5.0/protobuf-2.5.0.tar.gz
tar zxf protobuf-2.5.0.tar.gz
cd protobuf-2.5.0
./configure --prefix=/usr --libdir=/usr/lib64
make
make check
make install

cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para instalar o Jansson, é necessário baixar o pacote, configurar para as pastas do CentOS (64 bits) e instalar.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -O http://www.digip.org/jansson/releases/jansson-2.7.tar.gz
tar zxf jansson-2.7.tar.gz
cd jansson-2.7
./configure --prefix=/usr --libdir=/usr/lib64
make
make install

cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para completar o ambiente de compilação, precisamos do JDK e do Maven.&lt;/p&gt;

&lt;p&gt;No caso do JDK, usaremos o pacote RPM já disponibilizado pela Oracle.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -k -L -H &amp;quot;Cookie: oraclelicense=accept-securebackup-cookie&amp;quot; -O http://download.oracle.com/otn-pub/java/jdk/8u60-b27/jdk-8u60-linux-x64.rpm
rpm -i jdk-8u60-linux-x64.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;No caso do Maven, usaremos o pacote binário de distribuição da Apache.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -O http://archive.apache.org/dist/maven/maven-3/3.3.3/binaries/apache-maven-3.3.3-bin.tar.gz
tar zxf apache-maven-3.3.3-bin.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;O ambiente  de compilação está completo.&lt;/p&gt;

&lt;p&gt;Agora estamos pronto para a compilação do Hadoop. Nesse caso, estaremos gerando o pacote de distribuição somente com o binário Java e as bibliotecas nativas.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;su - hadoop

export PATH=$PATH:/hadoop/apache-maven-3.3.3/bin

curl -O http://archive.apache.org/dist/hadoop/common/hadoop-2.7.1/hadoop-2.7.1-src.tar.gz
tar zxf hadoop-2.7.1-src.tar.gz
cd hadoop-2.7.1-src

mvn clean package -Pdist,native -DskipTests -Drequire.snappy -Drequire.openssl -Dtar

&amp;gt; (...)
&amp;gt; main:
&amp;gt;      [exec] $ tar cf hadoop-2.7.1.tar hadoop-2.7.1
&amp;gt;      [exec] $ gzip -f hadoop-2.7.1.tar
&amp;gt;      [exec]
&amp;gt;      [exec] Hadoop dist tar available at: /hadoop/hadoop-2.7.1-src/hadoop-dist/target/hadoop-2.7.1.tar.gz
&amp;gt;      [exec]
&amp;gt; [INFO] Executed tasks
&amp;gt; [INFO]
&amp;gt; [INFO] --- maven-javadoc-plugin:2.8.1:jar (module-javadocs) @ hadoop-dist ---
&amp;gt; [INFO] Building jar: /hadoop/hadoop-2.7.1-src/hadoop-dist/target/hadoop-dist-2.7.1-javadoc.jar
&amp;gt; [INFO] ------------------------------------------------------------------------
&amp;gt; [INFO] Reactor Summary:
&amp;gt; [INFO]
&amp;gt; [INFO] Apache Hadoop Main ................................. SUCCESS [01:56 min]
&amp;gt; [INFO] Apache Hadoop Project POM .......................... SUCCESS [ 42.134 s]
&amp;gt; [INFO] Apache Hadoop Annotations .......................... SUCCESS [ 37.761 s]
&amp;gt; [INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.125 s]
&amp;gt; [INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [ 23.183 s]
&amp;gt; [INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 25.962 s]
&amp;gt; [INFO] Apache Hadoop MiniKDC .............................. SUCCESS [03:23 min]
&amp;gt; [INFO] Apache Hadoop Auth ................................. SUCCESS [02:11 min]
&amp;gt; [INFO] Apache Hadoop Auth Examples ........................ SUCCESS [ 10.145 s]
&amp;gt; [INFO] Apache Hadoop Common ............................... SUCCESS [03:29 min]
&amp;gt; [INFO] Apache Hadoop NFS .................................. SUCCESS [  4.724 s]
&amp;gt; [INFO] Apache Hadoop KMS .................................. SUCCESS [02:35 min]
&amp;gt; [INFO] Apache Hadoop Common Project ....................... SUCCESS [  0.024 s]
&amp;gt; [INFO] Apache Hadoop HDFS ................................. SUCCESS [02:15 min]
&amp;gt; [INFO] Apache Hadoop HttpFS ............................... SUCCESS [02:13 min]
&amp;gt; [INFO] Apache Hadoop HDFS BookKeeper Journal .............. SUCCESS [ 38.598 s]
&amp;gt; [INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [  3.213 s]
&amp;gt; [INFO] Apache Hadoop HDFS Project ......................... SUCCESS [  0.032 s]
&amp;gt; [INFO] hadoop-yarn ........................................ SUCCESS [  0.030 s]
&amp;gt; [INFO] hadoop-yarn-api .................................... SUCCESS [ 29.193 s]
&amp;gt; [INFO] hadoop-yarn-common ................................. SUCCESS [02:02 min]
&amp;gt; [INFO] hadoop-yarn-server ................................. SUCCESS [  0.040 s]
&amp;gt; [INFO] hadoop-yarn-server-common .......................... SUCCESS [  8.499 s]
&amp;gt; [INFO] hadoop-yarn-server-nodemanager ..................... SUCCESS [ 12.283 s]
&amp;gt; [INFO] hadoop-yarn-server-web-proxy ....................... SUCCESS [  2.359 s]
&amp;gt; [INFO] hadoop-yarn-server-applicationhistoryservice ....... SUCCESS [  5.298 s]
&amp;gt; [INFO] hadoop-yarn-server-resourcemanager ................. SUCCESS [ 15.095 s]
&amp;gt; [INFO] hadoop-yarn-server-tests ........................... SUCCESS [  3.772 s]
&amp;gt; [INFO] hadoop-yarn-client ................................. SUCCESS [  4.641 s]
&amp;gt; [INFO] hadoop-yarn-server-sharedcachemanager .............. SUCCESS [  2.433 s]
&amp;gt; [INFO] hadoop-yarn-applications ........................... SUCCESS [  0.019 s]
&amp;gt; [INFO] hadoop-yarn-applications-distributedshell .......... SUCCESS [  1.884 s]
&amp;gt; [INFO] hadoop-yarn-applications-unmanaged-am-launcher ..... SUCCESS [  1.263 s]
&amp;gt; [INFO] hadoop-yarn-site ................................... SUCCESS [  0.020 s]
&amp;gt; [INFO] hadoop-yarn-registry ............................... SUCCESS [  3.532 s]
&amp;gt; [INFO] hadoop-yarn-project ................................ SUCCESS [  3.452 s]
&amp;gt; [INFO] hadoop-mapreduce-client ............................ SUCCESS [  0.036 s]
&amp;gt; [INFO] hadoop-mapreduce-client-core ....................... SUCCESS [ 15.195 s]
&amp;gt; [INFO] hadoop-mapreduce-client-common ..................... SUCCESS [ 12.459 s]
&amp;gt; [INFO] hadoop-mapreduce-client-shuffle .................... SUCCESS [  2.645 s]
&amp;gt; [INFO] hadoop-mapreduce-client-app ........................ SUCCESS [  6.342 s]
&amp;gt; [INFO] hadoop-mapreduce-client-hs ......................... SUCCESS [  3.845 s]
&amp;gt; [INFO] hadoop-mapreduce-client-jobclient .................. SUCCESS [ 11.295 s]
&amp;gt; [INFO] hadoop-mapreduce-client-hs-plugins ................. SUCCESS [  1.546 s]
&amp;gt; [INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [  4.573 s]
&amp;gt; [INFO] hadoop-mapreduce ................................... SUCCESS [  2.164 s]
&amp;gt; [INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [  7.874 s]
&amp;gt; [INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [ 19.660 s]
&amp;gt; [INFO] Apache Hadoop Archives ............................. SUCCESS [  2.071 s]
&amp;gt; [INFO] Apache Hadoop Rumen ................................ SUCCESS [  3.966 s]
&amp;gt; [INFO] Apache Hadoop Gridmix .............................. SUCCESS [  3.215 s]
&amp;gt; [INFO] Apache Hadoop Data Join ............................ SUCCESS [  1.818 s]
&amp;gt; [INFO] Apache Hadoop Ant Tasks ............................ SUCCESS [  1.478 s]
&amp;gt; [INFO] Apache Hadoop Extras ............................... SUCCESS [  2.037 s]
&amp;gt; [INFO] Apache Hadoop Pipes ................................ SUCCESS [  5.880 s]
&amp;gt; [INFO] Apache Hadoop OpenStack support .................... SUCCESS [  3.407 s]
&amp;gt; [INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [ 40.013 s]
&amp;gt; [INFO] Apache Hadoop Azure support ........................ SUCCESS [ 11.557 s]
&amp;gt; [INFO] Apache Hadoop Client ............................... SUCCESS [  7.659 s]
&amp;gt; [INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [  0.042 s]
&amp;gt; [INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [  3.072 s]
&amp;gt; [INFO] Apache Hadoop Tools Dist ........................... SUCCESS [  8.519 s]
&amp;gt; [INFO] Apache Hadoop Tools ................................ SUCCESS [  0.014 s]
&amp;gt; [INFO] Apache Hadoop Distribution ......................... SUCCESS [ 30.616 s]
&amp;gt; [INFO] ------------------------------------------------------------------------
&amp;gt; [INFO] BUILD SUCCESS
&amp;gt; [INFO] ------------------------------------------------------------------------
&amp;gt; [INFO] Total time: 29:26 min
&amp;gt; [INFO] Finished at: 2015-09-01T00:47:31+00:00
&amp;gt; [INFO] Final Memory: 224M/785M
&amp;gt; [INFO] ------------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para completar a compilação, executamos os testes, contudo, alguns deles podem apresentar falhas intermitentes (acontecem algumas vezes, outras não).&lt;/p&gt;

&lt;p&gt;Os testes podem levar algumas horas para rodar por completo.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mkdir hadoop-common-project/hadoop-common/target/test-classes/webapps/test

mvn test -Pnative -Drequire.snappy -Drequire.openssl -Dmaven.test.failure.ignore=true -Dsurefire.rerunFailingTestsCount=3

&amp;gt; (...)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(alguns testes com falha intermitente)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;org.apache.hadoop.ipc.TestDecayRpcScheduler#testAccumulate&lt;/li&gt;
&lt;li&gt;org.apache.hadoop.ipc.TestDecayRpcScheduler#testPriority&lt;/li&gt;
&lt;li&gt;org.apache.hadoop.hdfs.server.datanode.TestDataNodeMetrics#testDataNodeTimeSpend&lt;/li&gt;
&lt;li&gt;org.apache.hadoop.hdfs.shortcircuit.TestShortCircuitCache#testDataXceiverHandlesRequestShortCircuitShmFailure&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;No final desse procedimento, o pacote do Hadoop estará gerado em:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;/hadoop/hadoop-2.7.1-src/hadoop-dist/target/hadoop-2.7.1.tar.gz&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Para copiar do container para a máquina host:
&lt;br/&gt;(&lt;code&gt;3cc2bc5e593b&lt;/code&gt; é o identificador do container no Docker)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# shell na máquina
sudo docker cp 3cc2bc5e593b:/hadoop/hadoop-2.7.1-src/hadoop-dist/target/hadoop-2.7.1.tar.gz .
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusão&#34;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;Esse procedimento mostra como o Hadoop pode ser customizado para necessidades específicas e que não requer um esforço muito grande.&lt;/p&gt;

&lt;p&gt;Contudo, ter uma &amp;ldquo;versão&amp;rdquo; própria do Hadoop é uma decisão que deve ser tomada com cautela.&lt;/p&gt;

&lt;p&gt;No momento, a gente considera que essa seja a melhor escolha para o nosso trabalho na Globo.com e estamos querendo formar um time para evoluir e dar suporte a essa plataforma. O maior benefício é a liberdade de escolher como configurar e melhorar nossa infraestrutura. O custo é não ter uma empresa especializada &amp;ldquo;cuidando&amp;rdquo; dessa responsabilidade.&lt;/p&gt;

&lt;p&gt;No futuro, pode ser que mudemos esse modo de operação e busquemos uma distribuição &amp;ldquo;profissional&amp;rdquo; como Cloudera, Hortonworks ou outra.&lt;/p&gt;

&lt;p&gt;Particularmente, eu prefiro manter uma plataforma própria.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BigData na Globo.com</title>
      <link>http://cirocavani.github.io/post/bigdata-na-globocom/</link>
      <pubDate>Thu, 27 Aug 2015 06:00:00 -0300</pubDate>
      
      <guid>http://cirocavani.github.io/post/bigdata-na-globocom/</guid>
      <description>

&lt;p&gt;A proposta desse artigo é fundamentar alguns conceitos de BigData e explorar a dinâmica de como tratar um grande volume de dados para extrair valor. A ideia é apresentar a solução de dados na Plataforma de BigData da Globo.com usada pelo Sistema de Recomendação e comentar a experiência do seu desenvolvimento.&lt;/p&gt;

&lt;p&gt;Esse artigo é uma atualização e expansão da palestra realizada no Rio BigData Meetup em 21 de Outubro de 2014.&lt;/p&gt;

&lt;p&gt;Os slides originais dessa palestra podem ser acessados &lt;a href=&#34;http://www.slideshare.net/cirocavani/rio-big-data-meetup-20141021&#34;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;personalização&#34;&gt;Personalização&lt;/h2&gt;

&lt;p&gt;A Globo.com é a empresa de Internet do Grupo Globo e tem alguns dos maiores portais do Brasil (G1, Globo Esporte, GShow e Vídeos). São dezenas de milhões de acessos por dia aos portais com cerca de 10 milhões de visitantes únicos e uma produção massiva de conteúdo bastante variado em Jornalismo, Esporte, Variedades e Vídeos (juntamente com TV Globo, Editora Globo e demais empresas do Grupo).&lt;/p&gt;

&lt;p&gt;Com a proposta de personalizar o conteúdo da Globo.com nos diversos produtos que formam cada portal, o time de Personalização foi criado a um pouco mais de 2 anos e meio. Esse time recebeu o desafio de implementar um Sistema de Recomendação que pudesse recomendar para milhões de usuários ativos milhares de itens variados (notícias, vídeos, filmes, &amp;hellip;).&lt;/p&gt;

&lt;p&gt;A Plataforma de BigData da Globo.com surgiu dessa experiência.&lt;/p&gt;

&lt;h3 id=&#34;recomendação&#34;&gt;Recomendação&lt;/h3&gt;

&lt;p&gt;O Sistema de Recomendação é projetado para coletar os sinais produzidos tanto pela dinâmica do usuário quanto pela dinâmica do conteúdo e filtrar o que é relevante para o usuário no momento em que isso é importante.&lt;/p&gt;

&lt;p&gt;Nesse sistema, os dados coletados são processados e transformados em modelos de Usuário, Conteúdo e Contexto. Esses modelos são usados para apresentar as melhores opções de acordo com parâmetros do Produto. A interação do usuário com o conteúdo recomendado é avaliada através de testes usados para orientar constantes mudanças em busca de melhorar a performance do sistema.&lt;/p&gt;

&lt;p&gt;A plataforma desenvolvida na Globo.com para recomendação personalizada de conteúdo é projetada para suportar milhões de usuários ativos, milhares de itens variados em fluxo, diversos contextos de produtos.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://cirocavani.github.io/images/posts/bigdata_na_globocom/recsys.png&#34; alt=&#34;Sistema de Recomendação na Globo.com&#34; style=&#34;width: 100%; padding:1px; border:1px solid #021a40;&#34;/&gt;&lt;/p&gt;

&lt;p&gt;Essa plataforma pode ser dividida em três partes: recebimento de dados em tempo real, processamento de algoritmos de recomendação, e; consulta on-line com combinação de modelos.&lt;/p&gt;

&lt;p&gt;O processo de recebimento de dados em tempo real é baseado no Kafka, um cluster de distribuição de mensagens de alta performance que suporta bilhões de mensagens por dia. Esse sistema é usado para receber todos os sinais coletados sobre o usuário possibilitando o processamento stream em tempo real. Eventualmente, esse stream é armazenado permanentemente no cluster Hadoop para processamento em lote e análise.&lt;/p&gt;

&lt;p&gt;O processo dos algoritmos de recomendação varia para cada caso. Um caso típico é o processamento em três etapas no cluster Hadoop. Na primeira etapa, é feito um condicionamento de dados novos, seja para transformar em um formato específico ou para atualizar um pré-modelo gerado na última execução. Na segunda etapa, é executado o algoritmo de recomendação. Na terceira etapa, o resultado do algoritmo é verificado e transformado no formato que pode ser consultado. Normalmente a execução é condicionada por vários parâmetros e é comum ter diferentes configurações do mesmo algoritmo em teste.&lt;/p&gt;

&lt;p&gt;A consulta on-line é a API que os desenvolvedores da Globo.com podem usar para recomendar conteúdo personalizado. A API faz a combinação dos diversos modelos de recomendação e filtra o conteúdo mais relevante para o usuário. A API permite criar testes A/B com combinações diferentes dos modelos permitindo avaliar a performance de um algoritmo contra outro, ou o mesmo com parâmetros diferentes.&lt;/p&gt;

&lt;p&gt;Um exemplo de como esse sistema funciona:&lt;br/&gt;
(esse exemplo captura a essência, mas é só uma ilustração)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Collaborative Filtering por Fatores Latentes usando Spark&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Toda vez que uma página da Globo.com é visitada, é gerado um sinal de pageview com o id do usuário, a URL e o timestamp. Esse sinal passa pelo Kafka e é armazenado no Hadoop. Um job Spark varre todos os sinais não processados na última execução (timestamp) gerando um arquivo com o formato user-item-rating (rating = número de visitas, por simplicidade) correspondendo a matriz de Preferências. Outro job Spark é executado para processar a matriz de Preferências, fazer a fatoração e gerar os vetores de fatores latentes por usuário e item. Os fatores latentes são usados para calcular os scores (ratings) não observados (itens não vistos pelo usuário) e os 30 maiores são armazenados no HBase. Quando configurada para esse algoritmo, a API consulta o HBase e usa a lista com score para combinar com outros resultados e fazer a recomendação.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;tecnologia&#34;&gt;Tecnologia&lt;/h2&gt;

&lt;p&gt;A revolução em BigData é um fenômeno da tecnologia desenvolvida ao longo dos últimos anos focada na manipulação de um grande volume de dados em máquinas de baixo custo. Essa é a tecnologia que torna possível combinar uma solução de dados escalável com processos para geração de resultados relevantes, tanto no desenvolvimento de produtos quanto na evolução do conhecimento. O importante é entender como essa tecnologia pode ser usada para agregar valor ao negócio e permitir imaginar soluções inovadoras.&lt;/p&gt;

&lt;p&gt;A Globo.com tem Data Center próprio e a instalação do cluster de BigData foi feito em máquinas físicas. Esse cluster é composto por um conjunto maior de máquinas com Hadoop e HBase e outro conjunto menor de máquinas com Kafka e ZooKeeper. O principal framework que usamos é o Spark, mas ainda temos alguns scripts Pig de processos antigos (legado). É um cluster de média capacidade que deve crescer com o tempo / necessidades.&lt;/p&gt;

&lt;p&gt;Em artigos futuros, entrarei em mais detalhes sobre como usar essas tecnologias.&lt;/p&gt;

&lt;h3 id=&#34;hadoop&#34;&gt;Hadoop&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://cirocavani.github.io/images/posts/bigdata_na_globocom/hadoop-logo.jpg&#34; alt=&#34;Hadoop logo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;O Hadoop torna o custo por byte de armazenamento / processamento marginal e por isso torna BigData possível na escala de hoje. O problema que o Hadoop resolve é de otimizar a utilização de múltiplas máquinas por vários processos.&lt;/p&gt;

&lt;p&gt;Com o Hadoop é possível ter escalabilidade horizontal, alta taxa de utilização (CPU/Memória) e tolerância a falhas com máquinas de baixo custo para armazenar e processar dados. Um processo que é dimensionado inicialmente para um certo volume pode ser escalado colocando mais máquinas no cluster, muitos processos podem compartilhar esse cluster sem preocupação de sobrecarga, ociosidade ou interferência e a perda de máquinas ou falhas de disco não resultam em indisponibilidade ou perda de dado.&lt;/p&gt;

&lt;p&gt;O paradigma essencial é partir da premissa de que máquinas de baixo custo não são confiáveis e que é mais eficiente levar o processamento para onde o dado fica armazenado do que movimentar dados. O Hadoop é um sistema que foi desenvolvido com essa essência.&lt;/p&gt;

&lt;p&gt;O Hadoop é um sistema de arquivos distribuído (HDFS) e um sistema de execução distribuído (YARN), de outra forma, é essencialmente um sistema operacional para um cluster de máquinas onde dados são armazenados e processados. O processamento de dados é feito através de aplicações que fazem uso das APIs desses sistemas, contudo elas são mais elementares. Existem vários frameworks que são abstrações dessas APIs e oferecem modelos de programação mais elaborados, com funcionalidades mais avançadas.&lt;/p&gt;

&lt;p&gt;O framework original do Hadoop é o MapReduce voltado para processamento em lote com grande latência. Pig é uma ferramenta que permite escrever scripts em uma linguagem alto nível de manipulação de dados; os scripts são compilado para Java (byte-code) e executados no Hadoop na forma de vários jobs MapReduce. Essa é a tecnologia que usamos no início de BigData na Globo (Recomendação e Busca). Contudo, MapReduce é uma tecnologia que está sendo abandonada - o Google, seu criador, anunciou há algum tempo que deixou de usar MapReduce e passou a usar a tecnologia do Dataflow (oferecida ao grande público como Cloud Dataflow).&lt;/p&gt;

&lt;p&gt;Por outro lado, a indústria também vem abandonando o MapReduce e, entre várias opções, duas tecnologias estão em evidência: Spark e Tez. O Spark apresenta uma API simples de operações sobre coleções de dados, internamente constrói um grafo de tarefas que são executadas em blocos de memória distribuídos; suporta SQL, Machine Learning, Streaming e Processamento de Grafos; está sendo usado como engine para diversas ferramentas analíticas (Mahout, por exemplo). O Tez é mais baixo nível e encapsula a construção de grafos de execução que podem ser otimizados dinamicamente; é usado principalmente como engine em ferramentas mais elaboradas (Hive e Pig, por exemplo). Uma terceira tecnologia que pode ganhar tração é o Flink, também segue o princípio de API simples, construção de plano de execução, otimização.&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;O HDFS é composto por dois serviços: o NameNode que mantém o índice de arquivos e o mapeamento desses arquivos para os blocos de dados distribuídos pelo cluster (roda em uma máquina); e o DataNode que armazena os blocos de arquivo (roda em todas as máquinas). Um arquivo pode ser dividido em vários blocos e cada bloco ainda é replicado em 3 DataNodes. O NameNode controla as operações de manipulação de arquivo, mas as operação de leitura e escrita são feitas diretamente com os DataNodes. A perda do DataNode não resulta em perda de dados e o número de réplicas é sempre ajustado. As réplicas também são usadas para o acesso local aos dados por processos paralelos.&lt;/p&gt;

&lt;p&gt;O YARN é composto por dois serviços: o ResourceManager que mantém registro e controla a alocação de memória e processador das máquinas do cluster (roda em uma máquina); e o NodeManager que controla os container de execução de uma aplicação (roda em todas as máquinas). Um terceiro serviço é o ApplicationMaster associado a cada aplicação que roda no cluster e negocia com o ResourceManager a alocação de recursos providas nos NodeManagers. Um framework como Spark roda como um ApplicationMaster que cria containers &amp;lsquo;workers&amp;rsquo; para processar os dados em memória. MapReduce segue esse mesmo princípio, criando containers de Mappers e Reducers. Também é possível rodar aplicações standalone como HBase e Kafka usando YARN.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Hadoop&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://hadoop.apache.org/&#34;&gt;https://hadoop.apache.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spark&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://spark.apache.org/&#34;&gt;https://spark.apache.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tez&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://tez.apache.org/&#34;&gt;https://tez.apache.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Flink&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://flink.apache.org/&#34;&gt;https://flink.apache.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MapReduce and Spark&lt;/strong&gt; (30/Dez/2013)&lt;br/&gt;
&lt;a href=&#34;http://vision.cloudera.com/mapreduce-spark/&#34;&gt;http://vision.cloudera.com/mapreduce-spark/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Google Cound Dataflow&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://cloud.google.com/dataflow/&#34;&gt;https://cloud.google.com/dataflow/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Google File System&lt;/strong&gt; (Out/2003)&lt;br/&gt;
&lt;a href=&#34;http://research.google.com/archive/gfs.html&#34;&gt;http://research.google.com/archive/gfs.html&lt;/a&gt;&lt;br/&gt;
&lt;a href=&#34;http://research.google.com/archive/gfs-sosp2003.pdf&#34;&gt;http://research.google.com/archive/gfs-sosp2003.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MapReduce: Simplified Data Processing on Large Clusters&lt;/strong&gt; (Dez/2004)&lt;br/&gt;
&lt;a href=&#34;http://research.google.com/archive/mapreduce.html&#34;&gt;http://research.google.com/archive/mapreduce.html&lt;/a&gt;&lt;br/&gt;
&lt;a href=&#34;http://research.google.com/archive/mapreduce-osdi04.pdf&#34;&gt;http://research.google.com/archive/mapreduce-osdi04.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Apache Hadoop YARN: yet another resource negotiator&lt;/strong&gt; (Out/2013)&lt;br/&gt;
&lt;a href=&#34;http://www.socc2013.org/home/program/a5-vavilapalli.pdf&#34;&gt;http://www.socc2013.org/home/program/a5-vavilapalli.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;hbase&#34;&gt;HBase&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://cirocavani.github.io/images/posts/bigdata_na_globocom/hbase-logo.png&#34; alt=&#34;HBase logo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;O HBase é um banco de dados sem esquema que suporta bilhões de linhas por tabela. Inspirado no paper BigTable do Google, suporta uma estrutura chave-valor ordenada, multidimensional, esparsa, distribuída e persistente. O principal benefício é o suporte para operação eficiente com pequenos blocos de bytes em complemento ao HDFS que opera melhor com grandes blocos.&lt;/p&gt;

&lt;p&gt;O HBase opera como um cluster distribuído tolerante a falhas. A leitura e escrita são consistentes, a consulta pela chave tem baixa latência e suporta um grande número de operações concorrentes. Contudo, sua API é limitada a operações simples (put, get, scan, array de bytes) e estendida com processadores executados dentro do cluster (filter, coprocessor). Não tem suporte nativo para linguagem de consulta e também não suporta transação multi-chave.&lt;/p&gt;

&lt;p&gt;Estruturar os dados no HBase conciliando performance de escrita, performance de leitura e representação expressiva é um grande desafio. Os dados tem índice único pela chave da tabela e a performance depende da distribuição dessas chaves entre os servidores.&lt;/p&gt;

&lt;p&gt;O Phoenix é uma solução complementar ao HBase com suporte a índices secundários e consultas SQL. O Phoenix opera dentro do HBase (coprocessador) e do lado da aplicação, fazendo a intermediação das operações de leitura e escrita condicionando os dados. A modelagem de dados fica acoplada com a ferramenta. A API para aplicação que usa o Phoenix é um driver JDBC, incluindo suporte para esquema e dados &amp;lsquo;tipados&amp;rsquo; (não só bytes).&lt;/p&gt;

&lt;p&gt;O Spark tem suporte ao HBase através da API de Input/Output padrão do Hadoop ou direto com a API do HBase. Ou seja, são operações simples para leitura e escrita, mas não consulta mais elaboradas. O HBase ainda não é suportado pelo Spark SQL / Catalyst, framework para processamento de dados estruturados com suporte a execução e otimização de consultas SQL e operações em tabela (DataFrame). Esse é um projeto em andamento e vai possibilitar que consultas complexas sejam otimizadas, minimizando a movimentação de dados e transferindo parte do processamento para o HBase.&lt;/p&gt;

&lt;p&gt;O HBase tem papel fundamental como intermediário entre as aplicações que fazem processamento de grandes volumes de dados e aplicações que fazem acesso a dados específicos. O caso de uso principal em que usamos o HBase é para o armazenamento de resultados que podem ser usados externamente ao Hadoop. Isso diminui a movimentação de grande volumes de dados para sistemas externos e mantém a baixa latência necessária para uso externo.&lt;/p&gt;

&lt;p&gt;O Google, criador do BigTable que inspirou o HBase, acabou &amp;lsquo;desistindo&amp;rsquo; dessa tecnologia e implementando um sistema mais complexo com suporte a operações transacionais em larga escala. Esse sistema chama-se Spanner e tem muitos dos princípios originais do BigTable, mas com suporte a serialização global de operações. Para manter esse sincronismo, são usados relógios atômicos e GPS de precisão. Apesar de ser um &amp;lsquo;sucessor&amp;rsquo; do BigTable no Google, dificilmente vai tornar obsoleto o HBase ou inspirar um novo banco (eu posso estar errado aqui!).&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;O HBase é composto por dois serviços: o HMaster monitora os RegionServers, distribui as Regiões de Dados (Splits, bloco de Linhas de uma Tabela) e recebe operações de DDL, e; o RegionServer mantém Regiões de Dados em um DataNode / HDFS, recebe acesso direto dos Clientes para operação de leitura/escrita. O ZooKeeper é usado para metadados / configurações. Escalabilidade através da adição de servidores (RegionServers).&lt;/p&gt;

&lt;p&gt;O RegionServer tem um WAL (Write Ahead Log), um BlockCache e várias Regiões de Dados. Cada Região tem vários Armazenamentos (Store), um por Família, cada um com vários arquivos (HFile) e um Armazenamento em Memória (MemStore). O WAL e os HFiles são persistidos no HDFS. Os arquivos de dados (HFiles) são reescritos de tempos em tempos para obter localidade e ajustar automaticamente a performance.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;HBase&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://hbase.apache.org/&#34;&gt;https://hbase.apache.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Phoenix&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://phoenix.apache.org/&#34;&gt;https://phoenix.apache.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spark SQL on HBase&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://github.com/Huawei-Spark/Spark-SQL-on-HBase&#34;&gt;https://github.com/Huawei-Spark/Spark-SQL-on-HBase&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Slider&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://slider.incubator.apache.org/&#34;&gt;https://slider.incubator.apache.org/&lt;/a&gt;&lt;br/&gt;
&lt;a href=&#34;https://wiki.apache.org/incubator/SliderProposal&#34;&gt;https://wiki.apache.org/incubator/SliderProposal&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bigtable: A Distributed Storage System for Structured Data&lt;/strong&gt; (Nov/2006)&lt;br/&gt;
&lt;a href=&#34;http://research.google.com/archive/bigtable.html&#34;&gt;http://research.google.com/archive/bigtable.html&lt;/a&gt;&lt;br/&gt;
&lt;a href=&#34;http://research.google.com/archive/bigtable-osdi06.pdf&#34;&gt;http://research.google.com/archive/bigtable-osdi06.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spanner: Google&amp;rsquo;s Globally-Distributed Database&lt;/strong&gt; (Out/2012)&lt;br/&gt;
&lt;a href=&#34;http://research.google.com/archive/spanner.html&#34;&gt;http://research.google.com/archive/spanner.html&lt;/a&gt;&lt;br/&gt;
&lt;a href=&#34;http://research.google.com/archive/spanner-osdi2012.pdf&#34;&gt;http://research.google.com/archive/spanner-osdi2012.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;kafka&#34;&gt;Kafka&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://cirocavani.github.io/images/posts/bigdata_na_globocom/kafka-logo.png&#34; alt=&#34;Kafka logo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;O Kafka é um cluster de distribuição de mensagens que escala para um grande número de produtores e consumidores. Criado pelo LinkedIn, o Kafka tornou-se a tecnologia fundamental para uma arquitetura de dados stream. Com o Kafka, é possível concentrar e distribuir um fluxo muito grande de dados que podem ser acessados tanto para processamento em tempo real quanto para processamento em lote. O principal benefício é o acesso direto ao dado quase no mesmo instante em que ele é gerado.&lt;/p&gt;

&lt;p&gt;O Kafka opera como um cluster distribuído tolerante a falhas. É baseado no modelo Publish-Subscribe e essencialmente funciona como um &amp;lsquo;append-only log&amp;rsquo;, com novas mensagens adicionadas no final de cada arquivo e leitura sequencial. As aplicações podem enviar mensagens para um determinado tópico; os servidores recebem essas mensagens e as armazenam em arquivos, e as aplicações podem pegar essas mensagens por tópico em qualquer momento (tempo real ou lote). Por &amp;lsquo;concepção&amp;rsquo;, as funcionalidades são bastante limitadas e orientadas para suportar um fluxo grande de mensagens.&lt;/p&gt;

&lt;p&gt;O Kafka se destaca em: Performance, com alto throughput no recebimento e distribuição; Escalabilidade, muitos consumidores, isolamento entre consumidores, e; Mensagens pequenas, não estruturadas / opacas (bytes).&lt;/p&gt;

&lt;p&gt;O Spark Streaming tem suporte ao Kafka como fonte de dados. São duas APIs, a de Receivers que consome constantemente mensagens e armazena na aplicação e a Direta que mapeia os offsets das mensagens e faz consumo por intervalo (do mini-batch) - a API Direta é mais específica e melhor com o Kafka. A partir do stream do Kafka, é possível processar os dados usando as funcionalidades sofisticadas do Spark. Um caso de uso é o armazenamento em lote de arquivos Parquet para consulta histórica. Outro exemplo é o processamento de janelas para a geração de métricas em tempo real (conversão, visualizações). O Spark é a ferramente ideal para o processamento e análise dos dados distribuídos pelo Kafka.&lt;/p&gt;

&lt;p&gt;Com o Kafka, a gente construiu um sistema de coleta de atividades. Seu propósito é tornar possível receber, distribuir e armazenar toda e qualquer informação sobre o usuário escalando para bilhões de mensagens por dia trafegando pelo sistema. Nesse sistema trafegam as páginas visitadas, vídeos assistidos, buscas, comentários, compartilhamentos, track da página, &amp;hellip; todos esses dados são processados e armazenados no HDFS no formato Parquet. Simultaneamente, esse dados são usados em vários outros sistemas, Recomendação e Busca entre eles. Essa arquitetura de dados vem se mostrando fundamental no desenvolvimento de soluções de dados inovadoras.&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;O Kafka é composto por três partes: o cluster de Brokers, a API do Produtor e a API do Consumidor. As APIs são usadas nas aplicações que se comunicam com o cluster e implementam o protocolo do Kafka. Implementações alternativas desse protocolo podem ser usadas para comunicação direta com o cluster.&lt;/p&gt;

&lt;p&gt;Os Brokers organizam as Mensagens em Tópicos, podem ficar em memória temporariamente e são sincronizadas para disco de tempos em tempos, mantidas por tempo determinado (também pode ser limitada por espaço). Os Tópicos são divididos em Partições, cada uma com um arquivo em disco diferente. Os Tópicos podem ser replicados em vários Broker. Cada Broker gerencia um grupo de Tópicos. A perda de um Broker ou a corrupção de um arquivo de Partição não resulta em perda dos dados. Os Tópicos são criados com um nome único, número de Partições e número de Réplicas.&lt;/p&gt;

&lt;p&gt;Produtores enviam Mensagens para um Tópico específico, para o qual é possível definir a Partição para distribuir a carga. Produtores também podem publicar mensagens de forma assíncrona, fazendo buffer em memória na aplicação. Consumidores precisam pegar mensagens de cada Partição, controlando o índice da mensagem consumida (offset). Ou seja, o número de Partições determina o paralelismo de consumo e o controle de consumo é responsabilidade da aplicação (o Broker não mantém controle do consumo).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kafka&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://kafka.apache.org/&#34;&gt;https://kafka.apache.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Building LinkedIn&amp;rsquo;s Real-time Activity Data Pipeline&lt;/strong&gt; (Jun/2012)&lt;br/&gt;
&lt;a href=&#34;http://sites.computer.org/debull/A12june/pipeline.pdf&#34;&gt;http://sites.computer.org/debull/A12june/pipeline.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Log: What every software engineer should know about real-time data&amp;rsquo;s unifying abstraction&lt;/strong&gt; (Dez/2013)&lt;br/&gt;
&lt;a href=&#34;http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying&#34;&gt;http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spark Streaming&lt;/strong&gt;&lt;br/&gt;
httpd://spark.apache.org/streaming/&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kafka on YARN (Slider)&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://github.com/DataTorrent/koya&#34;&gt;https://github.com/DataTorrent/koya&lt;/a&gt;&lt;br/&gt;
&lt;a href=&#34;https://slider.incubator.apache.org/&#34;&gt;https://slider.incubator.apache.org/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;spark&#34;&gt;Spark&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://cirocavani.github.io/images/posts/bigdata_na_globocom/spark-logo.png&#34; alt=&#34;Spark logo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;O Spark é um framework para processamento distribuído de grande quantidade de dados com foco em análise interativa. O Spark tem uma API simples que torna fácil escrever desde pequenas manipulações de dados, até algoritmos iterativos complexos, de forma escalável e com boas práticas de engenharia de software (testes). Essa característica o torna a ferramenta ideal de trabalho tanto para o Data Engineer que tem que desenvolver uma solução de dados, quanto para o Data Scientist que tem que desenvolver modelos e visualizações. Dessa forma, o Spark hoje é composto por um conjunto grande de funcionalidades com suporte DataFrame / SQL, Machine Learning, Streaming e Processamento de Grafos. Também é o projeto mais popular na área de BigData e cada nova versão adiciona ainda mais abrangência e inovação.&lt;/p&gt;

&lt;p&gt;O projeto começou em Berkeley (pesquisa) como alternativa ao MapReduce para processar dados no cluster usando memória das máquinas como armazenamento entre etapas e vem evoluindo para ser um framework completo de analise de dados. Já fazem parte do projeto: o Spark SQL / DataFrame para processamento de dados estruturados, tabular; Spark ML / MLlib com uma pipeline de processamento e algoritmos de Machine Learning; o Spark Streaming para processamento de stream, mini-batches, diversas fontes como o Kafka, e; o GraphX para processamento de grafo, algoritmos de rede. Esses componentes são construídos sobre a mesma API básica do Spark e podem ser usadas em conjunto. Da proposta original aos componentes avançados, o Spark substitui o MapReduce completamente (o Tez é outra abordagem para substituir o MapReduce).&lt;/p&gt;

&lt;p&gt;O MapReduce foi a solução original do Google para processamento de grande volume de dados com tolerância a falhas. O grande problema do MapReduce é a grande latência entre as diversas etapas de processamento que se baseiam em resultados intermediários armazenados no HDFS. Processamentos mais complexos são feitos através do encadeamento de vários jobs MapReduce (Pig faz isso). Esse modelo de execução escala bem, mas é bastante limitante para implementar algoritmos que fazem várias iterações sobre os mesmos dados, por exemplo. Esse é o cenário que fez o pessoal de Berkeley procurar uma nova solução que deu origem ao Spark.&lt;/p&gt;

&lt;p&gt;O Spark apresenta uma API simples de operações sobre coleções de dados (RDD), internamente constrói um grafo de tarefas (DAG) que são executadas em blocos de memória distribuídos (Blocks). A ideia básica é carregar os dados em coleções imutáveis e aplicar transformações que resultam em novas coleções, no final, usar uma ação para materializar os resultados (como salvar em disco). Esse encadeamento de ações forma o grafo das transformações, contudo, a execução as tarefas em si é feita sob-demanda (lazy), os resultados intermediários são mantidos em memória e a perda dessa informação resulta na reexecução das tarefas necessárias para regerar o dado (ao invés de quebrar a aplicação). Dessa forma, o Spark é capaz de representar o processamento através de tarefas e usar a memória para comunicação entre essas tarefas, falhas de execução ou limpeza de memória fazem o grafo ser reprocessado, mantendo a garantia de tolerância a falhas e resiliência (ponto forte do MapReduce).&lt;/p&gt;

&lt;p&gt;Na prática, o Spark facilita bastante o desenvolvimento de aplicações de processamento de dados mas ainda exige que o desenvolvedor dê grande atenção ao detalhes da aplicação, tendo que buscar soluções &amp;lsquo;eficientes&amp;rsquo; para pontos críticos só identificados com carga. Com Spark, é possível escrever testes (funcionais) que rodam na Integração Contínua validando o comportamento da aplicação, contudo, quando submetido a carga de produção, a aplicação pode apresentar muitos erros difíceis de identificar o motivo (problema de memória, demora excessiva). A solução costuma ser analisar os logs / histórico, procurar as ineficiências, ajustar parâmetros, particionar, cachear, mudar algoritmos e/ou a ordem em que os dados são processados. Ou seja, além de escrever um código correto, é preciso que o código seja &amp;lsquo;dimensionado&amp;rsquo; para os dados corretamente também.&lt;/p&gt;

&lt;p&gt;O pessoal do Spark vem trabalhando em várias abordagem para resolver esses problemas. Uma primeira abordagem é a alocação dinâmica de &amp;lsquo;worker&amp;rsquo; dando elasticidade a aplicação quando tem mais processamento e liberando recursos quando não é mais necessário. Uma segunda abordagem é oferecer uma abstração de dados mais estruturada que permite otimizar dinamicamente as manipulações de dados - o Spark introduziu o DataFrame que é uma tabela (ao invés de puramente uma coleção) que usa o Catalyst, um framework para processamento de dados estruturados que constrói um plano de execução e faz otimização para operações com a tabela e consultas SQL. Uma terceira abordagem é o esforço do Projeto Tungsten com objetivo trazer para o Spark funcionalidades de mais baixo nível, como representação compacta de dados em memória (evitando o GC da JVM), cache automático e geração dinâmica de código - nesse primeiro momento, o foco é o DataFrame / SQL. Por fim, de forma geral, uma abordagem é tornar a execução das tarefas &amp;lsquo;automaticamente&amp;rsquo; mais eficientes através de heurísticas da execução - esse é um projeto que deve frutificar nas próximas versões do Spark. Hoje a recomendação é usar o DataFrame e deixar o framework executar de forma otimizada, e usar o RDD (coleção) quando isso não funcionar.&lt;/p&gt;

&lt;p&gt;O Spark é o framework principal que a gente usa no processamento de dados. Com Spark Streaming, usamos para consumir mensagens no Kafka e persistir de forma permanente em Parquet no HDFS. Com Spark ML, usamos para construir a matriz de preferência do usuário-conteúdo e calcular a fatoração do modelo do usuário e do item para o Collaborative Filtering (ALS). Com Spark SQL / DataFrame construímos visualizações de métricas para avaliar os resultados dos testes A/B. Esses são alguns exemplos da versatilidade do Spark e de como ele é útil em uma Plataforma de BigData.&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;Uma aplicação feita com Spark pode rodar em um cluster do próprio Spark (standalone), no Mesos ou no YARN. A gente usa somente o YARN.&lt;/p&gt;

&lt;p&gt;A execução da aplicação Spark é composta por dois componentes: o Driver que inicializa a aplicação, define o grafo de tarefas e as transformações e controla a execuções das tarefas, e; os Executors que armazenam blocos de dados em memória e executam as tarefas. Em um cluster YARN, o Spark tem duas formas de execução: uma em que o Driver é executada em uma máquina local e os Executores no cluster e a outra em que o Driver também é executado no cluster. No segundo caso, o launcher do Spark negocia com o ResourceManager um container para rodar o Driver. Esse conteiner é alocado pelo NodeManager e inicializado com o Driver. Em ambos os casos, o Driver negocia com o ResourceManager os containers dos Executores que são inicializados pelo NodeManager, nesse momento, eles sabem o endereço de volta do Driver e abrem a comunicação (os dados podem trafegar entre Driver-Executores e entre Executores). Nesse ponto em diante, o processamento começa.&lt;/p&gt;

&lt;p&gt;O processamento em si é composto pelo grafo de tarefas que é construída a partir das transformações nas coleções de dados. A coleção que pode ser gerada a partir das transformações é chamada de RDD (Resilient Distributed Datasets) e é uma abstração lazy de um conjunto de dados. O resultado só é realmente materializado quando uma ação é executada no RDD, por exemplo, count ou collect. A materialização é feita através da análise de todas as tarefas do grafo de execução que precisam ser feitas para gerar esse resultado. Nesse momento, o Driver passa a enviar tarefas para os executores e armazenar os resultados nos Blocos de memória de cada Executor. Um RDD materializado é representado em partições que ficam distribuídas nos Executores. No momento em que novos RDDs são materializados, os antigos vão sendo desalocados, contudo, caso os dados sejam novamente necessários, o Driver reexecuta as tarefas para materializar esses resultados. É possível fazer cache dos dados materializados, tanto em memória quanto em disco, evitando assim o reprocessamento.&lt;/p&gt;

&lt;p&gt;Essa é a base do modelo de execução do Spark que é estendido pelas outras ferramentas do pacote, DataFrame / SQL com Catalyst, Spark Streaming com DStream e os mini-batches, as pipelines e algoritmos de Machine Learning e o processamento de grafos do GraphX.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Spark&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://spark.apache.org/&#34;&gt;https://spark.apache.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Provide elastic scaling within a Spark application&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://issues.apache.org/jira/browse/SPARK-3174&#34;&gt;https://issues.apache.org/jira/browse/SPARK-3174&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Projeto Tungsten&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://issues.apache.org/jira/browse/SPARK-7075&#34;&gt;https://issues.apache.org/jira/browse/SPARK-7075&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Adaptive execution in Spark&lt;/strong&gt;&lt;br/&gt;
&lt;a href=&#34;https://issues.apache.org/jira/browse/SPARK-9850&#34;&gt;https://issues.apache.org/jira/browse/SPARK-9850&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing&lt;/strong&gt; (2012)&lt;br/&gt;
&lt;a href=&#34;http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&#34;&gt;http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discretized Streams: A Fault-Tolerant Model for Scalable Stream Processing&lt;/strong&gt; (2012)&lt;br/&gt;
&lt;a href=&#34;http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-259.pdf&#34;&gt;http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-259.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;conclusão&#34;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;Esse artigo é a catarse de um pouco mais de 2 anos de trabalho feito pelo time de Personalização no desenvolvimento do Sistema de Recomendação. Esse trabalho teve início com um grupo de cerca de 5 pessoas e, com alguma troca, se manteve nesse tamanho por grande parte do tempo. Há cerca de 9 meses, BigData ganhou internamente em importância e foi criada uma área dentro da empresa com esse foco. Onde antes tínhamos o Sistema de Recomendação como o único produto de dados dessa Plataforma de BigData, desse ponto em diante passamos a ter o desafio de tornar essa tecnologia disponível e útil para toda a empresa e todo o Grupo. Também estamos aumentando o time e planejando o crescimento para atender outros casos de uso. O mais importante dessa experiência é que construímos um conhecimento sólido para continuarmos desenvolvendo essa plataforma e expandir sua capacidade e valor dentro do Grupo.&lt;/p&gt;

&lt;p&gt;Eu gostaria de pensar no futuro, e nesse sentido, a minha expectativa é que a gente construa na Globo.com uma plataforma similar ao Google Cloud Platform.&lt;/p&gt;

&lt;div class=&#34;embed video-player&#34;&gt;
&lt;iframe class=&#34;youtube-player&#34; type=&#34;text/html&#34;
    width=&#34;640&#34; height=&#34;385&#34;
    src=&#34;http://www.youtube.com/embed/Y0Z58YQSXv0&#34;
    allowfullscreen frameborder=&#34;0&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;A gente vem de um modelo em que um mesmo time faz o desenvolvimento do Sistema de Recomendação, coleta e processamento de dados, a análise e construção de modelos, e a gestão da Plataforma de BigData. A proposta é expandir cada uma dessas responsabilidade em times próprios e adicionar outros com foco em suportar novas demandas. É nesse processo que nos encontramos nesse momento.&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;Meu interesse pessoal é Inteligência Artificial e, alinhado com os avanços que vem sendo feitos na área, BigData é muito importante para o desenvolvimento dos modelos que estão produzindo os melhores resultados. De outra forma, também é válido dizer que a construção de agentes inteligentes é uma forma de tornar tratável uma quantidade muito grande de dados. Por uma formulação ou por outra, a ideia comum é que BigData e Inteligência Artificial tem uma intersecção importante.&lt;/p&gt;

&lt;p&gt;Tem muita tecnologia nova sendo desenvolvida que propõe formas de tratar imagens, vídeo, linguagem natural na qual um algoritmo pode executar funções que hoje são feitas por pessoas. O que eu pretendo fazer é identificar onde essa tecnologia pode ser usada na Globo.com e trazer esse conhecimento.&lt;/p&gt;

&lt;p&gt;Em específico, acredito que em algum momento teremos um time de Deep Learning para estudo e desenvolvimento de soluções novas usando BigData.&lt;/p&gt;

&lt;p&gt;Esse é o tema do meu Mestrado e no futuro, trarei mais material sobre o assunto.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>